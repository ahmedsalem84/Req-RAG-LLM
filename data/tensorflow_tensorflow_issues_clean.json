[
  "Requirement ID: ISSUE-102399\nTitle: [XLA:GPU] Don't fail Autotuner::GetSupportedConfigs if one of the backend fails\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[XLA:GPU] Don't fail Autotuner::GetSupportedConfigs if one of the backend fails",
  "Requirement ID: ISSUE-102398\nTitle: Automated Code Change\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102397\nTitle: Automated Code Change\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102396\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102395\nTitle: [XLA] scheduling fixes\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[XLA] scheduling fixes\n\nReverts a2c5a160dae9e24151401d3693819ab0f95a99c6",
  "Requirement ID: ISSUE-102394\nTitle: Automated Code Change\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102393\nTitle: Automated Code Change\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102392\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102391\nTitle: Automated Code Change\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102390\nTitle: [xla:ffi] Fix clang macro expansion warnings\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[xla:ffi] Fix clang macro expansion warnings",
  "Requirement ID: ISSUE-102389\nTitle: Automated Code Change\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102388\nTitle: Automated Code Change\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102387\nTitle: Automated Code Change\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102386\nTitle: Automated Code Change\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102385\nTitle: Automated Code Change\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102384\nTitle: Automated Code Change\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102383\nTitle: [GPU][UnsortedSegmentProd] GPU kernel assertion on large num_segments (overflow in launch config)\nState: open\nAuthor: tinywisdom\nLabels: type:bug\nBody:\n### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntensorflow version: 2.20.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 22.04.4 LTS (x86_64)\n\n### Mobile device\n\nUbuntu 22.04.4 LTS (x86_64)\n\n### Python version\n\n3.10.16\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n12.5.1/9.3.0\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\n### Summary\nCalling tf.raw_ops.UnsortedSegmentProd(data, segment_ids, num_segments) with a very large num_segments (e.g. 2^31) triggers a GPU kernel launch assertion:\n```\nCheck failed: work_element_count >= 0 (-2147483648 vs. 0)\n```\nThis happens eagerly (no tf.function required) when running on GPU. The negative launch count is likely due to overflow in internal arithmetic computing the total work size.\n\n\n### Standalone code to reproduce the issue\n\n```shell\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n\nimport tf_keras as keras\nimport tensorflow as tf\n\nclass TempModel(keras.Model):\n    def call(self, x, y, z):\n        return tf.raw_ops.UnsortedSegmentProd(data=x, segment_ids=y, num_segments=z)\n\ndef main():\n    print(\"TF version:\", tf.__version__)\n    print(\"Physical GPUs:\", tf.config.list_physical_devices(\"GPU\"))\n\n    model = TempModel()\n    with tf.device(\"/GPU:0\"):\n        x = tf.constant([3], dtype=tf.int32)\n        y = tf.constant([1], dtype=tf.int64)\n        # large num_segments triggers overflow\n        z = tf.constant(2**31, dtype=tf.int64)\n\n        _ = model(x, y, z)  # causes GPU kernel assertion\n\nif __name__ == \"__main__\":\n    main()\n```\n\n### Relevant log output\n\n```shell\nF0000 \u2026 gpu_launch_config.h:129] Check failed: work_element_count >= 0 (-2147483648 vs. 0)\n\u2026 tensorflow::functor::UnsortedSegmentFunctor<>::operator()()\n\u2026 Aborted (core dumped)\n```",
  "Requirement ID: ISSUE-102382\nTitle: Automated Code Change\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102381\nTitle: [XLA:GPU] Use the schedule sequence to statically check for deadlocks\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[XLA:GPU] Use the schedule sequence to statically check for deadlocks\n\nThe schedule can be different from the instruction order and it ultimately decides the order of execution.",
  "Requirement ID: ISSUE-102380\nTitle: Change EnterHostCallback() and\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nChange EnterHostCallback() and\nLeaveHostCallback() to use a c++ raii object to ensure\nthat Enter and Leave are always matched.",
  "Requirement ID: ISSUE-102379\nTitle: [xla:ffi] Add a test for automatic FFI handler signature inference from C++ function\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[xla:ffi] Add a test for automatic FFI handler signature inference from C++ function",
  "Requirement ID: ISSUE-102378\nTitle: Replace GpuComputeCapability with custom class\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nReplace GpuComputeCapability with custom class",
  "Requirement ID: ISSUE-102377\nTitle: [tf2xla] Move allocator testing to allocator_test.cc\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[tf2xla] Move allocator testing to allocator_test.cc",
  "Requirement ID: ISSUE-102376\nTitle: Enable multi-host support for trace viewer.\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nEnable multi-host support for trace viewer.",
  "Requirement ID: ISSUE-102375\nTitle: Integrate LLVM at llvm/llvm-project@bfee9db78577\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nIntegrate LLVM at llvm/llvm-project@bfee9db78577\n\nUpdates LLVM usage to match\n[bfee9db78577](https://github.com/llvm/llvm-project/commit/bfee9db78577)",
  "Requirement ID: ISSUE-102374\nTitle: Support the Shardy dialect in ConvertSerializedStableHloModuleToBfloat16.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nSupport the Shardy dialect in ConvertSerializedStableHloModuleToBfloat16.",
  "Requirement ID: ISSUE-102373\nTitle: PR #31030: [XLA:GPU] Move ReduceScatterCreator after AlgebraicSimplifier\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nPR #31030: [XLA:GPU] Move ReduceScatterCreator after AlgebraicSimplifier\n\nImported from GitHub PR https://github.com/openxla/xla/pull/31030\n\n\ud83d\udcdd Summary of Changes\nThis PR moves the ReduceScatterCreator pass to run after AlgebraicSimplifier, simplifying the transformation pattern and allowing ReduceScatterCreator to convert more all-reduces into reduce-scatters that would otherwise be missed.\n\n\ud83c\udfaf Justification\nRunning ReduceScatterCreator after AlgebraicSimplifier makes the input patterns easier to recognize. This allows more all-reduces to be converted into reduce-scatters, which would otherwise be missed, leading to better performance. _This was reported internally as an optimization for llama3.3-70b._ \n\n\ud83d\ude80 Kind of Contribution\n\u26a1\ufe0f Performance Improvement,\n\n\ud83d\udcca Benchmark (for Performance Improvements)\nOn H100:\n|  | PR | main |\n|----------|----------|----------|\n| llama31_8b_bf16_1x8    | 1372251 us   | 1369631 us    |\n| llama31_8b_fp8_1x8    | 1106135 us   | 1107605 us    |\n| llama31_8b_bf16_2x8    | 1373637 us   | 1370564 us    |\n| llama31_8b_fp8_2x8    | 1111912 us   | 1108061 us    |\n| llama31_70b_bf16_16x8    | 13933022 us   | 13913957 us    |\n| llama31_70b_fp8_16x8    | 9848173 us   | 9867955 us    |\n| llama31_70b_bf16_32x8    | 14103619 us   | 14065225 us    |\n| llama31_70b_fp8_32x8    | 9732961 us   | 9760739 us    |\n| llama31_405b_bf16_64x8    | 52926476 us   | 52886529 us    |\n| llama31_405b_fp8_64x8    | 35576505 us   | 37929776 us   |\n| mixtral_8x7b_bf16_1x8   | 744367 us   | 744491 us    |\n| mixtral_8x7b_bf16_2x8    | 1126425 us   | 1130912 us    |\n\n\ud83e\uddea Unit Tests:\nAdded a new unit test\n\n\ud83e\uddea Execution Tests:\nTested for functionality with llama3.3 70b zero1 + gradient accumulation and saw ~5% performance improvement.\n\nCopybara import of the project:\n\n--\n2d999987762ac3d90960179b06587bc95fc954d1 by Sevin Varoglu <svaroglu@nvidia.com>:\n\nMove ReduceScatterCreator after AlgebraicSimplifier\n\n--\n0e41c2b8281234eec9af21a98fd5f81bd4884689 by Sevin Varoglu <svaroglu@nvidia.com>:\n\nAdd unit test\n\nMerging this change closes #31030\n\nFUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/31030 from sfvaroglu:sevin/rs_creator_order 0e41c2b8281234eec9af21a98fd5f81bd4884689",
  "Requirement ID: ISSUE-102372\nTitle: Introduce `tsl::WithCurrentContext` for capturing the current context.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nIntroduce `tsl::WithCurrentContext` for capturing the current context.",
  "Requirement ID: ISSUE-102371\nTitle: Enable lowering from FQ Composite for 2-bit\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nEnable lowering from FQ Composite for 2-bit\n\nThis also adds an additional test for this lowering.",
  "Requirement ID: ISSUE-102370\nTitle: Update tfl.transpose version inconsistency in register_ref.cc\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nUpdate tfl.transpose version inconsistency in register_ref.cc\n\nregister.cc already declares support for versions 1-7 of transpose but this seems like it was previously missed for register_ref.",
  "Requirement ID: ISSUE-102369\nTitle: [XLA:GPU] Add multimem setup.\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[XLA:GPU] Add multimem setup.",
  "Requirement ID: ISSUE-102368\nTitle: PR #32688: [XLA:GPU] Enable command buffer DynamicSliceCopyFusion command unrolling\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nPR #32688: [XLA:GPU] Enable command buffer DynamicSliceCopyFusion command unrolling\n\nImported from GitHub PR https://github.com/openxla/xla/pull/32688\n\n\ud83d\udcdd Summary of Changes\nThis PR enables command buffer DynamicSliceCopy command to be recorded into an unrolled cuda-graph, when it is surrounded by WhileCmd\n\n\n\ud83c\udfaf Justification\nThis feature is required if we want to fully command buffer WhileCmd into an unrolled cuda-graph.\n\n\n\ud83d\ude80 Kind of Contribution\nPlease remove what does not apply: \n\u2728 New Feature\n\n\n\n\ud83e\uddea Unit Tests:\nxla/backends/gpu/runtime/command_buffer_cmd_test.cc: CommandBufferCmdTest:DynamicSliceCopyFusionCmd\n\nCopybara import of the project:\n\n--\n3de87c4b611335bae736570c66cce5ee32d9cf0d by Shawn Wang <shawnw@nvidia.com>:\n\nEnable command buffer DynamicSliceCopyFusion command unrolling\n\n--\nf706dee060d1b545df6bcdbaa0e0fdfbc9af8fea by Shawn Wang <shawnw@nvidia.com>:\n\nfi xtypo:\n\nMerging this change closes #32688\n\nFUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/32688 from shawnwang18:shawnw/enable_dus_copy_unrolling f706dee060d1b545df6bcdbaa0e0fdfbc9af8fea",
  "Requirement ID: ISSUE-102367\nTitle: Add support for int2/int4 in tfl.cast\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAdd support for int2/int4 in tfl.cast",
  "Requirement ID: ISSUE-102366\nTitle: PR #32719: \u3010XLA:GPU] Command buffer DynamicSliceFusionCmd supports cuda graph loop unrolling\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nPR #32719: \u3010XLA:GPU] Command buffer DynamicSliceFusionCmd supports cuda graph loop unrolling\n\nImported from GitHub PR https://github.com/openxla/xla/pull/32719\n\n\ud83d\udcdd Summary of Changes\nThis PR enables command buffer DynamicSliceFusion command to be recorded into an unrolled cuda-graph, when it is surrounded by WhileCmd\n\n\ud83c\udfaf Justification\nThis feature is required if we want to fully command buffer WhileCmd into an unrolled cuda-graph.\n\n\ud83d\ude80 Kind of Contribution\nPlease remove what does not apply:\n\u2728 New Feature\n\n\ud83e\uddea Unit Tests:\nxla/backends/gpu/codegen/dynamic_slice_fusion_test.cc \nCopybara import of the project:\n\n--\n40489a48ed12002709a885dd9ab9f9c03d3230ca by Shawn Wang <shawnw@nvidia.com>:\n\nDynamicSliceFsuionCmd supports unrolling\n\nMerging this change closes #32719\n\nFUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/32719 from shawnwang18:shawnw/dus_unrolling 40489a48ed12002709a885dd9ab9f9c03d3230ca",
  "Requirement ID: ISSUE-102365\nTitle: [XLA:MSA] When block prefetching, finalize the original value if a sliced value is prefetched successfully and the original value is not.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[XLA:MSA] When block prefetching, finalize the original value if a sliced value is prefetched successfully and the original value is not.\n\nWe already have a pinned allocation for the original value, it should be finalized to avoid re-allocation causing multiple pinned allocations for the same buffer.",
  "Requirement ID: ISSUE-102364\nTitle: Add initial bits for YNNPACK support.\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAdd initial bits for YNNPACK support.",
  "Requirement ID: ISSUE-102363\nTitle: Remove linking libnvidia-ml.so from hermetic CUDA forward compatibility mode.\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nRemove linking libnvidia-ml.so from hermetic CUDA forward compatibility mode.\n\n`libnvidia-ml.so` version is coupled with kernel mode driver version, hence we can't provide a custom version of `libnvidia-ml.so` if the machine has a different KMD installed on it.",
  "Requirement ID: ISSUE-102362\nTitle: Disable broken se_gpu_pjrt_client_test_2gpu_b200 test\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nDisable broken se_gpu_pjrt_client_test_2gpu_b200 test",
  "Requirement ID: ISSUE-102361\nTitle: Disable `StreamExecutorGpuClientTest.CopyRawToHostOutOfRange` due to flakiness on B200.\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nDisable `StreamExecutorGpuClientTest.CopyRawToHostOutOfRange` due to flakiness on B200.",
  "Requirement ID: ISSUE-102360\nTitle: [XLA:GPU] Avoid use-after-free in StreamExecutorGpuClientTest::CopyRawToHostOutOfRange\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[XLA:GPU] Avoid use-after-free in StreamExecutorGpuClientTest::CopyRawToHostOutOfRange",
  "Requirement ID: ISSUE-102359\nTitle: Reverts 5a3a4bcd44baf08c22af3f007f9c28d75c8ec405\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nReverts 5a3a4bcd44baf08c22af3f007f9c28d75c8ec405",
  "Requirement ID: ISSUE-102358\nTitle: [XLA:GPU] Add abstract class for multicast memory to GpuExecutor.\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[XLA:GPU] Add abstract class for multicast memory to GpuExecutor.",
  "Requirement ID: ISSUE-102357\nTitle: Reverts 7dbc996979d2da847311eb28f796770cefeeb065\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nReverts 7dbc996979d2da847311eb28f796770cefeeb065",
  "Requirement ID: ISSUE-102356\nTitle: [xla:gpu] Add padding to split-k to allow pipelining.\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[xla:gpu] Add padding to split-k to allow pipelining.\n\nLoads are required to be 16-byte aligned for Triton to apply pipelining. This change adds extra padding to both split-k rewriters so that the reduction dimensions are a multiple of 16 bytes.",
  "Requirement ID: ISSUE-102355\nTitle: Automated Code Change\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102354\nTitle: abeni AI\nState: closed\nAuthor: aabeni771-ai\nLabels: comp:lite\nBody:\n# Neural network \n#102313 \n\nimport\nfrom  import layers\nimport as np\n\n# 1. Data qopheessuu (fakkeenya data x fi y)\n# x = input, y = output (fakkeenya  y = 2x + 1 barachuu)\nx = np.array([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)\ny = np.array([-1.0, 1.0, 3.0, 5.0, 7.0, 9.0], dtype=float)\n\n# 2. Model ijaarruu\nmodel = keras.Sequential([\n    layers.Dense(units=1, input_shape=[1])\n])\n\n# 3. Model leenjisuu (training)\nmodel.compile(optimizer='sgd', loss='mean_squared_error')\nmodel.fit(x, y, epochs=500, verbose=False)\n\n# 4. Test gochuu\nprint(\"Tilmaama:\", model.predict([10.0]))",
  "Requirement ID: ISSUE-102353\nTitle: Make file handling utilities compatible with files larger than 4GiB on 32 bit Windows.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nMake file handling utilities compatible with files larger than 4GiB on 32 bit Windows.\n\nThis also changes from using `_MSC_VER` to `_WIN32` to detect compilation on windows.",
  "Requirement ID: ISSUE-102352\nTitle: Return Bug find in func `tf.raw_ops.SelfAdjointEigV2()`: Tensors or just one Tensor\nState: open\nAuthor: ILCSFNO\nLabels: type:bug\nBody:\n### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.20.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04\n\n### Mobile device\n\nLinux Ubuntu 22.04\n\n### Python version\n\n3.9\n\n### Bazel version\n\nNone\n\n### GCC/compiler version\n\nNone\n\n### CUDA/cuDNN version\n\nNone\n\n### GPU model and memory\n\nNone\n\n### Current behavior?\n\nThe doc of [tf.raw_ops.SelfAdjointEigV2()](https://tensorflow.google.cn/api_docs/python/tf/raw_ops/SelfAdjointEigV2) shows its description as below:\n\nhttps://github.com/tensorflow/tensorflow/blob/2408b9968eaa2ce2c0db63718e14f377639980cf/tensorflow/core/api_def/base_api/api_def_SelfAdjointEigV2.pbtxt#L12-L23\n\nHere is an example given in the doc:\n\nhttps://github.com/tensorflow/tensorflow/blob/2408b9968eaa2ce2c0db63718e14f377639980cf/tensorflow/core/api_def/base_api/api_def_SelfAdjointEigV2.pbtxt#L37-L43\n\nBut I tried repro below, which unexpectedly failed:\n### Repro 1\n```python\nimport tensorflow as tf\ninput_matrix = tf.random.uniform(shape=[5, 5], minval=(-1.0), maxval=1.0, dtype=tf.float32)\nprint(\"1.Test with compute_v=True:\")\n(eigenvalues, eigenvectors) = tf.raw_ops.SelfAdjointEigV2(input=input_matrix, compute_v=True)\nprint(\"eigenvalues:\", eigenvalues.numpy())\nprint(\"eigenvectors:\", eigenvectors.numpy())\n\nprint(\"2.Test with compute_v=False:\")\neigenvalues = tf.raw_ops.SelfAdjointEigV2(input=input_matrix, compute_v=False)\nprint(\"eigenvalues:\", eigenvalues.numpy())\n```\n### Output 1\n```text\n1.Test with compute_v=True:\neigenvalues: [-1.0009114 -0.6460789  0.7155234  1.2467262  2.3232522]\neigenvectors: [[-0.3181498  -0.07437888  0.6905371  -0.60331917  0.2289384 ]\n [ 0.2426288   0.7176468  -0.0824292  -0.06633561  0.6441423 ]\n [-0.19543147  0.416028    0.5817248   0.62220293 -0.2513704 ]\n [ 0.8259207   0.087778    0.30651134 -0.2454011  -0.39494225]\n [ 0.34579447 -0.5465043   0.28984657  0.42924032  0.5599117 ]]\n2.Test with compute_v=False:\nAttributeError: 'SelfAdjointEigV2' object has no attribute 'numpy'\n```\n\nSo I tried the repro below, which shows that the return is actually has two tensors in, including `e` and `v`, which conflicts with the requirements in doc:\n### Repro 2\n```python\nimport tensorflow as tf\ninput_matrix = tf.random.uniform(shape=[5, 5], minval=(-1.0), maxval=1.0, dtype=tf.float32)\neigenvalues = tf.raw_ops.SelfAdjointEigV2(input=input_matrix, compute_v=False)\nprint(eigenvalues)\n```\n### Output 2\n```text\nSelfAdjointEigV2(e=<tf.Tensor: shape=(5,), dtype=float32, numpy=\narray([-1.6933075 , -1.1986178 , -0.99324477,  0.13985443,  1.9494003 ],\n      dtype=float32)>, v=<tf.Tensor: shape=(), dtype=float32, numpy=-8.247622141588684e+21>)\n```\n\nThen I tried to locate the bug and find:\n\nhttps://github.com/tensorflow/tensorflow/blob/2408b9968eaa2ce2c0db63718e14f377639980cf/tensorflow/core/kernels/linalg/self_adjoint_eig_v2_op_impl.h#L49-L57\n\nhttps://github.com/tensorflow/tensorflow/blob/2408b9968eaa2ce2c0db63718e14f377639980cf/tensorflow/core/kernels/linalg/self_adjoint_eig_v2_op_impl.h#L80-L82\n\nWe can see that both `size` and `outputs` should work well.\n\nI wonder where the bug exists in and leads to this phenomenon.\n\nThanks for noting!\n\n### Standalone code to reproduce the issue\n\n```shell\nSee repros above for not only one repro shown.\n```\n\n### Relevant log output\n\n```shell\nSee outputs above for not only one output shown.\n```",
  "Requirement ID: ISSUE-102351\nTitle: [XLA:GPU] Run hlo lit tests on several GPU platforms.\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[XLA:GPU] Run hlo lit tests on several GPU platforms.\n\nThis increases test coverage.\nAlso remove the empty test suite mlir_lit_tests. These tests have been moved to\nanother directory long ago.",
  "Requirement ID: ISSUE-102350\nTitle: Add the limitation on types in func `tf.raw_ops.Imag()`\nState: open\nAuthor: ILCSFNO\nLabels: size:XS\nBody:\nFixes #102349",
  "Requirement ID: ISSUE-102349\nTitle: Add the limitation on types in func `tf.raw_ops.Imag()`\nState: open\nAuthor: ILCSFNO\nLabels: type:docs-bug, type:bug\nBody:\n### Issue type\n\nDocumentation Bug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.20.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04\n\n### Mobile device\n\nLinux Ubuntu 22.04\n\n### Python version\n\n3.9\n\n### Bazel version\n\nNone\n\n### GCC/compiler version\n\nNone\n\n### CUDA/cuDNN version\n\nNone\n\n### GPU model and memory\n\nNone\n\n### Current behavior?\n\nThe doc of [tf.raw_ops.Imag()](https://tensorflow.google.cn/api_docs/python/tf/raw_ops/Imag) shows its description as below:\n\nhttps://github.com/tensorflow/tensorflow/blob/9a25b01c7e213f4831f21f1d36271c149bf3abd3/tensorflow/core/api_def/base_api/api_def_Imag.pbtxt#L1-L17\n\nFor repro below, I accept that it should error, but the limits for the type transferred need be noted in the docstring.\n\nThese limits are defined here:\n\nhttps://github.com/tensorflow/tensorflow/blob/9a25b01c7e213f4831f21f1d36271c149bf3abd3/tensorflow/core/kernels/mlir_generated/gpu_op_imag.cc#L22-L25\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\nimport numpy as np\ndtype, Tout = tf.complex64, tf.float64 # Fail\n# dtype, Tout = tf.complex64, tf.float32 # Success\n# dtype, Tout = tf.complex128, tf.float64 # Success\ninput_data = (np.random.rand(10, 2) + (1j * np.random.rand(10, 2)))\ninput_tensor = tf.convert_to_tensor(input_data, dtype=dtype)\nimag_parts = tf.raw_ops.Imag(input=input_tensor, Tout=Tout)\nprint(imag_parts.numpy())\n```\n\n### Relevant log output\n\n```shell\nNotFoundError: Could not find device for node: {{node Imag}} = Imag[T=DT_COMPLEX64, Tout=DT_DOUBLE]\nAll kernels registered for op Imag:\n    device='XLA_CPU_JIT'; Tout in [DT_FLOAT, DT_DOUBLE]; T in [DT_COMPLEX64, DT_COMPLEX128]\n    device='XLA_GPU_JIT'; Tout in [DT_FLOAT, DT_DOUBLE]; T in [DT_COMPLEX64, DT_COMPLEX128]\n    device='GPU'; T in [DT_COMPLEX128]; Tout in [DT_DOUBLE]\n    device='GPU'; T in [DT_COMPLEX64]; Tout in [DT_FLOAT]\n    device='CPU'; T in [DT_COMPLEX128]; Tout in [DT_DOUBLE]\n    device='CPU'; T in [DT_COMPLEX64]; Tout in [DT_FLOAT]\n[Op:Imag] name:\n```",
  "Requirement ID: ISSUE-102348\nTitle: Automated Code Change\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102347\nTitle: [XLA][XTile] Use xtile entry, extract & insert in triton emitter.\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[XLA][XTile] Use xtile entry, extract & insert in triton emitter.",
  "Requirement ID: ISSUE-102346\nTitle: [XLA:GPU] Add simple multimem one-shot example.\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[XLA:GPU] Add simple multimem one-shot example.",
  "Requirement ID: ISSUE-102345\nTitle: [XLA][XTile] Add xtile lowering passes for triton.\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[XLA][XTile] Add xtile lowering passes for triton.",
  "Requirement ID: ISSUE-102344\nTitle: Add the limitation on types transferred in func `tf.raw_ops.ComplexAbs()`\nState: open\nAuthor: ILCSFNO\nLabels: size:XS\nBody:\nFixes #102343",
  "Requirement ID: ISSUE-102343\nTitle: Add the limitation on types transferred in func `tf.raw_ops.ComplexAbs()`\nState: open\nAuthor: ILCSFNO\nLabels: type:docs-bug, type:bug\nBody:\n### Issue type\n\nDocumentation Bug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.20.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04\n\n### Mobile device\n\nLinux Ubuntu 22.04\n\n### Python version\n\n3.9\n\n### Bazel version\n\nNone\n\n### GCC/compiler version\n\nNone\n\n### CUDA/cuDNN version\n\nNone\n\n### GPU model and memory\n\nNone\n\n### Current behavior?\n\nThe doc of [tf.raw_ops.ComplexAbs()](https://tensorflow.google.cn/api_docs/python/tf/raw_ops/ComplexAbs) shows its description as below:\n\nhttps://github.com/tensorflow/tensorflow/blob/9a25b01c7e213f4831f21f1d36271c149bf3abd3/tensorflow/core/api_def/base_api/api_def_ComplexAbs.pbtxt#L1-L17\n\nFor repro below, I accept that it should error, but the limits for the type transferred need be noted in the docstring.\n\nThese limits are defined here:\n\nhttps://github.com/tensorflow/tensorflow/blob/9a25b01c7e213f4831f21f1d36271c149bf3abd3/tensorflow/core/kernels/mlir_generated/gpu_op_complex_abs.cc#L22-L25\n\nError reported is related here:\n\nhttps://github.com/tensorflow/tensorflow/blob/9a25b01c7e213f4831f21f1d36271c149bf3abd3/tensorflow/core/framework/op_kernel.cc#L90-L96\n\nhttps://github.com/tensorflow/tensorflow/blob/9a25b01c7e213f4831f21f1d36271c149bf3abd3/tensorflow/core/framework/op_kernel.cc#L285-L289\n\nhttps://github.com/tensorflow/tensorflow/blob/9a25b01c7e213f4831f21f1d36271c149bf3abd3/tensorflow/core/kernels/cwise_ops_common.h#L310-L314\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\nimport numpy as np\ndtype, Tout = tf.complex64, tf.float64 # Fail\n# dtype, Tout = tf.complex64, tf.float32 # Success\n# dtype, Tout = tf.complex128, tf.float64 # Success\nx = np.random.rand(10, 3) + (1j * np.random.rand(10, 3))\nx = tf.convert_to_tensor(np.asarray(x), dtype=dtype)\nresult = tf.raw_ops.ComplexAbs(x=x, Tout=Tout)\nprint(result)\n```\n\n### Relevant log output\n\n```shell\nInvalidArgumentError: {{function_node __wrapped__ComplexAbs_device_/job:localhost/replica:0/task:0/device:CPU:0}} Signature mismatch, have: complex64->double expected: complex64->float [Op:ComplexAbs] name:\n```",
  "Requirement ID: ISSUE-102342\nTitle: Add proto [de]serialization for `SelectKThunk`.\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAdd proto [de]serialization for `SelectKThunk`.",
  "Requirement ID: ISSUE-102341\nTitle: Automated Code Change\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102340\nTitle: Automated Code Change\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102339\nTitle: [XLA:GPU] Consider multi-output fusions supported by Triton codegen.\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[XLA:GPU] Consider multi-output fusions supported by Triton codegen.\n\nCurrently we would fail when trying to check whether the element type of the\nroot tuple is supported. We should not even access the element type on a tuple\nshape. Therefore we skip the root tuple.",
  "Requirement ID: ISSUE-102338\nTitle: Integrate Triton up to [de2ba394](https://github.com/openai/triton/commits/de2ba3946bc2a7a55ad77331ed60fd7c685156bf)\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nIntegrate Triton up to [de2ba394](https://github.com/openai/triton/commits/de2ba3946bc2a7a55ad77331ed60fd7c685156bf)\n\nhttps://github.com/openxla/triton/tree/triton_integrate_branch-",
  "Requirement ID: ISSUE-102337\nTitle: PR #32003: [GPU][NFC] Merge methods querying fusion kind.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nPR #32003: [GPU][NFC] Merge methods querying fusion kind.\n\nImported from GitHub PR https://github.com/openxla/xla/pull/32003\n\n\nCopybara import of the project:\n\n--\n2a3ad034522e871edc9c7f580e86fc3980025542 by Ilia Sergachev <isergachev@nvidia.com>:\n\n[GPU][NFC] Merge methods querying fusion kind.\n\n--\nebeb25599d6017d34ea92ece415a255d109af049 by Ilia Sergachev <isergachev@nvidia.com>:\n\nAddress review requests.\n\nMerging this change closes #32003\n\nFUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/32003 from openxla:fusion_kind_query ebeb25599d6017d34ea92ece415a255d109af049",
  "Requirement ID: ISSUE-102336\nTitle: Automated Code Change\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102335\nTitle: [XLA:GPU/WS] Adding `xla_gpu_experimental_enable_triton_warp_specialization` flag. This is currently only used to decorate the contracting dimension loop for dot fusions going through Triton with `tt.warp_specialize`, enabling the feature in Triton.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[XLA:GPU/WS] Adding `xla_gpu_experimental_enable_triton_warp_specialization` flag. This is currently only used to decorate the contracting dimension loop for dot fusions going through Triton with `tt.warp_specialize`, enabling the feature in Triton.",
  "Requirement ID: ISSUE-102334\nTitle: Refactor `SelectKThunk` to accept `ThunkInfo` instead of `HloInstruction` pointer.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nRefactor `SelectKThunk` to accept `ThunkInfo` instead of `HloInstruction` pointer.",
  "Requirement ID: ISSUE-102333\nTitle: [XLA:GPU] Provide functions to setup multicast from a single process.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[XLA:GPU] Provide functions to setup multicast from a single process.",
  "Requirement ID: ISSUE-102332\nTitle: PR #32003: [GPU][NFC] Merge methods querying fusion kind.\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nPR #32003: [GPU][NFC] Merge methods querying fusion kind.\n\nImported from GitHub PR https://github.com/openxla/xla/pull/32003\n\n\nCopybara import of the project:\n\n--\n2a3ad034522e871edc9c7f580e86fc3980025542 by Ilia Sergachev <isergachev@nvidia.com>:\n\n[GPU][NFC] Merge methods querying fusion kind.\n\n--\nebeb25599d6017d34ea92ece415a255d109af049 by Ilia Sergachev <isergachev@nvidia.com>:\n\nAddress review requests.\n\n--\n393f906488846a7aec3e2fb55e353f0fa9c0402e by Ilia Sergachev <isergachev@nvidia.com>:\n\nAdd missing build dependency.\n\nMerging this change closes #32003\n\nFUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/32003 from openxla:fusion_kind_query 393f906488846a7aec3e2fb55e353f0fa9c0402e",
  "Requirement ID: ISSUE-102331\nTitle: Automated Code Change\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102330\nTitle: Expose `Subslice` to `PjRtTopologyDescription`.\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nExpose `Subslice` to `PjRtTopologyDescription`.",
  "Requirement ID: ISSUE-102329\nTitle: Add `PjRtDeviceDimensions` struct and proto.\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAdd `PjRtDeviceDimensions` struct and proto.",
  "Requirement ID: ISSUE-102328\nTitle: Automated Code Change\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102327\nTitle: fix(core): Add MSVC patch for 64-bit integer comparison\nState: open\nAuthor: CodersAcademy006\nLabels: awaiting review, size:S\nBody:\nThis PR resolves a platform-specific bug where `int64_t` comparisons could produce incorrect results on MSVC (Windows builds) due to potential 32-bit truncation.\r\n\r\nThe Problem: Operations like `tf.minimum` and `tf.maximum` were not behaving correctly for 64-bit integers in some cases on Windows.\r\n\r\nThe Fix: This change introduces a patch that specializes the `minimum` and `maximum` functors for `int64_t` on MSVC, ensuring a true 64-bit comparison is always performed. This improves numerical stability and correctness on Windows.",
  "Requirement ID: ISSUE-102326\nTitle: Enable multi-GPU tests on B200.\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nEnable multi-GPU tests on B200.",
  "Requirement ID: ISSUE-102325\nTitle: (draft) implement SPMD DUS as select\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\n(draft) implement SPMD DUS as select",
  "Requirement ID: ISSUE-102324\nTitle: Automated Code Change\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102323\nTitle: Automated Code Change\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102322\nTitle: Automated Code Change\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102321\nTitle: Implement StreamExecutorGpuClient::ScheduleRemoteSend. This allows migrating\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nImplement StreamExecutorGpuClient::ScheduleRemoteSend. This allows migrating\nCopyToRemoteDevice to CommonPjRtBuffer APIs.",
  "Requirement ID: ISSUE-102320\nTitle: Automated Code Change\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102319\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102318\nTitle: Automated Code Change\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102317\nTitle: Automated Code Change\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102316\nTitle: Automated Code Change\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102315\nTitle: Automated Code Change\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102314\nTitle: Automated Code Change\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102313\nTitle: [xla:ffi] Add XLA_FFI_TypeInfo in preparation for adding it to TypeRegistry\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[xla:ffi] Add XLA_FFI_TypeInfo in preparation for adding it to TypeRegistry",
  "Requirement ID: ISSUE-102312\nTitle: Replace `stream->BlockHostUntilDone()` with `BlockHostUntilDoneWithHostCallback()`.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nReplace `stream->BlockHostUntilDone()` with `BlockHostUntilDoneWithHostCallback()`.\n\nBlockHostUntilDone calls `cuStreamSynchronize`, which has some performance issues.",
  "Requirement ID: ISSUE-102311\nTitle: In profile_data_lib.cc, throw std::runtime_error instead of check fail.\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nIn profile_data_lib.cc, throw std::runtime_error instead of check fail.",
  "Requirement ID: ISSUE-102310\nTitle: [XLA:CPU] Use asm to set name of intrinsic generated IR functions.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[XLA:CPU] Use asm to set name of intrinsic generated IR functions.",
  "Requirement ID: ISSUE-102309\nTitle: Fix MacOS nightly wheel builds by adding h5py version limit.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nFix MacOS nightly wheel builds by adding h5py version limit.\n\nThe new h5py releases support only MacOS 14, 15 (TF needs MacOS 10, 11).",
  "Requirement ID: ISSUE-102308\nTitle: Create cuda 12.9 image for testing\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nCreate cuda 12.9 image for testing",
  "Requirement ID: ISSUE-102307\nTitle: Sort op's first operand is now generated without duplicates if the\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nSort op's first operand is now generated without duplicates if the\nsort is stable.",
  "Requirement ID: ISSUE-102306\nTitle: [XLA:CPU] Fix intrinsic library failing when passed an already vectorized call. From Will Froom.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[XLA:CPU] Fix intrinsic library failing when passed an already vectorized call. From Will Froom.",
  "Requirement ID: ISSUE-102305\nTitle: [xla:ffi] Add TypeRegistry::TypeInfo to be able to register functions to manipulate user-defined types\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[xla:ffi] Add TypeRegistry::TypeInfo to be able to register functions to manipulate user-defined types",
  "Requirement ID: ISSUE-102304\nTitle: Allow specifying a custom session name instead of always using timestamp.\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAllow specifying a custom session name instead of always using timestamp.\n\nThis change introduces a `session_name` option in `ProfileOptions` and `RemoteProfilerSessionManagerOptions`. When provided, this name will be used as the subdirectory for storing profile data, instead of the default timestamp.",
  "Requirement ID: ISSUE-102303\nTitle: [XLA:CPU] Add test showing exp intrinsic vectorizations.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[XLA:CPU] Add test showing exp intrinsic vectorizations.\n\nThis test will serve to illustrate an upcoming change in intrinsic_lib's vectorization logic.",
  "Requirement ID: ISSUE-102302\nTitle: Add MLIR definition for `RecordEventMetricForTensor` op and model the side effect with `MemoryEffects`.\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAdd MLIR definition for `RecordEventMetricForTensor` op and model the side effect with `MemoryEffects`.",
  "Requirement ID: ISSUE-102301\nTitle: Enable placing TPU input on corresponding local CPU devices.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nEnable placing TPU input on corresponding local CPU devices.",
  "Requirement ID: ISSUE-102300\nTitle: Use hostcallback for h2d\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nUse hostcallback for h2d",
  "Requirement ID: ISSUE-102299\nTitle: Add peak private footprint memory measurement tracking\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAdd peak private footprint memory measurement tracking",
  "Requirement ID: ISSUE-102298\nTitle: Support Windows memory info\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nSupport Windows memory info\n\nMemory info is a useful class for recording memory usage across\na variety of devices. Add Windows support to it.",
  "Requirement ID: ISSUE-102297\nTitle: Support Windows metrics and private peak memory metrics\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nSupport Windows metrics and private peak memory metrics\n\nThe standard libraries now support Windows so use them instead\nof custom memory processing. Also, report the private\npeak memory metrics since some users care about them.",
  "Requirement ID: ISSUE-102296\nTitle: [XLA] Clarify an additional invariant with regards to command buffer compatibility in the FFI API.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[XLA] Clarify an additional invariant with regards to command buffer compatibility in the FFI API.",
  "Requirement ID: ISSUE-102295\nTitle: [XLA:GPU] Run `GpuKernelTilingTest`s on default GPU platforms. So far, this test is limited to Pascal.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[XLA:GPU] Run `GpuKernelTilingTest`s on default GPU platforms. So far, this test is limited to Pascal.",
  "Requirement ID: ISSUE-102294\nTitle: Use nvml impl lib based wrapper\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nUse nvml impl lib based wrapper",
  "Requirement ID: ISSUE-102293\nTitle: PR #31994: [NFC] Move computation simplification methods from command buffer scheduling to a new library.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nPR #31994: [NFC] Move computation simplification methods from command buffer scheduling to a new library.\n\nImported from GitHub PR https://github.com/openxla/xla/pull/31994\n\n\nCopybara import of the project:\n\n--\ndd037f3ef1c2da262029a9ebc34845ddb3c8a7f1 by Ilia Sergachev <isergachev@nvidia.com>:\n\n[NFC] Move computation simplification methods from command buffer scheduling to a new library.\n\n--\n2594c7a473945f5d410ae8e8894b7e90f5812c1e by Ilia Sergachev <isergachev@nvidia.com>:\n\nAddress review feedback.\n\nMerging this change closes #31994\n\nFUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/31994 from openxla:computation_canonicalizers 2594c7a473945f5d410ae8e8894b7e90f5812c1e",
  "Requirement ID: ISSUE-102292\nTitle: Reland #102094: fix 16kb alignment\nState: closed\nAuthor: mihaimaruseac\nLabels: ready to pull, size:XS\nBody:\nI accidentally merged it from the UI. Force pushed a rebase that removes it.",
  "Requirement ID: ISSUE-102291\nTitle: Internal visibility change.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nInternal visibility change.",
  "Requirement ID: ISSUE-102290\nTitle: test for presubmit workflow\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\ntest for presubmit workflow",
  "Requirement ID: ISSUE-102289\nTitle: Reverts 6e534c2dc1cd7ca12e2686cecdd845f138952d3c\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nReverts 6e534c2dc1cd7ca12e2686cecdd845f138952d3c",
  "Requirement ID: ISSUE-102288\nTitle: [XLA:GPU] Add a check that each replica group is equally distributed across hosts.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[XLA:GPU] Add a check that each replica group is equally distributed across hosts.\n\nThis way we also make sure that we don't decompose ra2a that stays within single hostl.",
  "Requirement ID: ISSUE-102287\nTitle: Provide correct `libnvidia-ml.so` version to XLA and TF GPU tests by using the docker container with CUDA driver preinstalled.\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nProvide correct `libnvidia-ml.so` version to XLA and TF GPU tests by using the docker container with CUDA driver preinstalled.\n\n`libnvidia-ml.so` version should be exactly the same as kernel mode driver version, hence we need to use Docker container with pre-installed driver (that includes `libnvidia-ml.so`) for GPU tests that use nvml implementation.",
  "Requirement ID: ISSUE-102286\nTitle: Add proto [de]serialization for `Memset32BitValueThunk`.\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAdd proto [de]serialization for `Memset32BitValueThunk`.",
  "Requirement ID: ISSUE-102285\nTitle: Adds more logging to coordination service.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAdds more logging to coordination service.",
  "Requirement ID: ISSUE-102284\nTitle: Open-source the multiprocess test runner and a subset of multiprocess tests.\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nOpen-source the multiprocess test runner and a subset of multiprocess tests.",
  "Requirement ID: ISSUE-102283\nTitle: [XLA:CPU] Erase legacy compute function path in cpu_executable.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[XLA:CPU] Erase legacy compute function path in cpu_executable.\n\nThis has been unused for a while now.",
  "Requirement ID: ISSUE-102282\nTitle: Add proto [de]serialization for CholeskyThunk.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAdd proto [de]serialization for CholeskyThunk.\n\nThe only non-obvious part of the thunk is `solver_context_creator`, but we can retrieve it during the deserialization from `stream_executor::Platform`, which is available during runtime.",
  "Requirement ID: ISSUE-102281\nTitle: [XLA:GPU] Use string literals for tarcemes\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[XLA:GPU] Use string literals for tarcemes",
  "Requirement ID: ISSUE-102280\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102279\nTitle: Add (de)serialization for `FftThunk`\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAdd (de)serialization for `FftThunk`\n\nThis one is a pretty direct mapping from the struct to the proto.",
  "Requirement ID: ISSUE-102278\nTitle: [NFC] Moving extraction utility out of fusion_emitter to emitter_helpers. Also added a test for coverage as I realize this function wasn't tested.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[NFC] Moving extraction utility out of fusion_emitter to emitter_helpers. Also added a test for coverage as I realize this function wasn't tested.\n\nMore utilities will follow as part of an upcoming change, so this refactor makes sense to land first.\n\nFUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/32003 from openxla:fusion_kind_query ebeb25599d6017d34ea92ece415a255d109af049",
  "Requirement ID: ISSUE-102277\nTitle: Introduce `HERMETIC_PYTHON_VERSION_KIND` for the Bzlmod build.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nIntroduce `HERMETIC_PYTHON_VERSION_KIND` for the Bzlmod build.\n\nAdd a placeholder for `HERMETIC_PYTHON_VERSION_KIND` in the generated `py_version.bzl` file. This new variable is currently set to an empty string until we figure out how to deal with it.",
  "Requirement ID: ISSUE-102276\nTitle: Reformat MODULE.bazel and adjust pip repo usage.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nReformat MODULE.bazel and adjust pip repo usage.\n\nThis change reorders arguments within various override and extension calls for consistency. It also removes the explicit `xla_pypi_311_numpy` from the `use_repo(pip, ...)` call.",
  "Requirement ID: ISSUE-102275\nTitle: Fix test in case when driver is old.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nFix test in case when driver is old.",
  "Requirement ID: ISSUE-102274\nTitle: Integrate Triton up to [de2ba394](https://github.com/openai/triton/commits/de2ba3946bc2a7a55ad77331ed60fd7c685156bf)\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nIntegrate Triton up to [de2ba394](https://github.com/openai/triton/commits/de2ba3946bc2a7a55ad77331ed60fd7c685156bf)\n\nhttps://github.com/openxla/triton/tree/triton_integrate_branch-",
  "Requirement ID: ISSUE-102273\nTitle: [XLA:GPU] Double the maximum unroll factor on Blackwell.\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[XLA:GPU] Double the maximum unroll factor on Blackwell.\n\nGuard this with some additional conditions, as more unrolling may cause\nperformance issues that are not offset with more vectorization.",
  "Requirement ID: ISSUE-102272\nTitle: Fix AArch64 CPUIDInfo init\nState: open\nAuthor: davsva01\nLabels: size:XS\nBody:\nFixes an issue where CPUIDInfo was being created without initializing parameters. Result was AArch64 builds falling back to Eigen for many ops which should now be fixed.",
  "Requirement ID: ISSUE-102271\nTitle: Move Attribute types from call_frame.cc into attribute_map.cc\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nMove Attribute types from call_frame.cc into attribute_map.cc\n\nThis is moving `Scalar`, `Array`, `Dictionary`, `FlatAttribute`, `FlatAttributeMap`, and `AttributeMap` from `CallFrameBuilder` into the `xla::ffi` namespace.\n\nIt also moves the code into `attribute_map.{cc|h}`.\n\nAll these types are basically aliases from some kind of `std::variant` type. This change is a preparation for making them proper types and add `ToProto` and `FromProto` methods.",
  "Requirement ID: ISSUE-102270\nTitle: Clean up `FftThunk` includes and BUILD dependencies\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nClean up `FftThunk` includes and BUILD dependencies\n\nDoing this before touching the files, also adding missing brackets in an if.",
  "Requirement ID: ISSUE-102269\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102268\nTitle: Use `size_t` for byte and element counts\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nUse `size_t` for byte and element counts\n\nChanges `bytes_len` and `elem_count` to `size_t` to match the return type of `bytes.size()` and handle potentially large sizes correctly.",
  "Requirement ID: ISSUE-102267\nTitle: test. split. xla vs shardy.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\ntest. split. xla vs shardy.",
  "Requirement ID: ISSUE-102266\nTitle: Add (de)serialization for the `ConvolutionReorderThunk`\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAdd (de)serialization for the `ConvolutionReorderThunk`",
  "Requirement ID: ISSUE-102265\nTitle: [XLA:GPU] Make sure to simplify both lhs and rhs of convolution.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[XLA:GPU] Make sure to simplify both lhs and rhs of convolution.\n\nWe need to ensure that symbols for trivial dimensions are simplified away\nconsistently. If we simplify it on one side, we also need to simplify it on\nthe other, as we want to use the same iteration space for both lhs and rhs.",
  "Requirement ID: ISSUE-102264\nTitle: Refactor `ConvolutionReorderThunk` member fields\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nRefactor `ConvolutionReorderThunk` member fields\n\nIn practice the thunk always has:\n* an input and output filter\n* either:\n  * no biases\n  * both an input and output bias\n\nSo specify this invariant into the data structure, to make this more readable and to make it harder to create an invalid thunk.",
  "Requirement ID: ISSUE-102263\nTitle: PR #32454: Respect debug options override in LHS.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nPR #32454: Respect debug options override in LHS.\n\nImported from GitHub PR https://github.com/openxla/xla/pull/32454\n\nAny place inside the compiler has to respect the debug options override set by the users. \n\nCopybara import of the project:\n\n--\n6971175737582aad4e9256f983890ac04009a074 by Yunlong Liu <yliu120@users.noreply.github.com>:\n\nRespect debug options override in LHS.\n\nMerging this change closes #32454\n\nFUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/32454 from yliu120:patch-5 6971175737582aad4e9256f983890ac04009a074",
  "Requirement ID: ISSUE-102262\nTitle: PR #32283: [ROCm] Change misleading method name RocmComputeCapability::has_amd_matrix_core()\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nPR #32283: [ROCm] Change misleading method name RocmComputeCapability::has_amd_matrix_core()\n\nImported from GitHub PR https://github.com/openxla/xla/pull/32283\n\n\ud83d\udcdd Summary of Changes\nChange misleading method name RocmComputeCapability::has_amd_matrix_core() to more suitable name has_amd_mat_acc_instructions() as gfx11xx do not have matrix cores, but support matrix acceleration instruction set known as WMMA.\n\n\ud83c\udfaf Justification\nRocmComputeCapability::has_amd_matrix_core() is misleading as gfx11xx do not have matrix cores but still support matrix acceleration instruction set - WMMA.\n\n\ud83d\ude80 Kind of Contribution\n\u267b\ufe0f Cleanup\n\n@xla-rotation please review my changes. \n\nCopybara import of the project:\n\n--\n23cf1ab79fdcc4ee2ee4996973dee2c103d2762a by Aleksa Arsic <aleksa.arsic@amd.com>:\n\nChange misleading method name RocmComputeCapability::has_amd_matrix_core() to more suitable name has_amd_mat_acc_instructions() as gfx11xx do not have matrix cores, but support matrix acceleration instruction set known as WMMA.\n\nMerging this change closes #32283\n\nFUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/32283 from ROCm:ci_fix-misleading-method-name-has-amd-mat-core 23cf1ab79fdcc4ee2ee4996973dee2c103d2762a",
  "Requirement ID: ISSUE-102261\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102260\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102259\nTitle: Fix incorrect python interpreter path of non-bzlmod\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nFix incorrect python interpreter path of non-bzlmod\n\nRemove duplicated \"_host\" suffix.",
  "Requirement ID: ISSUE-102258\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102257\nTitle: Convert to nanobind.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nConvert to nanobind.\n\nThis just converts but doesn't rename to enable cleaner diff.",
  "Requirement ID: ISSUE-102256\nTitle: [xla:ffi] Add support for returning TypeId for stateful handlers\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[xla:ffi] Add support for returning TypeId for stateful handlers",
  "Requirement ID: ISSUE-102255\nTitle: [XLA] Implement a `TiledHloSchedule` that transposes the iteration order over the non-contracting dimensions of a `dot`.\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[XLA] Implement a `TiledHloSchedule` that transposes the iteration order over the non-contracting dimensions of a `dot`.\n\nA concrete use case when such a schedule is useful is when we have a matrix\nmultiplication such that a chunk of shape `(block_m, k)` of the left-hand\nside argument fully fits into L2. The transposed iteration order will step\nthrough the `n` dimension first, allowing to hit L2 cache more often when\nloading tiles of the left-hand side.\n\nThis schedule is intentionally restricted at the moment in order to unblock\nlaunching the generic Triton emitter for GEMMs.",
  "Requirement ID: ISSUE-102254\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102253\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102252\nTitle: Automated Code Change\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102251\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102250\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102249\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102248\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102247\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102246\nTitle: Automated Code Change\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102245\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102244\nTitle: [XLA:GPU] Support arbitrary replica groups in RaggedAllToAlMultiHostDecomposer.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[XLA:GPU] Support arbitrary replica groups in RaggedAllToAlMultiHostDecomposer.\n\nThis change lifts the original restriction that ra2a should have only one iota replica group.",
  "Requirement ID: ISSUE-102243\nTitle: [PjRt-IFRT] `ifrt::PjRtArray::pjrt_layout()` uses `nullptr` to indicate a default layout\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[PjRt-IFRT] `ifrt::PjRtArray::pjrt_layout()` uses `nullptr` to indicate a default layout\n\nPjRt-IFRT now returns a `nullptr` if it knows that the Array layout represents a default layout. The user code previously has been migrated to handle this new behavior gracefully, obtaining a concrete default layout as before.\n\n`ifrt::PjRtArray` creation now request extra information on whether the underlying `PjRtBuffer` is using a custom layout as IFRT tracks the defaultness of array layouts. This information cannot be inferred correctly from `PjRtBuffer` alone because `PjRtBuffer::layout()` only returns a concrete layout. PjRt would mostly work fine today if a default layout is said to be a custom layout, but some strict layout equality check can fail and require more precise information to be supplied.\n\nA few test cases in IFRT ArrayImplTest against PjRt CPU and GPU clients\nhave been disabled because the output array does not track the\nnon-default-ness of the layout correctly when\n`MakeArraysFromHostBufferShards()` is implemented using\n`ClientMakeArraysFromHostBufferShards()`.",
  "Requirement ID: ISSUE-102242\nTitle: Update PjRtStreamExecutorRawBuffer::CopyRawHostToDeviceAndReturnEvent to\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nUpdate PjRtStreamExecutorRawBuffer::CopyRawHostToDeviceAndReturnEvent to\nsupport staging host buffers (for non-pinned memory). This allows replacing the CopyRawToHost functions.",
  "Requirement ID: ISSUE-102241\nTitle: [XLA] Allow fine grain SparseCore offloading\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[XLA] Allow fine grain SparseCore offloading\n\nToday any kind of SparseCore offloading is done as all or nothing, where this behaviour is controlled by a global flag. This is not always desirable, however the existing `compute_on` annotation to place ops on SparseCore was designed for embeddings in mind during TF days.\n\nIntroduce a new `compute_on` annotation `tpu_sparsecore_offload`, which will inform the compiler what ops should be offloaded. Note that we only support a handfull of ops for offloading and this method requires the offload annotation to survive the entire compilation pipeline, as offloading happens as late as possible. In case the annotation survives and the op (or whatever is left of it) was not offloaded, a compilation error will be returned.",
  "Requirement ID: ISSUE-102240\nTitle: Add more precise stream synchronization which allows more aggressive stream\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAdd more precise stream synchronization which allows more aggressive stream\nsynchronization in raw buffer APIs (namely CopyRawHostToDeviceAndReturnEvent\nand CopyRawDeviceToHostAndReturnEvent). Old buffers will require no\nsynchronization, but recent buffers will get a cached compute_stream event\nand then it will sync with this compute_stream event repeatedly instead of\nsyncing with the stream itself.",
  "Requirement ID: ISSUE-102239\nTitle: [IFRT] Update the semantics of `ifrt::Array::pjrt_layout()` regarding default layouts\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[IFRT] Update the semantics of `ifrt::Array::pjrt_layout()` regarding default layouts\n\n`ifrt::Array::pjrt_layout()` will soon be returning a `nullptr` for a default layout (soon with removal of `absl::StatusOr<>` part). The user can continue to get a concrete default layout via `ifrt::Client::GetDefaultPjRtLayout()`.\n\nDuring a transition, IFRT implementations may return either `nullptr` or a concrete default layout, and this state will be permitted temporarily, while they will be migrated to return `nullptr` for default layouts.",
  "Requirement ID: ISSUE-102238\nTitle: Sort filenames to ensure deterministic test results\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nSort filenames to ensure deterministic test results",
  "Requirement ID: ISSUE-102237\nTitle: [XLA] Make gSPMD preserve frontend attributes\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[XLA] Make gSPMD preserve frontend attributes\n\nSDY does not preserve backend configs, while gSPMD does not preserve frontend attributes. This is a pick your poison situation, but the later is easier to fix.",
  "Requirement ID: ISSUE-102236\nTitle: Integrate LLVM at llvm/llvm-project@267fa8dd1efc\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nIntegrate LLVM at llvm/llvm-project@267fa8dd1efc\n\nUpdates LLVM usage to match\n[267fa8dd1efc](https://github.com/llvm/llvm-project/commit/267fa8dd1efc)",
  "Requirement ID: ISSUE-102235\nTitle: [XLA:GPU] Add verbose tracing for BlockHostUntilDone and stream synchronization\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[XLA:GPU] Add verbose tracing for BlockHostUntilDone and stream synchronization",
  "Requirement ID: ISSUE-102234\nTitle: Add `async_compute_instr_name` into backend_config to correlate between the true latency from `events_db` and estimated latency from `hlo_ops_db`\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAdd `async_compute_instr_name` into backend_config to correlate between the true latency from `events_db` and estimated latency from `hlo_ops_db`",
  "Requirement ID: ISSUE-102233\nTitle: Bump XNNPACK version for open source builds.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nBump XNNPACK version for open source builds.",
  "Requirement ID: ISSUE-102232\nTitle: Integrate LLVM at llvm/llvm-project@267fa8dd1efc\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nIntegrate LLVM at llvm/llvm-project@267fa8dd1efc\n\nUpdates LLVM usage to match\n[267fa8dd1efc](https://github.com/llvm/llvm-project/commit/267fa8dd1efc)",
  "Requirement ID: ISSUE-102231\nTitle: refactor example plugin to allow reuse of testing\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nrefactor example plugin to allow reuse of testing",
  "Requirement ID: ISSUE-102230\nTitle: PR #32642: [ROCm] Fix invalid run_under script for ci job and asan ignore files\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nPR #32642: [ROCm] Fix invalid run_under script for ci job and asan ignore files\n\nImported from GitHub PR https://github.com/openxla/xla/pull/32642\n\n\ud83d\udcdd Summary of Changes\nFix rocm build with asan settings\n\n\ud83c\udfaf Justification\nFix invalid run under script used in order to pass through the asan ignore lists file.\n\n\ud83d\ude80 Kind of Contribution\nPlease remove what does not apply: \ud83d\udc1b Bug Fix\n\ud83d\udcca Benchmark (for Performance Improvements)\nNot relevant\n\n\ud83e\uddea Unit Tests:\nNot relevant\n\n\ud83e\uddea Execution Tests:\nNot relevant\n\nCopybara import of the project:\n\n--\n708b0b274d18b88ca7467c3ab3f44aaa11710995 by Alexandros Theodoridis <atheodor@amd.com>:\n\nFix invalid run_under script for ci job and asan ignore files\n\nMerging this change closes #32642\n\nFUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/32642 from ROCm:ci_fix_build_break_due_to_invalid_run_under_script 708b0b274d18b88ca7467c3ab3f44aaa11710995",
  "Requirement ID: ISSUE-102229\nTitle: Make ApproxTopK Op don't fail with kMhloFrontendAttributes\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nMake ApproxTopK Op don't fail with kMhloFrontendAttributes",
  "Requirement ID: ISSUE-102228\nTitle: Rolling forward with fix.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nRolling forward with fix.\n\nReverts eefde231948bd6bee00f1d8f9e521b0f7ce22e50",
  "Requirement ID: ISSUE-102227\nTitle: Add B200 GPU spec to XLA GPU device info tests.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAdd B200 GPU spec to XLA GPU device info tests.",
  "Requirement ID: ISSUE-102226\nTitle: Update BUILD dependencies for `pjrt_c_api_client`.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nUpdate BUILD dependencies for `pjrt_c_api_client`.\n\nThe `pjrt_c_api_client` target has been moved from `//third_party/tensorflow/compiler/xla/pjrt` to `//third_party/tensorflow/compiler/xla/pjrt/c_api_client`. This CL updates all affected BUILD files to use the new path.",
  "Requirement ID: ISSUE-102225\nTitle: Make the gpu_static_registration work properly\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nMake the gpu_static_registration work properly\n\ngpu_static_registration is intended to pull in all dependencies needed for the target - this ensures that this is the case.",
  "Requirement ID: ISSUE-102224\nTitle: Update CuDNN FrontEnd to match the used one with XLA that is compatible with CUDA 13\nState: open\nAuthor: medaminezghal\nLabels: size:XS\nBody:\nThis will resolve this issue that occurs when building using  CuDNN FrontEnd 1.11.0 and CUDA 13 (the minimum version needed to work with it is [1.12.0](https://github.com/NVIDIA/cudnn-frontend/releases/tag/v1.12.0)):\r\n```\r\nERROR: /home/medaminezghal/.cache/bazel/_bazel_medaminezghal/26cbd07b274e70c8d776ff5c3b4b73d8/external/local_xla/xla/stream_executor/cuda/BUILD:520:11: Compiling xla/stream_executor/cuda/cuda_dnn.cc [for tool] failed: (Exit 1): nvcc_wrapper failed: error executing CppCompile command (from target @@local_xla//xla/stream_executor/cuda:cudnn_plugin) (cd /home/medaminezghal/.cache/bazel/_bazel_medaminezghal/26cbd07b274e70c8d776ff5c3b4b73d8/execroot/org_tensorflow && \\ exec env - \\ AR_PATH=external/llvm18_linux_x86_64/bin/llvm-ar \\ CPP_PATH=external/llvm18_linux_x86_64/bin/clang++ \\ GCC_PATH=external/llvm18_linux_x86_64/bin/clang \\ LD_PATH=external/llvm18_linux_x86_64/bin/ld.lld \\ NVCC_PATH=external/cuda_nvcc/bin/nvcc \\ NVCC_VERSION=13.0.2 \\ PATH=/home/medaminezghal/Desktop/tensorflow/src/bazel:/usr/local/sbin:/usr/local/bin:/usr/bin:/opt/cuda/bin:/opt/cuda/nsight_compute:/opt/cuda/nsight_systems/bin:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl \\ PWD=/proc/self/cwd \\ external/rules_ml_toolchain/cc/impls/linux_x86_64_linux_x86_64_cuda/wrappers/nvcc_wrapper -MD -MF bazel-out/k8-opt-exec-ST-0465588ec812/bin/external/local_xla/xla/stream_executor/cuda/_objs/cudnn_plugin/cuda_dnn.d '-frandom-seed=bazel-out/k8-opt-exec-ST-0465588ec812/bin/external/local_xla/xla/stream_executor/cuda/_objs/cudnn_plugin/cuda_dnn.o' '-DGOOGLE_PROTOBUF_USING_BAZEL=1' '-DEIGEN_MAX_ALIGN_BYTES=64' -DEIGEN_ALLOW_UNALIGNED_SCALARS '-DEIGEN_USE_AVX512_GEMM_KERNELS=0' -DHAVE_SYS_UIO_H -DTF_USE_SNAPPY -DTENSORFLOW_USE_NUMA '-DLLVM_ON_UNIX=1' '-DHAVE_BACKTRACE=1' '-DBACKTRACE_HEADER=<execinfo.h>' '-DLTDL_SHLIB_EXT=\".so\"' '-DLLVM_PLUGIN_EXT=\".so\"' '-DLLVM_ENABLE_LLVM_EXPORT_ANNOTATIONS=1' '-DLLVM_ENABLE_PLUGINS=1' '-DLLVM_ENABLE_THREADS=1' '-DHAVE_DEREGISTER_FRAME=1' '-DHAVE_LIBPTHREAD=1' '-DHAVE_PTHREAD_GETNAME_NP=1' '-DHAVE_PTHREAD_H=1' '-DHAVE_PTHREAD_SETNAME_NP=1' '-DHAVE_REGISTER_FRAME=1' '-DHAVE_SETENV_R=1' '-DHAVE_STRERROR_R=1' '-DHAVE_SYSEXITS_H=1' '-DHAVE_SYS_IOCTL_H=1' '-DHAVE_UNISTD_H=1' -D_GNU_SOURCE '-DHAVE_GETAUXVAL=1' '-DHAVE_MALLINFO=1' '-DHAVE_SBRK=1' '-DHAVE_STRUCT_STAT_ST_MTIM_TV_NSEC=1' -DHAVE_BUILTIN_THREAD_POINTER '-DLLVM_NATIVE_ARCH=\"X86\"' '-DLLVM_NATIVE_ASMPARSER=LLVMInitializeX86AsmParser' '-DLLVM_NATIVE_ASMPRINTER=LLVMInitializeX86AsmPrinter' '-DLLVM_NATIVE_DISASSEMBLER=LLVMInitializeX86Disassembler' '-DLLVM_NATIVE_TARGET=LLVMInitializeX86Target' '-DLLVM_NATIVE_TARGETINFO=LLVMInitializeX86TargetInfo' '-DLLVM_NATIVE_TARGETMC=LLVMInitializeX86TargetMC' '-DLLVM_NATIVE_TARGETMCA=LLVMInitializeX86TargetMCA' '-DLLVM_HOST_TRIPLE=\"x86_64-unknown-linux-gnu\"' '-DLLVM_DEFAULT_TARGET_TRIPLE=\"x86_64-unknown-linux-gnu\"' '-DLLVM_VERSION_MAJOR=22' '-DLLVM_VERSION_MINOR=0' '-DLLVM_VERSION_PATCH=0' '-DLLVM_VERSION_STRING=\"22.0.0git\"' -D__STDC_LIMIT_MACROS -D__STDC_CONSTANT_MACROS -D__STDC_FORMAT_MACROS '-DLLVM_HAS_AArch64_TARGET=1' '-DLLVM_HAS_AMDGPU_TARGET=1' '-DLLVM_HAS_ARM_TARGET=1' '-DLLVM_HAS_NVPTX_TARGET=1' '-DLLVM_HAS_PowerPC_TARGET=1' '-DLLVM_HAS_RISCV_TARGET=1' '-DLLVM_HAS_SystemZ_TARGET=1' '-DLLVM_HAS_X86_TARGET=1' '-DLLVM_HAS_SPIRV_TARGET=1' '-DBLAKE3_USE_NEON=0' -DBLAKE3_NO_AVX2 -DBLAKE3_NO_AVX512 -DBLAKE3_NO_SSE2 -DBLAKE3_NO_SSE41 -iquote external/local_xla -iquote bazel-out/k8-opt-exec-ST-0465588ec812/bin/external/local_xla -iquote external/com_google_protobuf -iquote bazel-out/k8-opt-exec-ST-0465588ec812/bin/external/com_google_protobuf -iquote external/com_google_absl -iquote bazel-out/k8-opt-exec-ST-0465588ec812/bin/external/com_google_absl -iquote external/zlib -iquote bazel-out/k8-opt-exec-ST-0465588ec812/bin/external/zlib -iquote external/local_tsl -iquote bazel-out/k8-opt-exec-ST-0465588ec812/bin/external/local_tsl -iquote external/eigen_archive -iquote bazel-out/k8-opt-exec-ST-0465588ec812/bin/external/eigen_archive -iquote external/ml_dtypes_py -iquote bazel-out/k8-opt-exec-ST-0465588ec812/bin/external/ml_dtypes_py -iquote external/snappy -iquote bazel-out/k8-opt-exec-ST-0465588ec812/bin/external/snappy -iquote external/hwloc -iquote bazel-out/k8-opt-exec-ST-0465588ec812/bin/external/hwloc -iquote external/com_googlesource_code_re2 -iquote bazel-out/k8-opt-exec-ST-0465588ec812/bin/external/com_googlesource_code_re2 -iquote external/local_config_cuda -iquote bazel-out/k8-opt-exec-ST-0465588ec812/bin/external/local_config_cuda -iquote external/cuda_cudart -iquote bazel-out/k8-opt-exec-ST-0465588ec812/bin/external/cuda_cudart -iquote external/cuda_cublas -iquote bazel-out/k8-opt-exec-ST-0465588ec812/bin/external/cuda_cublas -iquote external/cuda_cccl -iquote bazel-out/k8-opt-exec-ST-0465588ec812/bin/external/cuda_cccl -iquote external/cuda_nvtx -iquote bazel-out/k8-opt-exec-ST-0465588ec812/bin/external/cuda_nvtx -iquote external/cuda_nvcc -iquote bazel-out/k8-opt-exec-ST-0465588ec812/bin/external/cuda_nvcc -iquote external/cuda_cusolver -iquote bazel-out/k8-opt-exec-ST-0465588ec812/bin/external/cuda_cusolver -iquote external/cuda_cufft -iquote bazel-out/k8-opt-exec-ST-0465588ec812/bin/external/cuda_cufft -iquote external/cuda_cusparse -iquote bazel-out/k8-opt-exec-ST-0465588ec812/bin/external/cuda_cusparse -iquote external/cuda_curand -iquote bazel-out/k8-opt-exec-ST-0465588ec812/bin/external/cuda_curand -iquote external/cuda_cupti -iquote bazel-out/k8-opt-exec-ST-0465588ec812/bin/external/cuda_cupti -iquote external/cuda_nvml -iquote bazel-out/k8-opt-exec-ST-0465588ec812/bin/external/cuda_nvml -iquote external/cuda_nvjitlink -iquote bazel-out/k8-opt-exec-ST-0465588ec812/bin/external/cuda_nvjitlink -iquote external/cuda_crt -iquote bazel-out/k8-opt-exec-ST-0465588ec812/bin/external/cuda_crt -iquote external/cuda_cudnn -iquote bazel-out/k8-opt-exec-ST-0465588ec812/bin/external/cuda_cudnn -iquote external/highwayhash -iquote bazel-out/k8-opt-exec-ST-0465588ec812/bin/external/highwayhash -iquote external/farmhash_archive -iquote bazel-out/k8-opt-exec-ST-0465588ec812/bin/external/farmhash_archive -iquote external/llvm-project -iquote bazel-out/k8-opt-exec-ST-0465588ec812/bin/external/llvm-project -iquote external/cudnn_frontend_archive -iquote bazel-out/k8-opt-exec-ST-0465588ec812/bin/external/cudnn_frontend_archive -iquote external/local_config_tensorrt -iquote bazel-out/k8-opt-exec-ST-0465588ec812/bin/external/local_config_tensorrt -iquote external/nvshmem -iquote bazel-out/k8-opt-exec-ST-0465588ec812/bin/external/nvshmem -iquote external/local_config_nccl -iquote bazel-out/k8-opt-exec-ST-0465588ec812/bin/external/local_config_nccl -Ibazel-out/k8-opt-exec-ST-0465588ec812/bin/external/com_google_protobuf/src/google/protobuf/_virtual_includes/protobuf -Ibazel-out/k8-opt-exec-ST-0465588ec812/bin/external/com_google_protobuf/src/google/protobuf/_virtual_includes/internal_visibility -Ibazel-out/k8-opt-exec-ST-0465588ec812/bin/external/com_google_protobuf/src/google/protobuf/_virtual_includes/micro_string -Ibazel-out/k8-opt-exec-ST-0465588ec812/bin/external/com_google_protobuf/src/google/protobuf/_virtual_includes/arena -Ibazel-out/k8-opt-exec-ST-0465588ec812/bin/external/com_google_protobuf/src/google/protobuf/_virtual_includes/arena_align -Ibazel-out/k8-opt-exec-ST-0465588ec812/bin/external/com_google_protobuf/src/google/protobuf/_virtual_includes/port -Ibazel-out/k8-opt-exec-ST-0465588ec812/bin/external/com_google_protobuf/src/google/protobuf/stubs/_virtual_includes/lite -Ibazel-out/k8-opt-exec-ST-0465588ec812/bin/external/com_google_protobuf/src/google/protobuf/_virtual_includes/arena_allocation_policy -Ibazel-out/k8-opt-exec-ST-0465588ec812/bin/external/com_google_protobuf/src/google/protobuf/_virtual_includes/arena_cleanup -Ibazel-out/k8-opt-exec-ST-0465588ec812/bin/external/com_google_protobuf/src/google/protobuf/_virtual_includes/string_block -Ibazel-out/k8-opt-exec-ST-0465588ec812/bin/external/com_google_protobuf/src/google/protobuf/_virtual_includes/protobuf_lite -Ibazel-out/k8-opt-exec-ST-0465588ec812/bin/external/com_google_protobuf/src/google/protobuf/_virtual_includes/endian -Ibazel-out/k8-opt-exec-ST-0465588ec812/bin/external/com_google_protobuf/src/google/protobuf/_virtual_includes/varint_shuffle -Ibazel-out/k8-opt-exec-ST-0465588ec812/bin/external/com_google_protobuf/src/google/protobuf/io/_virtual_includes/io -Ibazel-out/k8-opt-exec-ST-0465588ec812/bin/external/com_google_protobuf/src/google/protobuf/io/_virtual_includes/io_win32 -Ibazel-out/k8-opt-exec-ST-0465588ec812/bin/external/com_google_protobuf/third_party/utf8_range/_virtual_includes/utf8_validity -Ibazel-out/k8-opt-exec-ST-0465588ec812/bin/external/com_google_protobuf/third_party/utf8_range/_virtual_includes/utf8_range -Ibazel-out/k8-opt-exec-ST-0465588ec812/bin/external/com_google_protobuf/src/google/protobuf/io/_virtual_includes/gzip_stream -Ibazel-out/k8-opt-exec-ST-0465588ec812/bin/external/com_google_protobuf/src/google/protobuf/stubs/_virtual_includes/stubs -Ibazel-out/k8-opt-exec-ST-0465588ec812/bin/external/com_google_protobuf/src/google/protobuf/io/_virtual_includes/printer -Ibazel-out/k8-opt-exec-ST-0465588ec812/bin/external/com_google_protobuf/src/google/protobuf/io/_virtual_includes/zero_copy_sink -Ibazel-out/k8-opt-exec-ST-0465588ec812/bin/external/com_google_protobuf/src/google/protobuf/io/_virtual_includes/tokenizer -Ibazel-out/k8-opt-exec-ST-0465588ec812/bin/external/com_google_protobuf/src/google/protobuf/_virtual_includes/any_proto -Ibazel-out/k8-opt-exec-ST-0465588ec812/bin/external/com_google_protobuf/src/google/protobuf/_virtual_includes/api_proto -Ibazel-out/k8-opt-exec-ST-0465588ec812/bin/external/com_google_protobuf/src/google/protobuf/_virtual_includes/source_context_proto -Ibazel-out/k8-opt-exec-ST-0465588ec812/bin/external/com_google_protobuf/src/google/protobuf/_virtual_includes/type_proto -Ibazel-out/k8-opt-exec-ST-0465588ec812/bin/external/com_google_protobuf/src/google/protobuf/compiler/_virtual_includes/plugin_proto -Ibazel-out/k8-opt-exec-ST-0465588ec812/bin/external/com_google_protobuf/src/google/protobuf/_virtual_includes/descriptor_proto -Ibazel-out/k8-opt-exec-ST-0465588ec812/bin/external/com_google_protobuf/src/google/protobuf/_virtual_includes/duration_proto -Ibazel-out/k8-opt-exec-ST-0465588ec812/bin/external/com_google_protobuf/src/google/protobuf/_virtual_includes/empty_proto -Ibazel-out/k8-opt-exec-ST-0465588ec812/bin/external/com_google_protobuf/src/google/protobuf/_virtual_includes/field_mask_proto -Ibazel-out/k8-opt-exec-ST-0465588ec812/bin/external/com_google_protobuf/src/google/protobuf/_virtual_includes/struct_proto -Ibazel-out/k8-opt-exec-ST-0465588ec812/bin/external/com_google_protobuf/src/google/protobuf/_virtual_includes/timestamp_proto -Ibazel-out/k8-opt-exec-ST-0465588ec812/bin/external/com_google_protobuf/src/google/protobuf/_virtual_includes/wrappers_proto -Ibazel-out/k8-opt-exec-ST-0465588ec812/bin/external/com_google_protobuf/src/google/protobuf/_virtual_includes/protobuf_layering_check_legacy -Ibazel-out/k8-opt-exec-ST-0465588ec812/bin/external/com_google_protobuf/src/google/protobuf/compiler/_virtual_includes/importer -Ibazel-out/k8-opt-exec-ST-0465588ec812/bin/external/com_google_protobuf/src/google/protobuf/util/_virtual_includes/delimited_message_util -Ibazel-out/k8-opt-exec-ST-0465588ec812/bin/external/com_google_protobuf/src/google/protobuf/util/_virtual_includes/differencer -Ibazel-out/k8-opt-exec-ST-0465588ec812/bin/external/com_google_protobuf/src/google/protobuf/util/_virtual_includes/field_mask_util -Ibazel-out/k8-opt-exec-ST-0465588ec812/bin/external/com_google_protobuf/src/google/protobuf/util/_virtual_includes/json_util -Ibazel-out/k8-opt-exec-ST-0465588ec812/bin/external/com_google_protobuf/src/google/protobuf/json/_virtual_includes/json -Ibazel-out/k8-opt-exec-ST-0465588ec812/bin/external/com_google_protobuf/src/google/protobuf/json/_virtual_includes/parser -Ibazel-out/k8-opt-exec-ST-0465588ec812/bin/external/com_google_protobuf/src/google/protobuf/json/_virtual_includes/descriptor_traits -Ibazel-out/k8-opt-exec-ST-0465588ec812/bin/external/com_google_protobuf/src/google/protobuf/json/_virtual_includes/lexer -Ibazel-out/k8-opt-exec-ST-0465588ec812/bin/external/com_google_protobuf/src/google/protobuf/json/_virtual_includes/message_path -Ibazel-out/k8-opt-exec-ST-0465588ec812/bin/external/com_google_protobuf/src/google/protobuf/json/_virtual_includes/zero_copy_buffered_stream -Ibazel-out/k8-opt-exec-ST-0465588ec812/bin/external/com_google_protobuf/src/google/protobuf/json/_virtual_includes/untyped_message -Ibazel-out/k8-opt-exec-ST-0465588ec812/bin/external/com_google_protobuf/src/google/protobuf/util/_virtual_includes/type_resolver -Ibazel-out/k8-opt-exec-ST-0465588ec812/bin/external/com_google_protobuf/src/google/protobuf/_virtual_includes/descriptor_legacy -Ibazel-out/k8-opt-exec-ST-0465588ec812/bin/external/com_google_protobuf/src/google/protobuf/json/_virtual_includes/unparser -Ibazel-out/k8-opt-exec-ST-0465588ec812/bin/external/com_google_protobuf/src/google/protobuf/json/_virtual_includes/writer -Ibazel-out/k8-opt-exec-ST-0465588ec812/bin/external/com_google_protobuf/src/google/protobuf/util/_virtual_includes/time_util -Ibazel-out/k8-opt-exec-ST-0465588ec812/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers -Ibazel-out/k8-opt-exec-ST-0465588ec812/bin/external/cuda_cudart/_virtual_includes/headers -Ibazel-out/k8-opt-exec-ST-0465588ec812/bin/external/cuda_cublas/_virtual_includes/headers -Ibazel-out/k8-opt-exec-ST-0465588ec812/bin/external/cuda_cccl/_virtual_includes/headers -Ibazel-out/k8-opt-exec-ST-0465588ec812/bin/external/cuda_nvtx/_virtual_includes/headers -Ibazel-out/k8-opt-exec-ST-0465588ec812/bin/external/cuda_nvcc/_virtual_includes/headers -Ibazel-out/k8-opt-exec-ST-0465588ec812/bin/external/cuda_cusolver/_virtual_includes/headers -Ibazel-out/k8-opt-exec-ST-0465588ec812/bin/external/cuda_cufft/_virtual_includes/headers -Ibazel-out/k8-opt-exec-ST-0465588ec812/bin/external/cuda_cusparse/_virtual_includes/headers -Ibazel-out/k8-opt-exec-ST-0465588ec812/bin/external/cuda_curand/_virtual_includes/headers -Ibazel-out/k8-opt-exec-ST-0465588ec812/bin/external/cuda_cupti/_virtual_includes/headers -Ibazel-out/k8-opt-exec-ST-0465588ec812/bin/external/cuda_nvml/_virtual_includes/headers -Ibazel-out/k8-opt-exec-ST-0465588ec812/bin/external/cuda_nvjitlink/_virtual_includes/headers -Ibazel-out/k8-opt-exec-ST-0465588ec812/bin/external/cuda_crt/_virtual_includes/headers -Ibazel-out/k8-opt-exec-ST-0465588ec812/bin/external/cuda_cudnn/_virtual_includes/headers -Ibazel-out/k8-opt-exec-ST-0465588ec812/bin/external/llvm-project/third-party/siphash/_virtual_includes/siphash -Ibazel-out/k8-opt-exec-ST-0465588ec812/bin/external/cudnn_frontend_archive/_virtual_includes/cudnn_frontend -Ibazel-out/k8-opt-exec-ST-0465588ec812/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers -Ibazel-out/k8-opt-exec-ST-0465588ec812/bin/external/nvshmem/_virtual_includes/nvshmem_config -Ibazel-out/k8-opt-exec-ST-0465588ec812/bin/external/local_config_nccl/_virtual_includes/hermetic_nccl_config -isystem external/zlib -isystem bazel-out/k8-opt-exec-ST-0465588ec812/bin/external/zlib -isystem external/eigen_archive -isystem bazel-out/k8-opt-exec-ST-0465588ec812/bin/external/eigen_archive -isystem external/eigen_archive/mkl_include -isystem bazel-out/k8-opt-exec-ST-0465588ec812/bin/external/eigen_archive/mkl_include -isystem external/hwloc/hwloc -isystem bazel-out/k8-opt-exec-ST-0465588ec812/bin/external/hwloc/hwloc -isystem external/hwloc/include -isystem bazel-out/k8-opt-exec-ST-0465588ec812/bin/external/hwloc/include -isystem external/local_config_cuda/cuda -isystem bazel-out/k8-opt-exec-ST-0465588ec812/bin/external/local_config_cuda/cuda -isystem external/cuda_cudart/include -isystem bazel-out/k8-opt-exec-ST-0465588ec812/bin/external/cuda_cudart/include -isystem external/cuda_cublas/include -isystem bazel-out/k8-opt-exec-ST-0465588ec812/bin/external/cuda_cublas/include -isystem external/cuda_cccl/include/cccl -isystem bazel-out/k8-opt-exec-ST-0465588ec812/bin/external/cuda_cccl/include/cccl -isystem external/cuda_nvtx/include -isystem bazel-out/k8-opt-exec-ST-0465588ec812/bin/external/cuda_nvtx/include -isystem external/cuda_nvcc/include -isystem bazel-out/k8-opt-exec-ST-0465588ec812/bin/external/cuda_nvcc/include -isystem external/cuda_cusolver/include -isystem bazel-out/k8-opt-exec-ST-0465588ec812/bin/external/cuda_cusolver/include -isystem external/cuda_cufft/include -isystem bazel-out/k8-opt-exec-ST-0465588ec812/bin/external/cuda_cufft/include -isystem external/cuda_cusparse/include -isystem bazel-out/k8-opt-exec-ST-0465588ec812/bin/external/cuda_cusparse/include -isystem external/cuda_curand/include -isystem bazel-out/k8-opt-exec-ST-0465588ec812/bin/external/cuda_curand/include -isystem external/cuda_cupti/include -isystem bazel-out/k8-opt-exec-ST-0465588ec812/bin/external/cuda_cupti/include -isystem external/cuda_nvml/include -isystem bazel-out/k8-opt-exec-ST-0465588ec812/bin/external/cuda_nvml/include -isystem external/cuda_nvjitlink/include -isystem bazel-out/k8-opt-exec-ST-0465588ec812/bin/external/cuda_nvjitlink/include -isystem external/cuda_crt/include -isystem bazel-out/k8-opt-exec-ST-0465588ec812/bin/external/cuda_crt/include -isystem external/cuda_cudnn/include -isystem bazel-out/k8-opt-exec-ST-0465588ec812/bin/external/cuda_cudnn/include -isystem external/farmhash_archive/src -isystem bazel-out/k8-opt-exec-ST-0465588ec812/bin/external/farmhash_archive/src -isystem external/llvm-project/llvm/include -isystem bazel-out/k8-opt-exec-ST-0465588ec812/bin/external/llvm-project/llvm/include '-isystem external/sysroot_linux_x86_64/usr/include/c++/8' '-isystem external/sysroot_linux_x86_64/usr/include/x86_64-linux-gnu/c++/8' '-isystem external/sysroot_linux_x86_64/usr/include/c++/8/backward' '-isystem external/sysroot_linux_x86_64/usr/include/c++/8/experimental' '-isystem external/llvm18_linux_x86_64/lib/clang/18' '-isystem external/llvm18_linux_x86_64/lib/clang/18/include' '-isystem external/sysroot_linux_x86_64/usr/local/include' '-isystem external/sysroot_linux_x86_64/usr/include/x86_64-linux-gnu' '-isystem external/sysroot_linux_x86_64/usr/include' '-include external/rules_ml_toolchain/cc/cuda/clang/clang_cuda_runtime_wrapper.h' -Wno-invalid-partial-specialization '--cuda-path=external/cuda_nvcc' -nocudainc -nostdinc++ -nostdinc -Wall -fcolor-diagnostics -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' --sysroot external/sysroot_linux_x86_64 '--target=x86_64-linux-gnu' -Wno-gnu-offsetof-extensions -Qunused-arguments '-Werror=mismatched-tags' -fPIE -ffunction-sections -fdata-sections -fmerge-all-constants -DNDEBUG -g0 -O2 -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fno-omit-frame-pointer -g0 -DGRPC_BAZEL_BUILD -w '-march=x86-64-v3' -O2 -mavx -g0 '-std=c++17' -DNV_CUDNN_DISABLE_EXCEPTION -c external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc -o bazel-out/k8-opt-exec-ST-0465588ec812/bin/external/local_xla/xla/stream_executor/cuda/_objs/cudnn_plugin/cuda_dnn.o) # Configuration: 85d8e927e7c38ab0b208eae59d6e9daa14c210f1a15188fe2378990fc889d337 # Execution platform: @@local_execution_config_platform//:platform In file included from external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:16: In file included from external/local_xla/xla/stream_executor/cuda/cuda_dnn.h:34: In file included from bazel-out/k8-opt-exec-ST-0465588ec812/bin/external/cudnn_frontend_archive/_virtual_includes/cudnn_frontend/third_party/cudnn_frontend/include/cudnn_frontend.h:102: In file included from bazel-out/k8-opt-exec-ST-0465588ec812/bin/external/cudnn_frontend_archive/_virtual_includes/cudnn_frontend/third_party/cudnn_frontend/include/cudnn_frontend_ConvDesc.h:32: In file included from bazel-out/k8-opt-exec-ST-0465588ec812/bin/external/cudnn_frontend_archive/_virtual_includes/cudnn_frontend/third_party/cudnn_frontend/include/cudnn_frontend_utils.h:186: bazel-out/k8-opt-exec-ST-0465588ec812/bin/external/cudnn_frontend_archive/_virtual_includes/cudnn_frontend/third_party/cudnn_frontend/include/cudnn_frontend_shim.h:236:46: error: no matching function for call to 'cudaGraphNodeGetDependentNodes' 236 | cuda_graph_node_get_dependent_nodes, cudaGraphNodeGetDependentNodes, node, pDependentNodes, pNumDependentNodes); | ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ bazel-out/k8-opt-exec-ST-0465588ec812/bin/external/cudnn_frontend_archive/_virtual_includes/cudnn_frontend/third_party/cudnn_frontend/include/cudnn_frontend_shim.h:155:68: note: expanded from macro 'NV_FE_CALL_TO_CUDA' 155 | #define NV_FE_CALL_TO_CUDA(function_name, cuda_symbol, ...) return cuda_symbol(__VA_ARGS__); | ^~~~~~~~~~~ external/cuda_cudart/include/cuda_runtime_api.h:11441:39: note: candidate function not viable: requires 4 arguments, but 3 were provided 11441 | extern __host__ cudaError_t CUDARTAPI cudaGraphNodeGetDependentNodes(cudaGraphNode_t node, cudaGraphNode_t *pDependentNodes, cudaGraphEdgeData *edgeData, size_t *pNumDependentNodes); | ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 1 error generated. Target //tensorflow/tools/pip_package:wheel failed to build\r\n```",
  "Requirement ID: ISSUE-102223\nTitle: Rolling back due to breakage.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nRolling back due to breakage.\n\nReverts 47ec6671b80d86bfc4cdf5c31d5915cb896a26ab",
  "Requirement ID: ISSUE-102222\nTitle: [NFC] Simplifications to `CommandBufferConversionPass`.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[NFC] Simplifications to `CommandBufferConversionPass`.",
  "Requirement ID: ISSUE-102221\nTitle: Enable multi-GPU tests on B200.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nEnable multi-GPU tests on B200.",
  "Requirement ID: ISSUE-102220\nTitle: Apply memory optimizations if options.allow_in_place_mlir_modification is true.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nApply memory optimizations if options.allow_in_place_mlir_modification is true.",
  "Requirement ID: ISSUE-102219\nTitle: [tsl:concurrency] Add support for detached futures\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[tsl:concurrency] Add support for detached futures\n\nRunning OnReady and Map callbacks in the thread that calls promise.Set() can be dangerous for performance. Add an API to execute all callbacks in the given executor.",
  "Requirement ID: ISSUE-102218\nTitle: [xla:gpu] NFC: Hoist computation index map creation in Triton dot emitters.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[xla:gpu] NFC: Hoist computation index map creation in Triton dot emitters.\n\nThis change moves the creation of the `computation_index_map` outside the loop body in `EmitDot` and `EmitScaledDot`, as it does not depend on the loop induction variable. It also simplifies how the tile size is retrieved in `GetDotLoopIterationCount` by using `TiledHloInstruction::tile_size`.",
  "Requirement ID: ISSUE-102217\nTitle: Extract launch information from the Triton compilation pipeline and use it instead of XLA's calculation. This is necessary in cases where the pipeline overrides the expected launch configuration.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nExtract launch information from the Triton compilation pipeline and use it instead of XLA's calculation. This is necessary in cases where the pipeline overrides the expected launch configuration.\nThis was observed when auto warp specialization was enabled. Triton requires more threads per block than expected, and this information is available in the module attributes.",
  "Requirement ID: ISSUE-102216\nTitle: Set call result shardings to the out shardings of func that is created or found from cache.\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nSet call result shardings to the out shardings of func that is created or found from cache.\n\nIt is a no op for `dedupFunctionsFully` is false which is also the default.\n\noutShardings is the the output shardings of the named computation at hand. However, if dedupFunctionsFully true, the func we pick from `createFuncOpOrGetFromCache`, which is the func the call will actually be calling, may have a different output sharding than the named computation, and call result sharding should be set to the output sharding it calls. For example,\n\nnamedComputation1(foo): insharding={\"x\"} outsharding={\"y\"}\nnamedComputation2(foo): insharding={\"x\"} outsharding={\"z\"}\n\ncall1 to namedComputation1\ncall2 to namedComputation2\n\nWhen dedupFunctionsFully is false, we have separate instances of foo as their outshardings are different.\n\nfunc foo1 insharding={\"x\"} outsharding={\"y\"} {...}\nfunc foo2 insharding={\"x\"} outsharding={\"z\"} {...}\ncall1 to foo_1 resultsharding={\"y\"}\ncall2 to foo_2 resultsharding={\"z\"}\n\nWhen dedupFunctionsFully is true, we do not have separate instance of foo, we need to pick either namedComputation1 or namedComputation2, say we pick namedComputation1, hence it becomes:\n\nfunc foo insharding={\"x\"} outsharding={\"y\"} {...}\ncall1 to foo resultsharding={\"y\"}\ncall2 to foo resultsharding={\"y\"}\n\nAs a result, call2 should have a resultsharding={\"y\"} since it is calling foo, instead of the out sharding of namedComputation2 which is {\"z\"}.",
  "Requirement ID: ISSUE-102215\nTitle: Replace `CustomCallThunk::Slice` by `ShapedSlice`\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nReplace `CustomCallThunk::Slice` by `ShapedSlice`\n\nThe two types have the same definition and represent the same semantically.",
  "Requirement ID: ISSUE-102214\nTitle: [XLA] Remove RunOnModelGroup.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[XLA] Remove RunOnModelGroup.\n\nWe no longer support model groups with size > 1, so there's no point in supporting generic RunOnModelGroup. One possible use-case of model groups with size == 1 is to be able to *replace* the module (instead of modifying it in-place). This adds a new interface to support that.",
  "Requirement ID: ISSUE-102213\nTitle: Refractor nchw filter dimension parsing in `ConvolutionReorderThunk`\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nRefractor nchw filter dimension parsing in `ConvolutionReorderThunk`\n\nInstead of inputing the filter dimensions as a span of integers, which we implicitly expect to be of size 4, we pass this a proto.\n\nUsing a proto instead of a struct since we'll need the `ConvolutionFilterDimensions` to serialize the `ConvolutionReorderThunk`. (We don't want to serialize the `FilterDescriptor` since most of its fields are only written during execution, so we'll serialize the `ConvolutionFilterDimensions` instead).\n\nNot sure where the best place for the `ConvolutionFilterDimensions` proto to live is. Other options would be to define it in:\n* In the thunk.proto, or;\n* In some other file more closely related to convolution filters (not sure where that could be).",
  "Requirement ID: ISSUE-102212\nTitle: absl::InitializeLog() must be called before logging.\nState: open\nAuthor: ThibaultDECO\nLabels: ready to pull, size:XS\nBody:\nabsl::InitializeLog() must be called before logging.\r\nOtherwise logs are written to STDERR and this warning appears:\r\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR",
  "Requirement ID: ISSUE-102211\nTitle: [XLA:GPU] SdcXorChecksumKernel: move trait to stream_executor/gpu\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[XLA:GPU] SdcXorChecksumKernel: move trait to stream_executor/gpu\n\nUse GpuKernelRegistry for loading the kernel rather than `TypedKernelFactory`.\nThe new header will help prevent errors related to use of \"gpu\"-tagged targets\nin non-\"gpu\"-tagged ones.\n\nAlso, avoid using atomic fetch_add to prevent JAX build failures on <sm60.\nWe're going to ensure that with a runtime check.",
  "Requirement ID: ISSUE-102210\nTitle: Tensorflow issue with Spotfire LTS 12.0.3 : A DLL initialization routine failed\nState: closed\nAuthor: suvarnaray-coriolis\nLabels: type:support\nBody:\n### Issue type\n\nSupport\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntensorflow-2.19.0 \n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nWindows 11\n\n### Mobile device\n\n_No response_\n\n### Python version\n\nPython 3.10.6 \n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nDear Team,\n\nWe are trying access TensorFlow from Spotfire Analyst LTS 12.0.3 via data function. However, we are getting a \"DLL load failed..\" error message. Please find the error message below and kindly suggest on how this issue could be fixed.\n\nCould not execute function call 'test df'\n\n\nError executing Python script:\n\nImportError: Traceback (most recent call last):\n  File \"C:\\Users\\suvarnaray\\AppData\\Local\\TIBCO\\Spotfire\\12.0.3\\Modules\\1_python-pkg-analyst_test_11Oct2025_1.0.0.0\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 73, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\n\n\nFailed to load the native TensorFlow runtime.\n\n\n\n### Standalone code to reproduce the issue\n\n```shell\nimport pandas as pd\nimport tensorflow as tf\nimport keras as ks\nfrom tensorflow.python.keras import Sequential\nfrom tensorflow.python.keras.layers import Dense\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nimport pickle\nimport warnings\n \nprint(\"TensorFlow Version: \", tf.version)\nwarnings.filterwarnings('ignore')\n```\n\n### Relevant log output\n\n```shell\n\n```",
  "Requirement ID: ISSUE-102209\nTitle: Integrate Triton up to [](https://github.com/openai/triton/commits/)\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nIntegrate Triton up to [](https://github.com/openai/triton/commits/)\n\nhttps://github.com/openxla/triton/tree/triton_integrate_branch-1.13",
  "Requirement ID: ISSUE-102208\nTitle: [XLA:GPU] Add xla:friends to GPU transforms.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[XLA:GPU] Add xla:friends to GPU transforms.",
  "Requirement ID: ISSUE-102207\nTitle: Remove duplicate check from unique_name generation\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nRemove duplicate check from unique_name generation",
  "Requirement ID: ISSUE-102206\nTitle: TF_SYSTEM_LIBS doesn't work\nState: open\nAuthor: medaminezghal\nLabels: type:build/install\nBody:\n### Issue type\n\nBuild/Install\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.20, tf 2.21 nightly\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nArch Linux\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.13\n\n### Bazel version\n\n7.4.1\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n13.0.2/9.14.0\n\n### GPU model and memory\n\nNVIDIA GTX 1650 Super\n\n### Current behavior?\n\nThe last errors talk about missing headers but after removing a lot of libs, I get this:\n```\nERROR: /home/medaminezghal/Desktop/tensorflow/src/tensorflow-2.21.0-cuda/tensorflow/tools/pip_package/BUILD:313:9 Action tensorflow/tools/pip_package/wheel_house/tensorflow-2.21.0.dev0+selfbuilt-cp313-cp313-linux_x86_64.whl failed: (Exit 1): nvcc_wrapper failed: error executing CppLink command (from target //tensorflow/cc:ops/user_ops_gen_cc) \n  (cd /home/medaminezghal/.cache/bazel/_bazel_medaminezghal/d52e28b7698f22a82a93b91f7e829fab/execroot/org_tensorflow && \\\n  exec env - \\\n    AR_PATH=external/llvm18_linux_x86_64/bin/llvm-ar \\\n    CPP_PATH=external/llvm18_linux_x86_64/bin/clang++ \\\n    GCC_PATH=external/llvm18_linux_x86_64/bin/clang \\\n    LD_PATH=external/llvm18_linux_x86_64/bin/ld.lld \\\n    NVCC_PATH=external/cuda_nvcc/bin/nvcc \\\n    NVCC_VERSION=13.0.2 \\\n    PATH=/home/medaminezghal/Desktop/tensorflow/src/bazel:/usr/local/sbin:/usr/local/bin:/usr/bin:/opt/cuda/bin:/opt/cuda/nsight_compute:/opt/cuda/nsight_systems/bin:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl \\\n    PWD=/proc/self/cwd \\\n  external/rules_ml_toolchain/cc/impls/linux_x86_64_linux_x86_64_cuda/wrappers/nvcc_wrapper @bazel-out/k8-opt-exec-ST-0465588ec812/bin/tensorflow/cc/ops/user_ops_gen_cc-2.params)\n```\n\n### Standalone code to reproduce the issue\n\n```shell\nFor example:\n\nexport TF_SYSTEM_LIBS=\"astor_archive,astunparse_archive,curl,cython,dill_archive,gast_archive,gif,libjpeg_turbo,nasm,pasta,six_archive,tblib_archive,termcolor_archive,typing_extensions_archive,wrapt\"  # Note that I have removed a lot of libraries du to fails.\nbazel \\\n    build --config=cuda_nvcc --config=cuda_wheel \\\n      ${BAZEL_ARGS[@]} \\\n      //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow\n```\n\n### Relevant log output\n\n```shell\n\n```",
  "Requirement ID: ISSUE-102205\nTitle: Update `rules_cc` to 0.2.9 and remove obsolete patches.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nUpdate `rules_cc` to 0.2.9 and remove obsolete patches.",
  "Requirement ID: ISSUE-102204\nTitle: PR #32504: [ROCm] Remove rocm_diagnostics\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nPR #32504: [ROCm] Remove rocm_diagnostics\n\nImported from GitHub PR https://github.com/openxla/xla/pull/32504\n\n\ud83d\udcdd Summary of Changes\nRemove rocm_diagnostics.cc\n\n\ud83c\udfaf Justification\n RocmDiagnostics module never worked and provides no meaningful information to the user.\n\n\ud83d\ude80 Kind of Contribution\n\u267b\ufe0f Cleanup\n\n\ud83d\udcca Benchmark (for Performance Improvements)\nN\\A\n\n\ud83e\uddea Unit Tests:\nNone\n\n\ud83e\uddea Execution Tests:\nNone\n\nCopybara import of the project:\n\n--\n73c4357ea80c720e2e46ddc0f91c8943e571b1ca by Dragan Mladjenovic <Dragan.Mladjenovic@amd.com>:\n\n[ROCm] Remove rocm_diagnostics\n\nMerging this change closes #32504\n\nFUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/32504 from ROCm:rocm_diag 73c4357ea80c720e2e46ddc0f91c8943e571b1ca",
  "Requirement ID: ISSUE-102203\nTitle: Add RISC-V 64-bit architecture support for TensorFlow builds\nState: open\nAuthor: balaraj74\nLabels: size:XL\nBody:\nThis commit fixes issue #102159 which prevented TensorFlow 2.19.1 from compiling on RISC-V 64-bit architecture due to an incorrectly structured Python C/C++ toolchain provider.\r\n\r\n## Problem\r\nUsers attempting to build TensorFlow on RISC-V encountered this error:\r\n```\r\nERROR: <target @python_riscv64-unknown-linux-gnu//:python_headers>\r\ndoesn't have provider 'headers'\r\n```\r\n\r\nThe error occurred because the custom py_cc_toolchain implementation didn't match what rules_python expects when it accesses: `py_cc_toolchain.headers.providers_map.values()`\r\n\r\n## Root Cause\r\nThe original toolchain implementation was returning python_headers as a direct label reference, but rules_python requires a specific provider structure with an intermediate `providers_map` dictionary.\r\n\r\n## Solution\r\nThis commit adds complete RISC-V support with:\r\n\r\n1. **Correct Toolchain Implementation** (`riscv64_py_cc_toolchain.bzl`)\r\n   - Properly structured provider with headers.providers_map\r\n   - Returns ToolchainInfo with correct hierarchy\r\n   - Well-documented for future maintenance\r\n\r\n2. **Build Configuration** (`BUILD.bazel`)\r\n   - Python runtime toolchain for RISC-V\r\n   - Python C/C++ toolchain for native extensions\r\n   - Platform constraints for riscv64 + linux\r\n\r\n3. **Automated Setup** (`setup_toolchain.sh`)\r\n   - Auto-detects Python installation\r\n   - Copies headers and libraries\r\n   - Creates proper directory structure\r\n\r\n4. **Verification Tools** (`verify_setup.sh`)\r\n   - Comprehensive checklist of requirements\r\n   - Color-coded pass/fail/warning output\r\n   - Helpful error messages and suggestions\r\n\r\n5. **Documentation**\r\n   - README.md: Complete setup guide\r\n   - ISSUE_RESOLUTION.md: Technical analysis\r\n   - SOLUTION_SUMMARY.md: Solution overview\r\n   - QUICKSTART.sh: Quick reference guide\r\n   - INDEX.md: Navigation hub\r\n\r\n6. **WORKSPACE Integration**\r\n   - Registers RISC-V Python toolchains\r\n   - Uses local_repository for toolchain location\r\n\r\n## Testing\r\nThe fix addresses the specific error mentioned in #102159 by providing the correct provider structure that rules_python expects. Users can verify the setup using the included verify_setup.sh script.\r\n\r\n## Platform Support\r\nAdds support for:\r\n- Architecture: RISC-V 64-bit (riscv64)\r\n- OS: Linux\r\n- Python: 3.11.6 (configurable for other versions)\r\n- Compiler: GCC 12.3.1+\r\n\r\n## Files Added\r\n- third_party/python_riscv64_toolchain/WORKSPACE\r\n- third_party/python_riscv64_toolchain/BUILD.bazel\r\n- third_party/python_riscv64_toolchain/riscv64_py_cc_toolchain.bzl\r\n- third_party/python_riscv64_toolchain/setup_toolchain.sh\r\n- third_party/python_riscv64_toolchain/verify_setup.sh\r\n- third_party/python_riscv64_toolchain/bazelrc_riscv64\r\n- third_party/python_riscv64_toolchain/README.md\r\n- third_party/python_riscv64_toolchain/ISSUE_RESOLUTION.md\r\n- third_party/python_riscv64_toolchain/SOLUTION_SUMMARY.md\r\n- third_party/python_riscv64_toolchain/QUICKSTART.sh\r\n- third_party/python_riscv64_toolchain/INDEX.md\r\n\r\n## Files Modified\r\n- WORKSPACE: Added local_repository and toolchain registration\r\n\r\n## Usage\r\n```bash\r\ncd third_party/python_riscv64_toolchain\r\n./setup_toolchain.sh\r\n./verify_setup.sh\r\ncd ../..\r\n./configure\r\nbazel build --config=opt //tensorflow/tools/pip_package:wheel\r\n```\r\n\r\nFixes #102159",
  "Requirement ID: ISSUE-102202\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102201\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102200\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102199\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102198\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102197\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102196\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102195\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102194\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102193\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102192\nTitle: Automated Code Change\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102191\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102190\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102189\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102188\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102187\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102186\nTitle: Automated Code Change\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102185\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102184\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102183\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102182\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102181\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102180\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102179\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102178\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102177\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102176\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102175\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102174\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102173\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102172\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102171\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102170\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102169\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102168\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102167\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102166\nTitle: Add methods to query chip count and logical devices per chip to PjRtTopologyDescription.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAdd methods to query chip count and logical devices per chip to PjRtTopologyDescription.",
  "Requirement ID: ISSUE-102165\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102164\nTitle: [xla:cpu] Construct BufferAllocationInfo from BufferAssignment\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[xla:cpu] Construct BufferAllocationInfo from BufferAssignment\n\nThis is no-op change, preparing for migration from cpu_function_runtime::BufferInfo to new BufferAllocationInfo type.\n\nFUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/101579 from ILCSFNO:patch-5 453a6e5f708b7c08de31a75a7ab86e3fabe53073",
  "Requirement ID: ISSUE-102163\nTitle: [XLA] Add `asinh` as a native HLO opcode.\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[XLA] Add `asinh` as a native HLO opcode.\n\nThis change promotes `asinh` from a composite operation to a native HLO opcode (`kAsinh`). This allows for direct lowering to device-specific intrinsics which should be more performant. Support is added for GPU.",
  "Requirement ID: ISSUE-102162\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102161\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102160\nTitle: [tsl:concurrency] Don't submit tasks to executor if Future::Map result is unused\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[tsl:concurrency] Don't submit tasks to executor if Future::Map result is unused",
  "Requirement ID: ISSUE-102159\nTitle: Can't compile tensorflow 2.19.1 on riscv\nState: open\nAuthor: Sherlockzhangjinge\nLabels: type:build/install, TF 2.19\nBody:\n### Issue type\n\nBuild/Install\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.19.1\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nEulixOS 12.3.1-33.eos30\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11.6\n\n### Bazel version\n\n2.19.1\n\n### GCC/compiler version\n\ngcc (GCC) 12.3.1 (EulixOS 12.3.1-33.eos30)\n\n### CUDA/cuDNN version\n\nNo\n\n### GPU model and memory\n\nNo\n\n### Current behavior?\n\nI am trying to compile tf 2.19.1 on riscv64. to add support and let bazel build tf with the python on riscv, I create a fold :third_party/python_riscv_reposity which contains BUILD.bazel ,WORKSPACE and riscv64_py_cc_toolchain.bzl. The detail comments is droped below.And I register them in WORKSPACE\nlocal_repository(\n    name =\"python_riscv64-unknown-linux-gnu\",\n    path = \"third_party/python_riscv64_toolchain\",\n)\nregister_toolchains(\n    \"@python_riscv64-unknown-linux-gnu//:python_riscv64_toolchain\",\n    \"@python_riscv64-unknown-linux-gnu//:riscv64_py_cc_toolchain\",\n)\n\n\n\n Now I meet:\nERROR: /root/.cache/bazel/_bazel_root/7b20b9b6ea80f6f214c7e8b22473e299/external/rules_python/python/cc/BUILD.bazel:15:22: in current_py_cc_headers rule @rules_python//python/cc:current_py_cc_headers:\nTraceback (most recent call last):\n        File \"/root/.cache/bazel/_bazel_root/7b20b9b6ea80f6f214c7e8b22473e299/external/rules_python/python/private/current_py_cc_headers.bzl\", line 21, column 27, in _current_py_cc_headers_impl\n                return py_cc_toolchain.headers.providers_map.values()\nError: <target @python_riscv64-unknown-linux-gnu//:python_headers> (rule 'cc_library') doesn't have provider 'headers'\nERROR: /root/.cache/bazel/_bazel_root/7b20b9b6ea80f6f214c7e8b22473e299/external/rules_python/python/cc/BUILD.bazel:15:22: Analysis of target '@rules_python//python/cc:current_py_cc_headers' failed\nERROR: Analysis of target '//tensorflow/tools/pip_package:wheel' failed; build aborted:\n\nHow can I solve it?\n\n### Standalone code to reproduce the issue\n\n```shell\nWORKSPACE \nworkspace(name = \"python_riscv64_toolchain_repo\")\n\nBUILD.bazel \nload(\"@rules_python//python:defs.bzl\", \"py_runtime\", \"py_runtime_pair\")\nload(\"riscv64_py_cc_toolchain.bzl\", \"riscv64_py_cc_toolchain_rule\")\n\npackage(default_visibility = [\"//visibility:public\"])\n\ncc_library(\n    name = \"python_headers\",\n    srcs=[],\n    hdrs = glob([\"python3.11/*.h\"]),\n    includes = [\"python3.11\"],\n    visibility = [\"//visibility:public\"],\n)\n\nfilegroup(\n    name = \"libpython\",\n    srcs = [\"libpython3.11.so\"],\n)\n\nfilegroup(\n    name = \"python3\",\n    srcs = [\"bin/python\"],\n)\n\npy_runtime(\n    name = \"riscv64_py_runtime\",\n    interpreter_path = \"/AI/zjg/python/venv11/bin/python\",\n    python_version = \"PY3\",\n    visibility = [\"//visibility:public\"],\n)\n\npy_runtime_pair(\n    name = \"riscv64_py_runtime_pair\",\n    py3_runtime = \":riscv64_py_runtime\",\n    visibility = [\"//visibility:public\"],\n)\n\ntoolchain(\n    name = \"python_riscv64_toolchain\",\n    toolchain_type = \"@rules_python//python:toolchain_type\",\n    toolchain = \":riscv64_py_runtime_pair\",\n    visibility = [\"//visibility:public\"],\n)\nriscv64_py_cc_toolchain_rule(\n    name = \"riscv64_py_cc_impl\",\n    python_includes = [\"python3.11\"],\n    python_headers = [\":python_headers\"],\n)\n\ntoolchain(\n    name = \"riscv64_py_cc_toolchain\",\n    toolchain_type = \"@rules_python//python/cc:toolchain_type\",\n    toolchain = \":riscv64_py_cc_impl\",\n    visibility = [\"//visibility:public\"],\n)\n\n\n\nriscv64_py_cc_toolchain.bzl\n\ndef _riscv64_py_cc_toolchain_impl(ctx):\n    py_cc = ctx.attr.python_headers[0]\n    return platform_common.ToolchainInfo(\n#        py_cc_toolchain = ctx.attr.python_headers,\n        py_cc_toolchain = py_cc,\n        python_headers = ctx.attr.python_headers,\n        python_includes = ctx.attr.python_includes,\n    )\n\nriscv64_py_cc_toolchain_rule = rule(\n    implementation = _riscv64_py_cc_toolchain_impl,\n    attrs = {\n        \"python_includes\": attr.string_list(),\n        \"python_headers\": attr.label_list(allow_files=False),\n    },\n)\n```\n\n### Relevant log output\n\n```shell\nNo\n```",
  "Requirement ID: ISSUE-102158\nTitle: Error: Request failed with status code 403\nState: closed\nAuthor: mochajavatest-cyber\nLabels: type:bug\nBody:\n### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n4.22.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nBug report\n1. Error: Request failed with status code 403. Please verify this URL points to the model JSON of the model to load.\n2.Running this function on node 20 async function loadYAMNetModel() {\n  const modelUrl = \"https://tfhub.dev/google/tfjs-model/yamnet/tfjs/1\";\n  //yamnetModel = await tf.loadGraphModel(modelUrl, { fromTFHub: true });\n  global.yamnetModel = await tf.loadGraphModel(modelUrl, { fromTFHub: true });\n  console.log(\"YAMNet model loaded successfully\");\n}\n\nloadYAMNetModel();\n3.Load the model successfully\n\n![Image](https://github.com/user-attachments/assets/fddfed2d-5976-4088-b78c-1e18cd14b96e)\n\n![Image](https://github.com/user-attachments/assets/58d251c7-dfaa-4b6c-aec1-925e6bacb7b0)\n\n### Standalone code to reproduce the issue\n\n```shell\nBug report\n1. Error: Request failed with status code 403. Please verify this URL points to the model JSON of the model to load.\n2.Running this function on node 20 async function loadYAMNetModel() {\n  const modelUrl = \"https://tfhub.dev/google/tfjs-model/yamnet/tfjs/1\";\n  //yamnetModel = await tf.loadGraphModel(modelUrl, { fromTFHub: true });\n  global.yamnetModel = await tf.loadGraphModel(modelUrl, { fromTFHub: true });\n  console.log(\"YAMNet model loaded successfully\");\n}\n\nloadYAMNetModel();\n3.Load the model successfully\n```\n\n### Relevant log output\n\n```shell\nC:\\easeSecurity\\backend\\node_modules\\@tensorflow\\tfjs-core\\dist\\tf-core.node.js:25943\n                            throw new Error(\"Request to \".concat(this.path, \" failed with status code \") +\n                                  ^\n\nError: Request to https://tfhub.dev/google/tfjs-model/yamnet/tfjs/1/model.json?tfjs-format=file failed with status code 403. Please verify this URL points to the model JSON of the model to load.\n    at HTTPRequest.<anonymous> (C:\\easeSecurity\\backend\\node_modules\\@tensorflow\\tfjs-core\\dist\\tf-core.node.js:25943:35)       \n    at step (C:\\easeSecurity\\backend\\node_modules\\@tensorflow\\tfjs-core\\dist\\tf-core.node.js:138:27)\n    at Object.next (C:\\easeSecurity\\backend\\node_modules\\@tensorflow\\tfjs-core\\dist\\tf-core.node.js:87:53)\n    at fulfilled (C:\\easeSecurity\\backend\\node_modules\\@tensorflow\\tfjs-core\\dist\\tf-core.node.js:68:28)\n    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)\n\nNode.js v20.19.0\n[nodemon] app crashed - waiting for file changes before starting...\n```",
  "Requirement ID: ISSUE-102157\nTitle: Use triple overload of lookupTarget\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nUse triple overload of lookupTarget\n\nThe overload accepting a llvm::StringRef will be removed when LLVM 22 branches.",
  "Requirement ID: ISSUE-102156\nTitle: [tsl:concurrency] Make it always safe to run AsyncValue waiters on executor\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[tsl:concurrency] Make it always safe to run AsyncValue waiters on executor",
  "Requirement ID: ISSUE-102155\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102154\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102153\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102152\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102151\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102150\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102149\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102148\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102147\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102146\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102145\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102144\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102143\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102142\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102141\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102140\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102139\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102138\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102137\nTitle: Automated Code Change\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102136\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102135\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102134\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102133\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102132\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102131\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102130\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102129\nTitle: Automated Code Change\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102128\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102127\nTitle: Internal change only\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nInternal change only",
  "Requirement ID: ISSUE-102126\nTitle: fix-command-injection-grpc-tpu-worker\nState: open\nAuthor: nishair\nLabels: awaiting review, ready to pull, size:S\nBody:\n## Summary\r\n\r\n  This PR fixes a command injection vulnerability in the gRPC TPU worker service script by replacing unsafe `subprocess.run(shell=True)` calls with safer alternatives.\r\n\r\n  ## Problem\r\n\r\n  The current implementation uses `subprocess.run()` with `shell=True` in three locations:\r\n  - Line 68: `subprocess.run(mkdir_command, shell=True, check=True)`\r\n  - Line 71: `subprocess.run(command, shell=True, check=True)`\r\n  - Line 83: `subprocess.run(command, shell=True, check=True)`\r\n\r\n  This creates a command injection vulnerability if user-controlled data reaches the service name parameter, potentially allowing arbitrary command execution.\r\n\r\n  ## Solution\r\n\r\n  - **File operations**: Replace shell `mv` and `mkdir` commands with `pathlib.Path` and `shutil.move()`\r\n  - **Systemctl commands**: Use `subprocess.run()` with argument lists instead of shell strings\r\n  - **Path handling**: Use `Path.home()` for reliable home directory resolution\r\n\r\n  ## Changes Made\r\n\r\n  ### Before:\r\n  ```python\r\n  subprocess.run(\"mkdir -p ~/.config/systemd/user\", shell=True, check=True)\r\n  subprocess.run(f\"mv {service_name} ~/.config/systemd/user/{service_name}\", shell=True, check=True)\r\n  subprocess.run(\"systemctl --user daemon-reload\", shell=True, check=True)\r\n\r\n  After:\r\n\r\n  user_systemd_dir.mkdir(parents=True, exist_ok=True)\r\n  shutil.move(str(source_file), str(dest_file))\r\n  subprocess.run([\"systemctl\", \"--user\", \"daemon-reload\"], check=True)\r\n\r\n  Security Impact\r\n\r\n  - Risk Level: Medium - prevents command injection attacks\r\n  - Scope: TPU worker service installation script\r\n  - Mitigation: Eliminates shell injection vectors while maintaining functionality\r\n\r\n  Testing\r\n\r\n  - Python syntax compilation passes\r\n  - Module imports successfully\r\n  - All functions maintain expected behavior\r\n  - No breaking changes to public API\r\n\r\n  Checklist\r\n\r\n  - Code follows TensorFlow style guidelines\r\n  - Security vulnerability eliminated\r\n  - Functionality preserved\r\n  - No additional dependencies required\r\n  - Signed Google CLA (if first contribution)\r\n\r\n  Related Issues\r\n\r\n  Addresses command injection vulnerability identified in security analysis of TensorFlow codebase.",
  "Requirement ID: ISSUE-102125\nTitle: [IFRT] Allow IFRT Proxy to use persistence API\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[IFRT] Allow IFRT Proxy to use persistence API\n\nThis changes allows IFRT Proxy client to use persistence API by sending a sidecar via plugin program.",
  "Requirement ID: ISSUE-102124\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102123\nTitle: [XLA] Remove brittleness regarding the `CHECK-HIGH-LEVEL` directives in `chlo_legalize_to_mhlo.mlir` tests by adding a `CHECK-HIGH-LEVEL-LABEL` to discard any output preceding the beginning of the test. Without this, the tests   are dependent; a `CHECK-HIGH-LEVEL` can match an output from a previous test, as it matches all output up until the latest preceding `CHECK-HIGH-LEVEL`. To prevent also matching output after the end of the test, a `CHECK-HIGH-LEVEL-LABEL` is added to each test following a test that contains a `CHECK-HIGH-LEVEL`.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[XLA] Remove brittleness regarding the `CHECK-HIGH-LEVEL` directives in `chlo_legalize_to_mhlo.mlir` tests by adding a `CHECK-HIGH-LEVEL-LABEL` to discard any output preceding the beginning of the test. Without this, the tests   are dependent; a `CHECK-HIGH-LEVEL` can match an output from a previous test, as it matches all output up until the latest preceding `CHECK-HIGH-LEVEL`. To prevent also matching output after the end of the test, a `CHECK-HIGH-LEVEL-LABEL` is added to each test following a test that contains a `CHECK-HIGH-LEVEL`.\n\nAlso, add `func.func` in some `CHECK-LABEL`s to make them more robust.",
  "Requirement ID: ISSUE-102122\nTitle: Integrate LLVM at llvm/llvm-project@3a6b818132e3\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nIntegrate LLVM at llvm/llvm-project@3a6b818132e3\n\nUpdates LLVM usage to match\n[3a6b818132e3](https://github.com/llvm/llvm-project/commit/3a6b818132e3)",
  "Requirement ID: ISSUE-102121\nTitle: [XLA] Remove a redundant `acosh_complex_f32` legalization test. The same block of lines exists in lines 467-617.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[XLA] Remove a redundant `acosh_complex_f32` legalization test. The same block of lines exists in lines 467-617.",
  "Requirement ID: ISSUE-102120\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102119\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102118\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102117\nTitle: Fix: GPU Numerical Precision in TFLite Add+Mul Ops (Issue #66740)\nState: open\nAuthor: kshiteej-mali\nLabels: awaiting review, size:L\nBody:\n## Fixes Issue #66740\r\n\r\n\ud83d\udd17 [TensorFlow Issue #66740](https://github.com/tensorflow/tensorflow/issues/66740)\r\n\r\n**Patch by kshiteej-mali for GPU numerical accuracy**\r\n\r\n---\r\n\r\n## Problem Statement\r\n\r\nTFLite's GPUv2 delegate was producing numerically incorrect results for Add and Mul operations, with errors up to **1e-2 or higher** compared to CPU reference outputs. This was caused by implicit FP16 operations and lack of explicit F32 precision guarantees in the generated OpenCL kernels.\r\n\r\n## Solution\r\n\r\nAdded explicit `convert_float4()` calls to ensure F32 precision in elementwise operations:\r\n\r\n**File**: `tensorflow/lite/delegates/gpu/common/tasks/elementwise.cc`\r\n\r\n```cpp\r\n// Patch by kshiteej-mali for GPU numerical accuracy\r\ncase OperationType::ADD:\r\n  c += \"  value_0 = convert_float4(value_0 + value_1);\\n\";\r\n  break;\r\ncase OperationType::MUL:\r\n  c += \"  value_0 = convert_float4(value_0 * value_1);\\n\";\r\n  break;\r\n```\r\n\r\nThis ensures:\r\n- \u2705 Consistent F32 precision across all GPU architectures\r\n- \u2705 Matching CPU reference behavior\r\n- \u2705 Numerical stability for chained operations\r\n\r\n## Test Results\r\n\r\n### Error Reduction (5 Orders of Magnitude!)\r\n\r\n| Metric | Before Fix | After Fix | Improvement |\r\n|--------|-----------|-----------|-------------|\r\n| Max Absolute Error | 1.23e-02 | 2.38e-07 | **50,000x** |\r\n| Max Relative Error | 11.2% | 0.0001% | **112,000x** |\r\n| Test Pass Rate | 17% (1/6) | 100% (6/6) | **+83%** |\r\n\r\n### Performance Impact\r\n\r\n- **Inference Time**: < 2.5% slowdown (8.7ms \u2192 8.9ms)\r\n- **Throughput**: 114.9 \u2192 112.4 imgs/sec\r\n- **Verdict**: Acceptable trade-off for correctness\r\n\r\n### Comprehensive Testing\r\n\r\nTested across 6 scenarios:\r\n- \u2705 Random inputs (various ranges)\r\n- \u2705 Edge cases (zeros, ones)\r\n- \u2705 Small values (precision test)\r\n- \u2705 Large values (stability test)\r\n\r\n**All tests now pass with errors < 1e-5** \u2728\r\n\r\n## Files Changed\r\n\r\n1. `tensorflow/lite/delegates/gpu/common/tasks/elementwise.cc`\r\n   - Added `convert_float4()` for ADD operation\r\n   - Added `convert_float4()` for MUL operation\r\n   - Inline comments crediting patch author\r\n\r\n2. `test_gpu_numerical_accuracy.py` (new)\r\n   - Comprehensive test script with 6 test cases\r\n   - Downloads model from issue #66740\r\n   - Compares GPU vs CPU outputs\r\n   - Generates detailed error metrics\r\n\r\n3. `GPU_FIX_TEST_RESULTS.md` (new)\r\n   - Complete test methodology documentation\r\n   - Before/after benchmarks\r\n   - Performance analysis\r\n   - Validation checklist\r\n\r\n## Validation\r\n\r\n- \u2705 Root cause identified and fixed\r\n- \u2705 All test cases pass (6/6)\r\n- \u2705 GPU outputs match CPU reference within F32 precision\r\n- \u2705 Performance impact minimal (< 3%)\r\n- \u2705 Code documented with patch credits\r\n- \u2705 Ready for production deployment\r\n\r\n## How to Reproduce Tests\r\n\r\n```bash\r\n# Clone the repository\r\ngit clone https://github.com/kshiteej-mali/tensorflow_ksh.git\r\ncd tensorflow_ksh\r\n\r\n# Install dependencies\r\npip install tensorflow numpy\r\n\r\n# Run the test suite\r\npython3 test_gpu_numerical_accuracy.py\r\n```\r\n\r\n## References\r\n\r\n- Original Issue: [tensorflow/tensorflow#66740](https://github.com/tensorflow/tensorflow/issues/66740)\r\n- Test Model: [tflite_66740_add_mul_gpu_numerically_incorrect.tflite](https://qaihub-public-issues.s3.us-west-2.amazonaws.com/tflite/tflite_66740_add_mul_gpu_numerically_incorrect.tflite)\r\n- Full Test Results: See `GPU_FIX_TEST_RESULTS.md`\r\n\r\n## Credits\r\n\r\n**Patch by kshiteej-mali** - Thank you for identifying and fixing this critical numerical accuracy issue! \ud83d\ude4f\r\n\r\nThis fix ensures TensorFlow Lite GPU delegate produces correct, reliable results that match CPU behavior, enabling confident deployment of TFLite models on GPU-accelerated devices.",
  "Requirement ID: ISSUE-102116\nTitle: Fix: GPU Numerical Precision in TFLite Add+Mul Ops (Issue #66740)\nState: closed\nAuthor: kshiteej-mali\nLabels: size:L\nBody:\n## Fixes Issue #66740\r\n\r\n\ud83d\udd17 [TensorFlow Issue #66740](https://github.com/tensorflow/tensorflow/issues/66740)\r\n\r\n**Patch by kshiteej-mali for GPU numerical accuracy**\r\n\r\n---\r\n\r\n## Problem Statement\r\n\r\nTFLite's GPUv2 delegate was producing numerically incorrect results for Add and Mul operations, with errors up to **1e-2 or higher** compared to CPU reference outputs. This was caused by implicit FP16 operations and lack of explicit F32 precision guarantees in the generated OpenCL kernels.\r\n\r\n## Solution\r\n\r\nAdded explicit `convert_float4()` calls to ensure F32 precision in elementwise operations:\r\n\r\n**File**: `tensorflow/lite/delegates/gpu/common/tasks/elementwise.cc`\r\n\r\n```cpp\r\n// Patch by kshiteej-mali for GPU numerical accuracy\r\ncase OperationType::ADD:\r\n  c += \"  value_0 = convert_float4(value_0 + value_1);\\n\";\r\n  break;\r\ncase OperationType::MUL:\r\n  c += \"  value_0 = convert_float4(value_0 * value_1);\\n\";\r\n  break;\r\n```\r\n\r\nThis ensures:\r\n- \u2705 Consistent F32 precision across all GPU architectures\r\n- \u2705 Matching CPU reference behavior\r\n- \u2705 Numerical stability for chained operations\r\n\r\n## Test Results\r\n\r\n### Error Reduction (5 Orders of Magnitude!)\r\n\r\n| Metric | Before Fix | After Fix | Improvement |\r\n|--------|-----------|-----------|-------------|\r\n| Max Absolute Error | 1.23e-02 | 2.38e-07 | **50,000x** |\r\n| Max Relative Error | 11.2% | 0.0001% | **112,000x** |\r\n| Test Pass Rate | 17% (1/6) | 100% (6/6) | **+83%** |\r\n\r\n### Performance Impact\r\n\r\n- **Inference Time**: < 2.5% slowdown (8.7ms \u2192 8.9ms)\r\n- **Throughput**: 114.9 \u2192 112.4 imgs/sec\r\n- **Verdict**: Acceptable trade-off for correctness\r\n\r\n### Comprehensive Testing\r\n\r\nTested across 6 scenarios:\r\n- \u2705 Random inputs (various ranges)\r\n- \u2705 Edge cases (zeros, ones)\r\n- \u2705 Small values (precision test)\r\n- \u2705 Large values (stability test)\r\n\r\n**All tests now pass with errors < 1e-5** \u2728\r\n\r\n## Files Changed\r\n\r\n1. `tensorflow/lite/delegates/gpu/common/tasks/elementwise.cc`\r\n   - Added `convert_float4()` for ADD operation\r\n   - Added `convert_float4()` for MUL operation\r\n   - Inline comments crediting patch author\r\n\r\n2. `test_gpu_numerical_accuracy.py` (new)\r\n   - Comprehensive test script with 6 test cases\r\n   - Downloads model from issue #66740\r\n   - Compares GPU vs CPU outputs\r\n   - Generates detailed error metrics\r\n\r\n3. `GPU_FIX_TEST_RESULTS.md` (new)\r\n   - Complete test methodology documentation\r\n   - Before/after benchmarks\r\n   - Performance analysis\r\n   - Validation checklist\r\n\r\n## Validation\r\n\r\n- \u2705 Root cause identified and fixed\r\n- \u2705 All test cases pass (6/6)\r\n- \u2705 GPU outputs match CPU reference within F32 precision\r\n- \u2705 Performance impact minimal (< 3%)\r\n- \u2705 Code documented with patch credits\r\n- \u2705 Ready for production deployment\r\n\r\n## Reviewers: Please See\r\n\r\n- **Test Results & Benchmarks**: `GPU_FIX_TEST_RESULTS.md`\r\n- **Test Script**: `test_gpu_numerical_accuracy.py`\r\n- **Original Issue**: [tensorflow/tensorflow#66740](https://github.com/tensorflow/tensorflow/issues/66740)\r\n\r\n## How to Reproduce Tests\r\n\r\n```bash\r\n# Clone the repository\r\ngit clone https://github.com/kshiteej-mali/tensorflow_ksh.git\r\ncd tensorflow_ksh\r\n\r\n# Install dependencies\r\npip install tensorflow numpy\r\n\r\n# Run the test suite\r\npython3 test_gpu_numerical_accuracy.py\r\n```\r\n\r\n## Credits\r\n\r\n**Patch by kshiteej-mali** - Thank you for identifying and fixing this critical numerical accuracy issue! \ud83d\ude4f\r\n\r\nThis fix ensures TensorFlow Lite GPU delegate produces correct, reliable results for millions of edge AI deployments worldwide.\r\n\r\n---\r\n\r\n**Status**: \u2705 Ready to merge\r\n**Recommended Reviewers**: @tensorflow/lite-team @tensorflow/sig-on-device-inference",
  "Requirement ID: ISSUE-102115\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102114\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102113\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102112\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102111\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102110\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102109\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102108\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102107\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102106\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102105\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102104\nTitle: feat(keras): Add SeparableConv2DTranspose layer\nState: closed\nAuthor: CodersAcademy006\nLabels: size:XL\nBody:\nThis PR introduces a new `SeparableConv2DTranspose` Keras layer, providing an efficient, lightweight option for upsampling operations. This is the counterpart to the existing `SeparableConv2D` layer and is useful for generative models and autoencoders, especially on mobile and edge devices.\r\n\r\nThis implementation includes:\r\n- A new `SeparableConv2DTranspose` op registration.\r\n- A reference CPU kernel for the operation.\r\n- The user-facing `tf.keras.layers.SeparableConv2DTranspose` Python API.\r\n\r\nFixes #89628",
  "Requirement ID: ISSUE-102103\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102102\nTitle: fix(cmake): Correctly install and export TFLite dependencies\nState: open\nAuthor: CodersAcademy006\nLabels: awaiting review, size:L\nBody:\nThis PR fixes a CMake build issue where critical dependencies were not being installed and exported, preventing the TFLite package from being used by other CMake projects.\r\n\r\nThe `install(EXPORT ...)` command was failing because targets like `XNNPACK`, `pthreadpool`, and `xnnpack-delegate` were required by `tensorflow-lite` but were not part of any export set.\r\n\r\nThis fix adds the necessary `install()` commands for these dependencies, ensuring they are correctly installed and exported along with the main `tensorflow-lite` library.\r\n\r\nFixes #100686",
  "Requirement ID: ISSUE-102101\nTitle: fix(core): Correct int64 comparison on Windows for cwise ops\nState: open\nAuthor: CodersAcademy006\nLabels: awaiting review, size:M, prtype:bugfix\nBody:\nThis PR fixes a platform-specific bug where `tf.clip_by_value`, `tf.minimum`, and `tf.maximum` produced incorrect results for `int64` tensors on Windows.\r\n\r\nThe root cause was that the MSVC compiler was incorrectly handling 64-bit integer comparisons in the underlying C++ kernels, leading to values not being clipped as expected.\r\n\r\nThis fix introduces a platform-specific patch in the `minimum` and `maximum` functors to ensure that the comparisons are always performed as true 64-bit operations, resolving the issue for all dependent ops.\r\n\r\nFixes #100590",
  "Requirement ID: ISSUE-102100\nTitle: fix(keras): Add validation to prevent zero-sized output in Pooling3D\nState: open\nAuthor: CodersAcademy006\nLabels: awaiting review, ready to pull, size:M, prtype:bugfix\nBody:\nThis PR fixes a fatal cuDNN abort that occurred when a `MaxPool3D` layer produced an output with a zero-sized spatial dimension.\r\n\r\nThe root cause was a missing pre-condition check in the `Pooling3DOp` kernel. Invalid output shapes were being passed directly to the underlying GPU libraries (cuDNN), causing a hard crash instead of a graceful Python error.\r\n\r\nThis fix adds a validation check to the `Pooling3DOp` kernel to ensure all calculated output dimensions are positive before dispatching the operation. This prevents the crash and raises a clear `InvalidArgumentError` to the user, as expected.\r\n\r\nFixes #101409",
  "Requirement ID: ISSUE-102099\nTitle: fix(image): Improve rgb_to_hsv precision with double accumulator\nState: open\nAuthor: CodersAcademy006\nLabels: awaiting review, size:M, prtype:bugfix\nBody:\nThis PR fixes a numerical stability bug in `tf.image.rgb_to_hsv` that caused a loss of precision with large `float32` inputs.\r\n\r\nThe root cause was catastrophic cancellation during the internal color space calculations, which made the function irreversible for values near the `float32` limit.\r\n\r\nThis fix resolves the issue by upcasting all intermediate calculations to `double` precision. This preserves the relative differences between large input values, ensuring the conversion is numerically stable.\r\n\r\nFixes #102048",
  "Requirement ID: ISSUE-102098\nTitle: Fix distribute reduce f16 overflow\nState: open\nAuthor: CodersAcademy006\nLabels: awaiting review, size:M, prtype:bugfix\nBody:\nThis PR fixes a premature overflow issue in `tf.distribute.Strategy.reduce(\"SUM\")` when used with `float16` tensors.\r\n\r\nThe root cause was that the underlying cross-device reduction kernel (NCCL-based) performed its summation in the native `float16` type, which overflowed with large inputs.\r\n\r\nThis fix modifies the `NcclAllReduceOpKernel` to use a `float32` accumulator for `float16` inputs. The intermediate sum is computed in full precision to prevent overflow, and the final result is cast back to `float16`, ensuring both numerical stability and consistency with `tf.math.reduce_sum`.\r\n\r\nFixes #102046",
  "Requirement ID: ISSUE-102097\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102096\nTitle: Fix ragged not equal truncation\nState: open\nAuthor: CodersAcademy006\nLabels: awaiting review, size:M, prtype:bugfix\nBody:\nThis PR fixes an integer truncation bug that occurred when using `tf.math.not_equal` between a scalar and a `tf.RaggedTensor` of a smaller dtype.\r\n\r\nThe root cause was the default binary operator promotion logic, which could incorrectly downcast the scalar's type, leading to data loss before the comparison.\r\n\r\nThis fix introduces a custom `ragged_ne` operator that explicitly promotes both operands to their widest common dtype, ensuring the comparison is always safe and preventing truncation.\r\n\r\nFixes #102044",
  "Requirement ID: ISSUE-102095\nTitle: Fix xla int32 accumulator\nState: open\nAuthor: CodersAcademy006\nLabels: awaiting review, size:XS, prtype:bugfix\nBody:\nThis PR fixes an integer overflow issue in XLA-compiled `reduce_mean` operations on `int32` tensors.\r\n\r\nThe root cause was that the `XlaHelpers::SumAccumulationType` helper function did not upcast `int32` inputs to use an `int64` accumulator, leading to premature overflow that was inconsistent with the non-XLA behavior.\r\n\r\nThis change adds the missing `int32` -> `int64` case to the helper function, ensuring all XLA sum reductions are numerically stable and consistent.\r\n\r\nFixes #102043",
  "Requirement ID: ISSUE-102094\nTitle: fix(build): Set 16KB page alignment for core JNI library\nState: closed\nAuthor: CodersAcademy006\nLabels: size:XS, prtype:bugfix\nBody:\nThis PR resolves the Google Play validation error for 16 KB memory page sizes on Android 15+ by applying the fix to the core JNI dependency.\r\n\r\nThe `:native_stable_framework_only` library target, a dependency for all JNI libraries, was being compiled with the default 4KB alignment. This change adds the `-Wl,-z,max-page-size=16384` linker flag to its build configuration, ensuring all dependent libraries are built with the required 16KB alignment.\r\n\r\nFixes #101991",
  "Requirement ID: ISSUE-102093\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102092\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102091\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102090\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102089\nTitle: Support async reduce-scatter for `GetLatencyBetween` in sol cost estimator && add verbose log\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nSupport async reduce-scatter for `GetLatencyBetween` in sol cost estimator && add verbose log",
  "Requirement ID: ISSUE-102088\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102087\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102086\nTitle: Fix weighted_moments to accept tensor axes with keepdims=False\nState: open\nAuthor: codingWizard-Nikhil\nLabels: awaiting review, size:S, prtype:bugfix, python\nBody:\nFixes #101580\r\n\r\n### Summary\r\n`tf.nn.weighted_moments()` was failing when `keepdims=False` and `axes` was passed as a tensor (as documented), because the internal `squeeze()` operation only accepts lists/tuples.\r\n\r\n### Changes\r\n- Modified `weighted_moments()` to convert tensor axes to lists internally before calling `squeeze()`\r\n- Added test case to verify tensor axes work with `keepdims=False`\r\n- Maintains backward compatibility - list/tuple axes still work as before\r\n\r\n### Testing\r\n- Added `weighted_moments_test.py` with test for tensor axes + keepdims=False\r\n- Test passes locally\r\n- No breaking changes to existing functionality",
  "Requirement ID: ISSUE-102085\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102084\nTitle: tf.linalg.trace returns opposite infinity signs with and without XLA (jit_compile=True) on float16 inputs\nState: open\nAuthor: syan427\nLabels: type:bug, comp:xla, TF 2.13\nBody:\n### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.13\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.9\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nWhen computing the matrix trace using tf.linalg.trace on a float16 matrix, the result differs dramatically depending on whether XLA compilation is enabled.\n\nWithout XLA (jit_compile=False): the result is -inf\n\nWith XLA (jit_compile=True): the result is inf\n\n### Standalone code to reproduce the issue\n\n```shell\nimport numpy as np\nimport tensorflow as tf\n\n@tf.function(jit_compile=False)\ndef run_without_xla(a):\n    return tf.linalg.trace(a)\n@tf.function(jit_compile=True)\ndef run_with_xla(a):\n    return tf.linalg.trace(a)\n\nx = np.array([[-60000, 0, 0, 0, 0, 0],\n[0,-10000,0,0,0,0],\n[0, 0, 60000, 0, 0, 0],\n[0,0,0, 60000,0,0],\n[0,0,0,0, 10000,0],\n[0,0,0,0,0, 10000]],dtype=np.float16)\n\nresult1 = run_without_xla(x)# [-inf]\nresult2 = run_with_xla(x)# [inf]\n\nprint(result1)\nprint(result2)\n```\n\n### Relevant log output\n\n```shell\n[-inf]\n[inf]\n```",
  "Requirement ID: ISSUE-102083\nTitle: Implement PjRtStreamExecutorRawBuffer::CopyToLiteralAsync and allow\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nImplement PjRtStreamExecutorRawBuffer::CopyToLiteralAsync and allow\nPjRtStreamExecutorBuffer to just use inherited literal conversion logic.",
  "Requirement ID: ISSUE-102082\nTitle: Use nvml impl lib based wrapper\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nUse nvml impl lib based wrapper",
  "Requirement ID: ISSUE-102081\nTitle: Update TfrtGpuExecutable::ExecuteHelper to force untupling to at least be true whenever the result is a tuple (to match other backends).\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nUpdate TfrtGpuExecutable::ExecuteHelper to force untupling to at least be true whenever the result is a tuple (to match other backends).",
  "Requirement ID: ISSUE-102080\nTitle: Fix bug in `IsInteger` condition.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nFix bug in `IsInteger` condition.",
  "Requirement ID: ISSUE-102079\nTitle: [XLA] Compilation metrics collector for XLA.\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[XLA] Compilation metrics collector for XLA.\n\nConvenience functions to collect metrics from a compilation:\n * Use XLA_SET_SCOPE(..), sets the name of the compiler pass in the HLO pipeline (or name of HLO being lowered in the LLO side).\n * Use XLA_ENABLE_METRIC_FAMILY(..) to enable collection for a particular family of metrics.\n * Use XLA_ADD_METRIC(family, key, value) to record the value of a metric.\n\nTo enable logging of metrics, run `--vmodule=metrics=1`.",
  "Requirement ID: ISSUE-102078\nTitle: [xla:ifrt] Replace Future::OnReady with Future::Map\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[xla:ifrt] Replace Future::OnReady with Future::Map",
  "Requirement ID: ISSUE-102077\nTitle: [XLA:GPU] Add verbose kernel scheduling tracing for debugging\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[XLA:GPU] Add verbose kernel scheduling tracing for debugging",
  "Requirement ID: ISSUE-102076\nTitle: [XLA:GPU] Only set channel id it is present in the original instruction.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[XLA:GPU] Only set channel id it is present in the original instruction.",
  "Requirement ID: ISSUE-102075\nTitle: Bump rules_ml_toolchain version to integrate @cuda_nvrtc header.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nBump rules_ml_toolchain version to integrate @cuda_nvrtc header.",
  "Requirement ID: ISSUE-102074\nTitle: PR #31795: [GPU] Assign default color to tuples\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nPR #31795: [GPU] Assign default color to tuples\n\nImported from GitHub PR https://github.com/openxla/xla/pull/31795\n\n\ud83d\udcdd Summary of Changes\n[Downstream check](https://github.com/openxla/xla/blob/main/xla/pjrt/pjrt_executable.cc#L288-L299) assumes tuples on default memory space, force assign default color to tuples will get around the check.\n\n\ud83c\udfaf Justification\nNCCL user buffer runs are crashing on MaxText main. This PR fixes the crash.\n\n\ud83d\ude80 Kind of Contribution\n\ud83d\udc1b Bug Fix\n\n\ud83d\udcca Benchmark (for Performance Improvements)\nN/A.\n\n\ud83e\uddea Unit Tests:\nExisting unit tests.\n\n\ud83e\uddea Execution Tests:\nAdded multiple execution tests.\n\nCopybara import of the project:\n\n--\nc60fe9d62827596eac57df2b480891520b40ab07 by Terry Sun <tesun@nvidia.com>:\n\nassign default color yo tuples\n\n--\n717412a55a94be71afcbb7627f03905c408f8b6a by Terry Sun <tesun@nvidia.com>:\n\nadd constant and polish doc string\n\n--\nc907b2d1ca5a62299b6bfd2534e99c6215313ffd by Terry Sun <tesun@nvidia.com>:\n\nupdate test\n\nMerging this change closes #31795\n\nFUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/31795 from terryysun:terryysun/tuple_color c907b2d1ca5a62299b6bfd2534e99c6215313ffd",
  "Requirement ID: ISSUE-102073\nTitle: Remove use of vector.splat\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nRemove use of vector.splat\n\nRemoved in https://github.com/llvm/llvm-project/commit/ea291d0e8c93d47d7953eff5ca1048891a5fcc55. The replacement is vector.broadcast.",
  "Requirement ID: ISSUE-102072\nTitle: [tsl:concurrency] Add Future::map overrides that can take Executor to run the map functor\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[tsl:concurrency] Add Future::map overrides that can take Executor to run the map functor",
  "Requirement ID: ISSUE-102071\nTitle: LiteRT Support in Latest Tensorflow version 2.20.0\nState: open\nAuthor: Apoorv-1009\nLabels: comp:lite, type:others, TF 2.19\nBody:\n### Issue type\n\nOthers\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.20.0\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nLinux Ubuntu 18.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nHey,\n\nI had a quick question regarding the TensorFlow 2.20.0 release. The release notes mention that: \n`tf.lite will be deprecated, in favor of the new repo https://github.com/google-ai-edge/LiteRT.`\n\nDoes this mean that TensorFlow 2.20.0 already includes support for LiteRT internally, or is LiteRT integration planned for a future TensorFlow release?\n\nThank you\n\n### Standalone code to reproduce the issue\n\n```shell\nNA\n```\n\n### Relevant log output\n\n```shell\n\n```",
  "Requirement ID: ISSUE-102070\nTitle: bump rules_ml_toolchain version\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nbump rules_ml_toolchain version",
  "Requirement ID: ISSUE-102069\nTitle: Replace a chain of xpose/reshape to a nop if the composite transpose permutation is idendity\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nReplace a chain of xpose/reshape to a nop if the composite transpose permutation is idendity",
  "Requirement ID: ISSUE-102068\nTitle: Optimize folding of transpose ops operating on splat constants.\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nOptimize folding of transpose ops operating on splat constants.\n\nInstead of expanding and iterating over the splat to create the new constant, we now just replace the splat constant's dimensions as specified by the transpose op.\n\nWhen tested on the same input program that brought this issue to light, this fix improved the optimizer's execution time from 8.68 s to 0.80 s, a 985% speedup.",
  "Requirement ID: ISSUE-102067\nTitle: [IFRT] Migrate `Array::pjrt_layout()` callers to interpret `nullptr` as a default layout\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[IFRT] Migrate `Array::pjrt_layout()` callers to interpret `nullptr` as a default layout\n\n`Array::pjrt_layout()` will be changed to return `nullptr` to indicate a default layout, where the callers can obtain the corresponding concrete default layout by using `Client::GetDefaultPjRtLayout()`.\n\nThis change adds `nullptr` handling preemptively before the new `Array::pjrt_layout()` semantics becomes effective so that the existing code works as before.\n\nTests using `Array::pjrt_layout()` method calls are minimally updated to add a non-nullness check. They will be updated as `Array::pjrt_layout()` actually returns `nullptr`.",
  "Requirement ID: ISSUE-102066\nTitle: [XLA:GPU] Move AsyncStreamKind and CollectiveOpGroupMode to xla_data.proto.\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[XLA:GPU] Move AsyncStreamKind and CollectiveOpGroupMode to xla_data.proto.\n\nThis is a preparation CL before adding serialization for collective thunks.",
  "Requirement ID: ISSUE-102065\nTitle: GPU crash with RTX A1000 on TensorFlow 2.13 \u2014 even simple Conv2D model fails\nState: open\nAuthor: achen1959\nLabels: comp:gpu, type:performance, TF 2.13\nBody:\n### Issue type\n\nPerformance\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.13.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 24.04.3 LTS\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.8.13\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\nCUDA 11.8, cuDNN 8.6\n\n### GPU model and memory\n\nNVIDIA RTX A1000 Laptop GPU, 6GB\n\n### Current behavior?\n\nEven a minimal Conv2D model causes the Python kernel to crash when run on GPU. The same model runs fine on CPU. No traceback is shown \u2014 GPU memory spikes briefly and then drops to 0 MiB. nvidia-smi shows no active processes after the crash.\nModel should train normally on GPU without crashing.\n\n### Standalone code to reproduce the issue\n\n```shell\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, Input\nimport numpy as np\n\nmodel = Sequential([\n    Input(shape=(32, 32, 3)),\n    Conv2D(8, (3, 3), activation='relu', padding='same'),\n    Conv2D(3, (3, 3), activation='sigmoid', padding='same'),\n])\nmodel.compile(optimizer='adam', loss='mse')\nmodel.train_on_batch(np.random.rand(1, 32, 32, 3), np.random.rand(1, 32, 32, 3))\n```\n\n### Relevant log output\n\n```shell\nnvidia-smi shows ~631 MiB memory usage briefly, then drops to 0 MiB.\nGPU utilization remains at 0%.\nNo error message in Python \u2014 kernel dies silently.\n```",
  "Requirement ID: ISSUE-102064\nTitle: [XLA:GPU] Add collective kind to rendezvous name in CollectiveThunk.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[XLA:GPU] Add collective kind to rendezvous name in CollectiveThunk.\n\nThis makes the rendezvous name more informative by including the specific type of collective operation, which helps in debugging.",
  "Requirement ID: ISSUE-102063\nTitle: [IFRT] Migrate `Array::pjrt_layout()` callers to interpret `nullptr` as a default layout\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[IFRT] Migrate `Array::pjrt_layout()` callers to interpret `nullptr` as a default layout\n\n`Array::pjrt_layout()` will be changed to return `nullptr` to indicate a default layout, where the callers can obtain the corresponding concrete default layout by using `Client::GetDefaultPjRtLayout()`.\n\nThis change adds `nullptr` handling preemptively before the new `Array::pjrt_layout()` semantics becomes effective so that the existing code works as before.\n\nTests using `Array::pjrt_layout()` method calls are minimally updated to add a non-nullness check. They will be updated as `Array::pjrt_layout()` actually returns `nullptr`.",
  "Requirement ID: ISSUE-102062\nTitle: Avoid changing the MLIR context of an IFRT IR program during compilation if the program does not exclusively own the context\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAvoid changing the MLIR context of an IFRT IR program during compilation if the program does not exclusively own the context",
  "Requirement ID: ISSUE-102061\nTitle: Integrate LLVM at llvm/llvm-project@3a6b818132e3\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nIntegrate LLVM at llvm/llvm-project@3a6b818132e3\n\nUpdates LLVM usage to match\n[3a6b818132e3](https://github.com/llvm/llvm-project/commit/3a6b818132e3)",
  "Requirement ID: ISSUE-102060\nTitle: [XLA:GPU] Remove unused mlir::Value fields in CollectiveThunk::Buffer.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[XLA:GPU] Remove unused mlir::Value fields in CollectiveThunk::Buffer.",
  "Requirement ID: ISSUE-102059\nTitle: [tsl:concurrency] Add Future::OnReady overrides that can take Executor to run the callback\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[tsl:concurrency] Add Future::OnReady overrides that can take Executor to run the callback\n\nname                        cpu/op        cpu/op      vs base\nBM_CreateOkFuture           1.784n \u00b1 0%   1.860n \u00b1 1%  +4.26% (p=0.000 n=40)\nBM_CopyFuture               1.737n \u00b1 0%   1.727n \u00b1 0%  -0.58% (p=0.001 n=40)\nBM_MapStatelessFuture       14.29n \u00b1 0%   14.27n \u00b1 0%       ~ (p=0.283 n=40)\nBM_TryMapStatelessFuture    14.31n \u00b1 0%   14.25n \u00b1 0%       ~ (p=0.062 n=40)\nBM_MapToFromStatelessFuture 14.43n \u00b1 0%   14.08n \u00b1 1%  -2.44% (p=0.000 n=40)\nBM_MapStatefulFuture        14.55n \u00b1 0%   14.51n \u00b1 0%       ~ (p=0.607 n=40)\nBM_TryMapStatefulFuture     14.54n \u00b1 0%   14.49n \u00b1 1%       ~ (p=0.405 n=40)\ngeomean                     7.908n        7.908n       -0.00%",
  "Requirement ID: ISSUE-102058\nTitle: Fix the build (linker) error on Windows\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nFix the build (linker) error on Windows",
  "Requirement ID: ISSUE-102057\nTitle: Allow `ifrt::CopyArraysOp` as IFRT IR program outputs.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAllow `ifrt::CopyArraysOp` as IFRT IR program outputs.\n\nThe output of an IFRT IR program can be either a direct argument (`mlir::BlockArgument`) or (`xla::ifrt::CallLoadedExecutableOp`) output or (`ifrt::CopyArraysOp`)\n\nFUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/32504 from ROCm:rocm_diag 73c4357ea80c720e2e46ddc0f91c8943e571b1ca",
  "Requirement ID: ISSUE-102056\nTitle: [XLA:GPU] Add SdcDumpLogThunk\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[XLA:GPU] Add SdcDumpLogThunk\n\nA `Thunk` that keeps all info needed to call `DumpPerExecutionProtobufToFile`.",
  "Requirement ID: ISSUE-102055\nTitle: Ucgcgcgvy.\nState: closed\nAuthor: matheushenrick304-code\nLabels: type:bug\nBody:\n### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\nFtgyctx\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nMatheushenrick07 \n\n\n### Standalone code to reproduce the issue\n\n```shell\nTy\nYdy\n```\n\n### Relevant log output\n\n```shell\n\n```",
  "Requirement ID: ISSUE-102054\nTitle: Spammer\nState: closed\nAuthor: matheushenrick304-code\nLabels: comp:lite\nBody:\n**System information**\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\n- TensorFlow installed from (source or binary):\n- TensorFlow version (or github SHA if from source):\n\n\n**Provide the text output from tflite_convert**\n\n```\n# Copy and paste here\n```\n\n**Standalone code to reproduce the issue** \nProvide a reproducible test case that is the bare minimum necessary to generate\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\n\nAlso, please include a link to a GraphDef or the model if possible.\n\n**Any other info / logs**\n\nInclude any logs or source code that would be helpful to diagnose the problem.\nIf including tracebacks, please include the full traceback. Large logs and files\nshould be attached.",
  "Requirement ID: ISSUE-102053\nTitle: I'm a spammer\nState: closed\nAuthor: matheushenrick304-code\nLabels: comp:lite\nBody:\n**System information**\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\n- TensorFlow installed from (source or binary):\n- TensorFlow version (or github SHA if from source):\n\n\n**Provide the text output from tflite_convert**\n\n```\n# Copy and paste here\n```\n\n**Standalone code to reproduce the issue** \nProvide a reproducible test case that is the bare minimum necessary to generate\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\n\nAlso, please include a link to a GraphDef or the model if possible.\n\n**Any other info / logs**\n\nInclude any logs or source code that would be helpful to diagnose the problem.\nIf including tracebacks, please include the full traceback. Large logs and files\nshould be attached.",
  "Requirement ID: ISSUE-102052\nTitle: Spammer\nState: closed\nAuthor: matheushenrick304-code\nLabels: comp:lite\nBody:\n**System information**\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\n- TensorFlow installed from (source or binary):\n- TensorFlow version (or github SHA if from source):\n\n\n**Provide the text output from tflite_convert**\n\n```\n# Copy and paste here\n```\n\n**Standalone code to reproduce the issue** \nProvide a reproducible test case that is the bare minimum necessary to generate\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\n\nAlso, please include a link to a GraphDef or the model if possible.\n\n**Any other info / logs**\n\nInclude any logs or source code that would be helpful to diagnose the problem.\nIf including tracebacks, please include the full traceback. Large logs and files\nshould be attached.",
  "Requirement ID: ISSUE-102051\nTitle: Spammer\nState: closed\nAuthor: matheushenrick304-code\nLabels: TFLiteConverter\nBody:\n### 1. System information\n\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\n- TensorFlow installation (pip package or built from source):\n- TensorFlow library (version, if pip package or github SHA, if built from source):\n\n### 2. Code\n\nProvide code to help us reproduce your issues using one of the following options:\n\n#### Option A: Reference colab notebooks\n\n1)  Reference [TensorFlow Model Colab](https://colab.research.google.com/gist/ymodak/e96a4270b953201d5362c61c1e8b78aa/tensorflow-datasets.ipynb?authuser=1): Demonstrate how to build your TF model.\n2)  Reference [TensorFlow Lite Model Colab](https://colab.research.google.com/gist/ymodak/0dfeb28255e189c5c48d9093f296e9a8/tensorflow-lite-debugger-colab.ipynb): Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).\n\n```\n(You can paste links or attach files by dragging & dropping them below)\n- Provide links to your updated versions of the above two colab notebooks.\n- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.\n```\n\n#### Option B: Paste your code here or provide a link to a custom end-to-end colab\n\n```\n(You can paste links or attach files by dragging & dropping them below)\n- Include code to invoke the TFLite Converter Python API and the errors.\n- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.\n```\n\n### 3. Failure after conversion\nIf the conversion is successful, but the generated model is wrong, then state what is wrong:\n\n- Model produces wrong results and/or has lesser accuracy.\n- Model produces correct results, but it is slower than expected.\n\n### 4. (optional) RNN conversion support\nIf converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.\n\n### 5. (optional) Any other info / logs\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.",
  "Requirement ID: ISSUE-102050\nTitle: Remove an unnecessary mutex lock\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nRemove an unnecessary mutex lock",
  "Requirement ID: ISSUE-102049\nTitle: [XLA:GPU] Allow unrolling for ReduceWindow with small window.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[XLA:GPU] Allow unrolling for ReduceWindow with small window.\n\nExperiments show that this performs still better than not unrolling.\nAlso rename the method MayPreventVectorization as the naming is misleading. The\nother logic makes sure that we can at least vectorize the stores, so this\nfunction should check whether there is an expected performance drop due to\nunrolling, not whether we may be able to vectorize loads.",
  "Requirement ID: ISSUE-102048\nTitle: tf.image.rgb_to_hsv and tf.image.hsv_to_rgb are not numerically reversible for large float32 inputs\nState: open\nAuthor: syan427\nLabels: type:bug, TF 2.13\nBody:\n### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.13.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.9\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nFor very large float32 RGB values (on the order of 1e38), the pair of functions\ntf.image.rgb_to_hsv \u2192 tf.image.hsv_to_rgb fails to recover the original input values.\n\ntf.image.rgb_to_hsv() produces finite outputs.\n\ntf.image.hsv_to_rgb() does not return the same RGB tensor \u2014 values differ significantly even though both functions should be approximately reversible.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport numpy as np\nimport tensorflow as tf\n\nx = np.array([[[1.0870518e+38, 2.0447458e+38, 6.2133610e+37]]],\ndtype=np. float32)\ntf_result1 = tf.image.rgb_to_hsv(x)\n#[[[3.3333334e-01, 6.9613039e-01, 2.0447458e+38]]]\ntf_result2 = tf.image.hsv_to_rgb(tf_result1)\n#[[[6.2133610e+37, 2.0447458e+38,6.2133610e+37]]]\n\nprint(tf_result1)\nprint(tf_result2)\n```\n\n### Relevant log output\n\n```shell\n[[[3.3333334e-01, 6.9613039e-01, 2.0447458e+38]]]\n[[[6.2133610e+37, 2.0447458e+38,6.2133610e+37]]]\n```",
  "Requirement ID: ISSUE-102047\nTitle: tf.signal.ifft and tf.raw_ops.IFFT produce inconsistent results on large complex128 inputs\nState: open\nAuthor: syan427\nLabels: type:bug, TF 2.13\nBody:\n### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.13.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.9\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nWhen performing an inverse FFT (ifft) on a complex128 array with a very large magnitude input, the high-level API tf.signal.ifft and the low-level op tf.raw_ops.IFFT produce inconsistent results:\n\ntf.signal.ifft returns a finite complex number (expected behavior)\n\ntf.raw_ops.IFFT returns inf + nan j, indicating a numerical instability or missing normalization\n\n### Standalone code to reproduce the issue\n\n```shell\nimport numpy as np\nimport tensorflow as tf\n\nx = np.array([6.80538983e+307 + 0.00000000e+000j], dtype=np.complex128)\nresult1 = tf.signal.ifft (x) # [6.80538983e+307 0.j]\nresult2 = tf.raw_ops.IFFT(input=x) # [inf + nan j]\n\nprint(result1)\nprint(result2)\n```\n\n### Relevant log output\n\n```shell\n[6.80538983e+307 0.j]\n[inf + nan j]\n```",
  "Requirement ID: ISSUE-102046\nTitle: tf.distribute.Strategy.reduce(\"SUM\") produces inf while tf.math.reduce_sum gives correct finite result on float16\nState: open\nAuthor: syan427\nLabels: type:bug, TF 2.13\nBody:\n### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.13.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.9\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nWhen performing a reduction across multiple GPU replicas using tf.distribute.MirroredStrategy.reduce(\"SUM\") with float16 tensors, the result becomes inf, even though the mathematically equivalent non-distributed reduction (tf.math.reduce_sum) returns a correct finite value.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport numpy as np\nimport tensorflow as tf\n\n# GPU device list\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\ntensor_list = [\ntf.constant([-3148., -16752.], dtype=tf.float16),\ntf.constant([62624., 16464.], dtype=tf.float16)]\nd_result = strategy.reduce(\"SUM\", tensor_list, axis=0) # [inf]\ns_result = tf.math.reduce_sum(tensor_list) # [59200.]\n\nprint(d_result)\nprint(s_result)\n```\n\n### Relevant log output\n\n```shell\n[inf]\n[59200.]\n```",
  "Requirement ID: ISSUE-102045\nTitle: PR #32475: [ROCm] Prepare asan builds to be rbe compatible, include sanitizer ignore lists as data dpependency\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nPR #32475: [ROCm] Prepare asan builds to be rbe compatible, include sanitizer ignore lists as data dpependency\n\nImported from GitHub PR https://github.com/openxla/xla/pull/32475\n\n\ud83d\udcdd Summary of Changes\nMake asan builds hermetic so they can be used with rbe\n\n\ud83c\udfaf Justification\nAdd sanitizer ignore lists as a dependency to run_under script so they are available in rbe worker\n\n\ud83d\ude80 Kind of Contribution\nPlease remove what does not apply: \ud83d\udc1b Bug Fix, \u267b\ufe0f Cleanup\n\n\ud83d\udcca Benchmark (for Performance Improvements)\nnot relevant\n\n\ud83e\uddea Unit Tests:\nnot relevant\n\n\ud83e\uddea Execution Tests:\nnot relevant\n\nCopybara import of the project:\n\n--\ncae2ea8d4808c161becb80602fba605ba08a4bd5 by Alexandros Theodoridis <atheodor@amd.com>:\n\nAdjust ci script to include asan ignore list as deps\n\nMerging this change closes #32475\n\nFUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/32475 from ROCm:ci_start_using_rbe_workers_rocm_upstream cae2ea8d4808c161becb80602fba605ba08a4bd5",
  "Requirement ID: ISSUE-102044\nTitle: tf.math.not_equal gives inconsistent results between dense and RaggedTensor inputs\nState: closed\nAuthor: syan427\nLabels: type:bug, TF 2.13\nBody:\n### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.13.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.9\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\ntf.math.not_equal produces inconsistent results when comparing a scalar int with a tf.RaggedTensor of type uint8, compared to the equivalent dense tf.Tensor.\n\nWhen the same scalar and array are used, one position in the RaggedTensor version incorrectly returns False, while all positions in the dense version correctly return True.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport numpy as np\nimport tensorflow as tf\n\nx=2565065840\ny1 = np.array([[160, 41, 112, 39, 242, 192, 44]], dtype=np.uint8)\ny2 = tf.RaggedTensor.from_tensor(y1)\nresult1 = tf.math.not_equal(x, y1)\n# [True, True, True, True, True, True, True]\nprint(result1)\nresult2 = tf.math.not_equal(x, y2)\n# [True, True, False, True, True, True, True]\nprint(result2)\n```\n\n### Relevant log output\n\n```shell\n[True, True, True, True, True, True, True]\n[True, True, False, True, True, True, True]\n```",
  "Requirement ID: ISSUE-102043\nTitle: Incorrect result when using XLA (jit_compile=True) with tf.math.reduce_mean on int32 tensors\nState: open\nAuthor: syan427\nLabels: type:bug, comp:xla\nBody:\n### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.13.0; tf 2.19.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.9\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nWhen computing the mean of an int32 tensor with XLA compilation enabled (jit_compile=True), the result differs from the non-XLA version, even though the same inputs and operations are used. tf.math.reduce_mean should produce the same numerical result regardless of whether XLA is enabled (jit_compile=True) or not.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport numpy as np\nimport tensorflow as tf\n\n@ tf.function(jit_compile=False)\ndef run_without_xla(a):\n    return tf.math.reduce_mean(a)\n@ tf.function(jit_compile=True)\ndef run_with_xla(a):\n    return tf.math.reduce_mean(a)\nx = np. array([-1259020531, -29136942, 600128499, 1295813448,-1325241620,-1204075648, -722710057,-1923428316,783823385, -1364058804,1368162570,-470258899, -1730468409], dtype=np.int32)\nresult1 = run_without_xla(x) # [-460036255]\nprint(result1)\nresult2 = run_with_xla(x) # [-129654156]\nprint(result2)\n```\n\n### Relevant log output\n\n```shell\n[-460036255]\n[-129654156]\n```",
  "Requirement ID: ISSUE-102042\nTitle: Add function to convert `ConvolutionKind` proto to the c++ enum\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAdd function to convert `ConvolutionKind` proto to the c++ enum\n\nFor the `ConvolutionThunk` (de)serialisation we need to make the `GpuConvDescriptor` serializable, and for that we need `CudnnConvKind` too.\n\nA couple additional changes:\n\n* Renamed the existing c++ enum to proto enum to a (hopefully) more readable name.\n* Enforce that all c++ enums can be mapped to the proto version at compile time. I can't think of a case where we wouldn't want this, and with this change we can get rid of some non-ok Status invariants.",
  "Requirement ID: ISSUE-102041\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102040\nTitle: Fix `CreateFileMappingA()` return value check in `mmap_handle.cc`\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nFix `CreateFileMappingA()` return value check in `mmap_handle.cc`\n\nThis also:\n- improves the log when a `HANDLE` cannot be converted to a file descriptor by\n  calling `strerror`;\n- simplifies the code that sanitizes the handle name.",
  "Requirement ID: ISSUE-102039\nTitle: [XLA:GPU] Add experimental buffer checksum tracing\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[XLA:GPU] Add experimental buffer checksum tracing\n\nA debugging tool meant to pinpoint nondeterministic computations by finding\ndifferences in buffer values across multiple runs. It makes XLA calculate\nchecksums of input/output buffers, and dump them to the output directory.\n\nEnabling the new `--xla_gpu_experimental_enable_checksum_tracing_on_thunks`\nflag enables a new ThunkChecksumTracingPass, which adds checksum thunks to the\nthunk graph:\n\n- Inserts SDC log initialization to beginning.\n- Replaces each thunk with a SequentialThunk [checksum inputs, run original\n  thunk, checksum outputs].\n- Inserts a thunk that dumps SDC log to a file at the end of execution.\n\nFUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/101579 from ILCSFNO:patch-5 453a6e5f708b7c08de31a75a7ab86e3fabe53073",
  "Requirement ID: ISSUE-102038\nTitle: Add `Proto` suffix to proto types to avoid name conflicts with corresponding cpp types as both are under `xla` namespace.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAdd `Proto` suffix to proto types to avoid name conflicts with corresponding cpp types as both are under `xla` namespace.\n\n#hloshardingv3",
  "Requirement ID: ISSUE-102037\nTitle: [XLA:GPU] Make sure to insert parameter copies if they feed Mosaic GPU collectives\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[XLA:GPU] Make sure to insert parameter copies if they feed Mosaic GPU collectives\n\nOtherwise the kernels don't actually get the operands in symmetric memory and can crash.",
  "Requirement ID: ISSUE-102036\nTitle: [XLA:GPU] ThunkPassPipeline: pass HloModule* to Run()\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[XLA:GPU] ThunkPassPipeline: pass HloModule* to Run()\n\nThis allows SDC log dumper to derive unique path for each module execution.\n\nReverts 5a3a4bcd44baf08c22af3f007f9c28d75c8ec405",
  "Requirement ID: ISSUE-102035\nTitle: Update nn_impl.py to fix the check between `dilation rate` and `strides`\nState: open\nAuthor: ILCSFNO\nLabels: awaiting review, comp:ops, size:XS, prtype:bugfix\nBody:\nFixes #100237",
  "Requirement ID: ISSUE-102034\nTitle: Fix the axis issue in func tf.raw_ops.Dequantize()\nState: open\nAuthor: ILCSFNO\nLabels: awaiting review, ready to pull, size:S, prtype:bugfix, comp:core\nBody:\nFixes #100175\r\n\r\nNot known how to express the aspect: `what axis serves as`.\r\n\r\nIf something should be changed, please tell me! Thanks a lot!",
  "Requirement ID: ISSUE-102033\nTitle: [XLA:GPU] Allow unrolling for ReduceWindow with small window.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[XLA:GPU] Allow unrolling for ReduceWindow with small window.\n\nExperiments show that this performs still better than not unrolling.\nAlso rename the method MayPreventVectorization as the naming is misleading. The\nother logic makes sure that we can at least vectorize the stores, so this\nfunction should check whether there is an expected performance drop due to\nunrolling, not whether we may be able to vectorize loads.",
  "Requirement ID: ISSUE-102032\nTitle: [XLA:CPU][XTile] Add lowering for StableHLO DotGeneral.\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[XLA:CPU][XTile] Add lowering for StableHLO DotGeneral.",
  "Requirement ID: ISSUE-102031\nTitle: [XLA:GPU] Add functions to allocate memory with VMM API.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[XLA:GPU] Add functions to allocate memory with VMM API.",
  "Requirement ID: ISSUE-102030\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102029\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102028\nTitle: Fix the descriptions of limitation in several RFFT funcs\nState: open\nAuthor: ILCSFNO\nLabels: awaiting review, ready to pull, size:S, prtype:bugfix, comp:core\nBody:\nFixes #102027\r\n\r\nOnly fix the limitation. The two issues in `Something more` is not dealt.\r\n\r\nIf needed to be changed, please tell me, thanks!",
  "Requirement ID: ISSUE-102027\nTitle: Fix the descriptions of limitation in several RFFT funcs\nState: open\nAuthor: ILCSFNO\nLabels: type:docs-bug, type:bug, comp:apis, TF 2.19\nBody:\n### Issue type\n\nDocumentation Bug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf v2.20.0-rc0-4-g72fbba3d20f 2.20.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04\n\n### Mobile device\n\nLinux Ubuntu 22.04\n\n### Python version\n\n3.9\n\n### Bazel version\n\nNone\n\n### GCC/compiler version\n\nNone\n\n### CUDA/cuDNN version\n\nNone\n\n### GPU model and memory\n\nNone\n\n### Current behavior?\n\nThe doc of [tf.raw_ops.RFFT()](https://tensorflow.google.cn/api_docs/python/tf/raw_ops/RFFT), [tf.raw_ops.RFFT2D()](https://tensorflow.google.cn/api_docs/python/tf/raw_ops/RFFT2D), [tf.raw_ops.RFFT3D()](https://tensorflow.google.cn/api_docs/python/tf/raw_ops/RFFT3D), [tf.raw_ops.RFFTND()](https://tensorflow.google.cn/api_docs/python/tf/raw_ops/RFFTND) shows its description as below:\n\nhttps://github.com/tensorflow/tensorflow/blob/197b1c2454e94c9ba1e1eac14063c2c242716527/tensorflow/core/api_def/base_api/api_def_RFFT.pbtxt#L1-L14\n\nhttps://github.com/tensorflow/tensorflow/blob/197b1c2454e94c9ba1e1eac14063c2c242716527/tensorflow/core/api_def/base_api/api_def_RFFT2D.pbtxt#L1-L14\n\nhttps://github.com/tensorflow/tensorflow/blob/197b1c2454e94c9ba1e1eac14063c2c242716527/tensorflow/core/api_def/base_api/api_def_RFFT3D.pbtxt#L1-L14\n\nhttps://github.com/tensorflow/tensorflow/blob/197b1c2454e94c9ba1e1eac14063c2c242716527/tensorflow/core/api_def/base_api/api_def_RFFTND.pbtxt#L1-L14\n\nFor the repro below, I accept that it should raise error, but this limit of dtype should be noted in the docstring.\n\n### Repro 1\n```python\nimport tensorflow as tf\ninput_tensor = tf.random.uniform(shape=[10], minval=(-10), maxval=10, dtype=tf.float32)\nfft_length = tf.constant([10], dtype=tf.int32)\nresult = tf.raw_ops.RFFT(input=input_tensor, fft_length=fft_length, Tcomplex=tf.complex128)\nprint(result)\n```\n### Output 1\n```text\nInvalidArgumentError: {{function_node __wrapped__RFFT_device_/job:localhost/replica:0/task:0/device:GPU:0}} Wrong types for forward real FFT: in=1 out=18 [Op:RFFT] name: \n```\n### Repro 2\n```python\nimport tensorflow as tf\ninput_tensor = tf.random.uniform(shape=(10, 10), minval=(-10), maxval=10, dtype=tf.float32)\nfft_length = tf.constant([10, 10], dtype=tf.int32)\noutput_tensor = tf.raw_ops.RFFT2D(input=input_tensor, fft_length=fft_length, Tcomplex=tf.complex128)\nprint(output_tensor)\n```\n### Output 2\n```text\nInvalidArgumentError: {{function_node __wrapped__RFFT2D_device_/job:localhost/replica:0/task:0/device:GPU:0}} Wrong types for forward real FFT: in=1 out=18 [Op:RFFT2D] name: \n```\n### Repro 3\n```python\nimport tensorflow as tf\ninput_tensor = tf.random.uniform(shape=(10, 10, 10), minval=(-10), maxval=10, dtype=tf.float32)\nfft_length = tf.convert_to_tensor([10, 10, 10], dtype=tf.int32)\nresult = tf.raw_ops.RFFT3D(input=input_tensor, fft_length=fft_length, Tcomplex=tf.complex128)\nprint(result)\n```\n### Output 3\n```text\nInvalidArgumentError: {{function_node __wrapped__RFFT3D_device_/job:localhost/replica:0/task:0/device:GPU:0}} Wrong types for forward real FFT: in=1 out=18 [Op:RFFT3D] name: \n```\n### Repro 4\n```python\nimport tensorflow as tf\ninput_tensor = tf.random.uniform(shape=(10, 10, 10), minval=(-10), maxval=10, dtype=tf.float32)\nfft_length = tf.convert_to_tensor([10, 10, 10], dtype=tf.int32)\naxes = tf.convert_to_tensor([0, 1, 2], dtype=tf.int32)\nresult = tf.raw_ops.RFFTND(input=input_tensor, axes=axes, fft_length=fft_length, Tcomplex=tf.complex128)\nprint(result)\n```\n### Output 4\n```text\nInvalidArgumentError: {{function_node __wrapped__RFFTND_device_/job:localhost/replica:0/task:0/device:GPU:0}} Wrong types for forward real FFT: in=1 out=18 [Op:RFFTND] name:\n```\n\n### Something more\n* The error raised is not clear enough while using the number `1`, `18` instead of `DT_FLOAT`, `DT_DOUBLE`, `DT_COMPLEX64`, `DT_COMPLEX128`. But not found a convenient way to transfer this.\n* There are some extra arg here, not sure if is needed: (will not fix in the PR later, if needed to be removed, please tell me! Thanks.)\n\nhttps://github.com/tensorflow/tensorflow/blob/197b1c2454e94c9ba1e1eac14063c2c242716527/tensorflow/core/api_def/base_api/api_def_RFFT.pbtxt#L15-L26\n\nhttps://github.com/tensorflow/tensorflow/blob/197b1c2454e94c9ba1e1eac14063c2c242716527/tensorflow/core/api_def/base_api/api_def_RFFT2D.pbtxt#L15-L27\n\nhttps://github.com/tensorflow/tensorflow/blob/197b1c2454e94c9ba1e1eac14063c2c242716527/tensorflow/core/api_def/base_api/api_def_RFFT3D.pbtxt#L15-L27\n\nhttps://github.com/tensorflow/tensorflow/blob/197b1c2454e94c9ba1e1eac14063c2c242716527/tensorflow/core/api_def/base_api/api_def_RFFTND.pbtxt#L21-L31\n\nThis phenomenon is also in `FFT`, `FFT2D`, `FFT3D`, not listed here for the length.\n\nThanks for noting!\n\n### Standalone code to reproduce the issue\n\n```shell\nSee above for not one repro.\n```\n\n### Relevant log output\n\n```shell\nSee above for not one output.\n```",
  "Requirement ID: ISSUE-102026\nTitle: PR #32525: [XLA:GPU] Fix block scaled dot global scaling for older cuDNN versions\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nPR #32525: [XLA:GPU] Fix block scaled dot global scaling for older cuDNN versions\n\nImported from GitHub PR https://github.com/openxla/xla/pull/32525\n\n\ud83d\udcdd Summary of Changes\nPass cuDNN version to the `BlockScalingRewriter` pass, and make lowering decisions based on that.\n\n\ud83c\udfaf Justification\nThe global scaling factor doesn't work before cuDNN v9.13 (the graph is compiled, but the scaling factor is not applied).\nUse the slower lowering (apply global scaling factor outside the fusion) in this case.\n\n\ud83d\ude80 Kind of Contribution\n\ud83d\udc1b Bug Fix\n\nCopybara import of the project:\n\n--\na47ef5175d076270e371c9e5cf355fc1ad96efc8 by Sergey Kozub <skozub@nvidia.com>:\n\n[XLA:GPU] Fix block scaled dot global scaling for older cuDNN versions\n\nMerging this change closes #32525\n\nFUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/32525 from openxla:skozub/block_scaling_cudnn_version a47ef5175d076270e371c9e5cf355fc1ad96efc8",
  "Requirement ID: ISSUE-102025\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102024\nTitle: Split `ConvolutionReorderThunk` into its own file\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nSplit `ConvolutionReorderThunk` into its own file\n\nThere's no shared logic in between the `ConvolutionReorderThunk` and the `ConvolutionThunk` thunk, so I think its cleaner for each to be defined in their own file.",
  "Requirement ID: ISSUE-102023\nTitle: PR #32475: [ROCm] Prepare asan builds to be rbe compatible, include sanitizer ignore lists as data dpependency\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nPR #32475: [ROCm] Prepare asan builds to be rbe compatible, include sanitizer ignore lists as data dpependency\n\nImported from GitHub PR https://github.com/openxla/xla/pull/32475\n\n\ud83d\udcdd Summary of Changes\nMake asan builds hermetic so they can be used with rbe\n\n\ud83c\udfaf Justification\nAdd sanitizer ignore lists as a dependency to run_under script so they are available in rbe worker\n\n\ud83d\ude80 Kind of Contribution\nPlease remove what does not apply: \ud83d\udc1b Bug Fix, \u267b\ufe0f Cleanup\n\n\ud83d\udcca Benchmark (for Performance Improvements)\nnot relevant\n\n\ud83e\uddea Unit Tests:\nnot relevant\n\n\ud83e\uddea Execution Tests:\nnot relevant\n\nCopybara import of the project:\n\n--\ncae2ea8d4808c161becb80602fba605ba08a4bd5 by Alexandros Theodoridis <atheodor@amd.com>:\n\nAdjust ci script to include asan ignore list as deps\n\nMerging this change closes #32475\n\nFUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/32475 from ROCm:ci_start_using_rbe_workers_rocm_upstream cae2ea8d4808c161becb80602fba605ba08a4bd5",
  "Requirement ID: ISSUE-102022\nTitle: [XLA:CPU][XTile] Create simple lowering for tiled ops.\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[XLA:CPU][XTile] Create simple lowering for tiled ops.",
  "Requirement ID: ISSUE-102021\nTitle: Add proto (de)serialisation for `ConvolutionThunk`\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAdd proto (de)serialisation for `ConvolutionThunk`",
  "Requirement ID: ISSUE-102020\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change\n\nFUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/101425 from tensorflow:dependabot/docker/tensorflow/tools/tf_sig_build_dockerfiles/ubuntu-4e0171b 46cc84e36116c82a76c74210bd62fe88b30a42b9",
  "Requirement ID: ISSUE-102019\nTitle: Use `xla::ifrt::HloSharding` for executable outputs\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nUse `xla::ifrt::HloSharding` for executable outputs",
  "Requirement ID: ISSUE-102018\nTitle: Revert PR #32366 because it contains a broken test.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nRevert PR #32366 because it contains a broken test.\n\nReverts ca808ff740a476b3e1c7a602c55b9a02accac373",
  "Requirement ID: ISSUE-102017\nTitle: Sort dumped execution files before checking.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nSort dumped execution files before checking.\n\nThe order of files returned by `GetMatchingPaths` is not guaranteed, so sorting ensures deterministic test behavior and should get rid of the flakyness of the test.\n\nAlso a tiny assertion cleanup for better error messages.",
  "Requirement ID: ISSUE-102016\nTitle: PR #32229: Changes to the compute capabilities to account for the two different Blackwell Edge GPUs.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nPR #32229: Changes to the compute capabilities to account for the two different Blackwell Edge GPUs.\n\nImported from GitHub PR https://github.com/openxla/xla/pull/32229\n\nRTX PRO 6000 has CC 12.0.\nSpark has CC 12.1.\n\nRemoved the IsAtLeastBlackwellPro method because there is no guarantee that future data center GPUs will have CC higher than 12.0.\n\nAlso skipped the latency estimator test on Edge GPUs because it uses the collective performance model and crashes here:\nhttps://github.com/openxla/xla/blob/784702574ee3f2df06116e7f7e4b837150c8694a/xla/service/gpu/model/gpu_collective_performance_model.cc#L239\nCopybara import of the project:\n\n--\nca47c656de78f8c5385dcf77b7454d7adc774203 by Dimitris Vardoulakis <dvardoulakis@nvidia.com>:\n\nSome Spark fixes.                                                                                                                                                                                                                                                             Rename kBlackwellPro to kBlackwell12, as the sm_12x compute capabilities also include Spark.\n\nFix the latency estimator test and the gemm fusion autotuner test for Spark.\n\nRemoved the IsAtLeastBlackwellPro method because there is no guarantee that future\ndata center GPUs will have CC higher than 12.0.\n\nMerging this change closes #32229\n\nFUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/32229 from dimvar:spark-cc ca47c656de78f8c5385dcf77b7454d7adc774203",
  "Requirement ID: ISSUE-102015\nTitle: [IFRT]Expand visibility of `mpmd_executable`.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[IFRT]Expand visibility of `mpmd_executable`.\n\nAdds `friends`, `internal`, and `users` to the visibility list for the `mpmd_executable` target.",
  "Requirement ID: ISSUE-102014\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102013\nTitle: [IFRT] Migrate `Array::pjrt_layout()` callers to interpret `nullptr` as a default layout\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[IFRT] Migrate `Array::pjrt_layout()` callers to interpret `nullptr` as a default layout\n\n`Array::pjrt_layout()` will be changed to return `nullptr` to indicate a default layout, where the callers can obtain the corresponding concrete default layout by using `Client::GetDefaultPjRtLayout()`.\n\nThis change adds `nullptr` handling preemptively before the new `Array::pjrt_layout()` semantics becomes effective so that the existing code works as before.\n\nTests using `Array::pjrt_layout()` method calls are minimally updated to add a non-nullness check. They will be updated as `Array::pjrt_layout()` actually returns `nullptr`.",
  "Requirement ID: ISSUE-102012\nTitle: [IFRT] Migrate `Array::pjrt_layout()` callers to interpret `nullptr` as a default layout\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[IFRT] Migrate `Array::pjrt_layout()` callers to interpret `nullptr` as a default layout\n\n`Array::pjrt_layout()` will be changed to return `nullptr` to indicate a default layout, where the callers can obtain the corresponding concrete default layout by using `Client::GetDefaultPjRtLayout()`.\n\nThis change adds `nullptr` handling preemptively before the new `Array::pjrt_layout()` semantics becomes effective so that the existing code works as before.\n\nTests using `Array::pjrt_layout()` method calls are minimally updated to add a non-nullness check. They will be updated as `Array::pjrt_layout()` actually returns `nullptr`.\n\nFUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/32525 from openxla:skozub/block_scaling_cudnn_version a47ef5175d076270e371c9e5cf355fc1ad96efc8",
  "Requirement ID: ISSUE-102011\nTitle: #HLODiffService Add forced mapping options to HLO Diff backend.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\n#HLODiffService Add forced mapping options to HLO Diff backend.",
  "Requirement ID: ISSUE-102010\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102009\nTitle: Reverts 6cb439dba1140a72f42d8447764af5697cc03c07\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nReverts 6cb439dba1140a72f42d8447764af5697cc03c07",
  "Requirement ID: ISSUE-102008\nTitle: Add support for kTfLiteInt2 type export/import.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAdd support for kTfLiteInt2 type export/import.\n\nThis change introduces a new `kTfLiteInt2` type to the TFLite schema and MLIR converter. It includes:\n-   Adding `INT2` to the flatbuffer schema.\n-   Mapping `TensorType_INT2` to `kTfLiteInt2` in flatbuffer conversions.\n-   Updating `tflite_types.h` to include `kTfLiteInt2`.\n-   Modifying `flatbuffer_export.cc` to handle 2-bit integer types from MLIR and pack them densely.\n-   Generalizing low-bit utility functions (`PackLowBitValuesDensely`, `UnpackDenseLowBitIntoInt8`) to support both 2-bit and 4-bit values.\n-   Updating type conversion utilities to recognize and handle `kTfLiteInt2`.\n-   Adjusting `util.cc` to correctly report the size and byte requirements for `kTfLiteInt2` tensors, considering their dense packing.",
  "Requirement ID: ISSUE-102007\nTitle: SYCL is built by default using hermetic Clang. \nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nSYCL is built by default using hermetic Clang. \nGCC is no longer supported for SYCL builds.",
  "Requirement ID: ISSUE-102006\nTitle: Generalize `PackInt8IntoDenseInt4` to support 2-bit and 4-bit packing.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nGeneralize `PackInt8IntoDenseInt4` to support 2-bit and 4-bit packing.\n\nRename `PackInt8IntoDenseInt4` to `PackInt8IntoDenseInt` and add a `bit_width` parameter. Implement packing logic for both 2-bit and 4-bit integers. Update existing call sites in `quantize.cc` and `transpose.cc` to use the new function signature. Add new unit tests for both 2-bit and 4-bit packing.",
  "Requirement ID: ISSUE-102005\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102004\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102003\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102002\nTitle: Implement CreateLinkedEventPromise() for PjRtStreamExecutorClient.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nImplement CreateLinkedEventPromise() for PjRtStreamExecutorClient.",
  "Requirement ID: ISSUE-102001\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-102000\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-101999\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-101998\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-101997\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-101996\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-101995\nTitle: [tsl:concurrency] Add ABSL_ATTRIBUTE_ALWAYS_INLINE to Future::{Map,OnReady}\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[tsl:concurrency] Add ABSL_ATTRIBUTE_ALWAYS_INLINE to Future::{Map,OnReady}\n\n```\nname                        cpu/op        cpu/op      vs base\nBM_MapStatelessFuture       46.03n \u00b1 1%   36.93n \u00b1 0%  -19.76% (p=0.000 n=40)\nBM_TryMapStatelessFuture    46.15n \u00b1 1%   36.00n \u00b1 1%  -22.01% (p=0.000 n=40)\nBM_MapToFromStatelessFuture 49.54n \u00b1 0%   39.19n \u00b1 0%  -20.90% (p=0.000 n=40)\nBM_MapStatefulFuture        45.88n \u00b1 0%   39.97n \u00b1 3%  -12.87% (p=0.000 n=40)\nBM_TryMapStatefulFuture     52.14n \u00b1 0%   36.90n \u00b1 0%  -29.24% (p=0.000 n=40)\n```",
  "Requirement ID: ISSUE-101994\nTitle: #tf-data-service Fix a flaky test.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\n#tf-data-service Fix a flaky test.\n\nWhen sharing a job using dynamic sharding, one dataset may be empty.",
  "Requirement ID: ISSUE-101993\nTitle: Delete `xla/tests/fuzz`\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nDelete `xla/tests/fuzz`\n\nThese tests haven't demonstrated much value, and the generated HLOs are no longer used anywhere else.",
  "Requirement ID: ISSUE-101992\nTitle: [tsl] Add AsExecutor() adaptor to tsl::thread::ThreadPool\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[tsl] Add AsExecutor() adaptor to tsl::thread::ThreadPool\n\nIt's hard to construct ThreadPoolExecutor from a ThreadPool& reference and correctly manage the lifetime of executor. Instead make it possible to get a tsl::Executor adaptor from a ThreadPool instance.",
  "Requirement ID: ISSUE-101991\nTitle: LiteRT Support 1.4.0 ships libimage_processing_util_jni.so with 4 KB alignment \u2014 fails Play\u2019s 16 KB page-size requirement\nState: closed\nAuthor: lordo223\nLabels: type:bug, type:support, comp:lite, subtype:windows\nBody:\n### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\nLiteRT 1.4.0 (from com.google.ai.edge.litert:litert-support:1.4.0)\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nWindows 11 / Android Studio Koala | AGP 8.8.2 | Gradle 8.11.1\n\n### Mobile device\n\nWindows 11 / Android Studio Koala | AGP 8.8.2 | Gradle 8.11.1\n\n### Python version\n\nN/A\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nLiteRT Support 1.4.0 includes a native binary built for 4 KB memory pages, which fails Google Play\u2019s new 16 KB page-size validation.\n\nGoogle Play Console error:\n> App must support 16 KB memory page sizes. Libraries that do not support 16 KB:\n> base/lib/arm64-v8a/libimage_processing_util_jni.so\n\nChecked with llvm-readelf (NDK 29.0.13113456):\n== libimage_processing_util_jni.so ==\nLOAD ... Align 0x1000   (4 KB)\n== libtensorflowlite_jni.so ==\nLOAD ... Align 0x4000   (16 KB)\n\nAffected dependency:\nimplementation(\"com.google.ai.edge.litert:litert-support:1.4.0\")\nAll other LiteRT libraries (litert-api, litert-gpu) use proper 16 KB alignment.\n\nExpected behavior:\nAll LiteRT .so libraries should use 16 KB ELF alignment (-z max-page-size=16384) so Play Console validation passes on Android 15+.\n\n\nadditional info:\nNDK: 29.0.13113456\nSDK: C:\\Users\\<user>\\AppData\\Local\\Android\\Sdk\nAffected platform: Google Play AAB upload check (2025 enforcement)\n\n\n### Standalone code to reproduce the issue\n\n```shell\nsettings.gradle\n\npluginManagement {\n  repositories {\n    gradlePluginPortal()\n    google()\n    mavenCentral()\n  }\n}\ndependencyResolutionManagement {\n  repositoriesMode.set(RepositoriesMode.FAIL_ON_PROJECT_REPOS)\n  repositories {\n    google()\n    mavenCentral()\n  }\n}\nrootProject.name = \"tflite-16kb-repro\"\ninclude(\":app\")\n\n\nProject build.gradle (top level)\n\nplugins {\n  id(\"com.android.application\") version \"8.8.2\" apply false\n  id(\"org.jetbrains.kotlin.android\") version \"1.9.24\" apply false\n}\n\n\napp/build.gradle\n\nplugins {\n  id(\"com.android.application\")\n  id(\"org.jetbrains.kotlin.android\")\n}\n\nandroid {\n  namespace = \"com.example.repro\"\n  compileSdk = 35\n\n  defaultConfig {\n    applicationId = \"com.example.repro\"\n    minSdk = 24\n    targetSdk = 35\n    versionCode = 1\n    versionName = \"1.0\"\n  }\n\n  // NDK r29 installed via SDK Manager (side-by-side)\n  ndkVersion = \"29.0.13113456\"\n\n  // arm64 only (to keep the output tiny)\n  defaultConfig {\n    ndk {\n      abiFilters += listOf(\"arm64-v8a\")\n    }\n  }\n\n  buildTypes {\n    release { isMinifyEnabled = false }\n  }\n}\n\ndependencies {\n  // Repro set \u2014 the problematic .so comes from litert-support:\n  implementation(\"com.google.ai.edge.litert:litert-api:1.4.0\")\n  implementation(\"com.google.ai.edge.litert:litert-gpu:1.4.0\")\n  implementation(\"com.google.ai.edge.litert:litert-support:1.4.0\")\n}\n\n\nTiny Kotlin app (any Activity works)\napp/src/main/java/com/example/repro/MainActivity.kt\n\npackage com.example.repro\nimport android.os.Bundle\nimport androidx.appcompat.app.AppCompatActivity\n\nclass MainActivity : AppCompatActivity() {\n  override fun onCreate(savedInstanceState: Bundle?) {\n    super.onCreate(savedInstanceState)\n  }\n}\n\n\napp/src/main/AndroidManifest.xml\n\n<manifest package=\"com.example.repro\" xmlns:android=\"http://schemas.android.com/apk/res/android\">\n  <application android:label=\"repro\" android:allowBackup=\"false\">\n    <activity android:name=\".MainActivity\">\n      <intent-filter>\n        <action android:name=\"android.intent.action.MAIN\"/>\n        <category android:name=\"android.intent.category.LAUNCHER\"/>\n      </intent-filter>\n    </activity>\n  </application>\n</manifest>\n```\n\n### Relevant log output\n\n```shell\n# Paths\n$AAB  = \"app\\build\\outputs\\bundle\\release\\app-release.aab\"\n$OUT  = \"aab-extract\"\n$SDK  = \"$env:LOCALAPPDATA\\Android\\Sdk\"\n\n# Unpack .aab (PS needs .zip)\nCopy-Item $AAB \"app-release.zip\" -Force\nRemove-Item $OUT -Recurse -Force -ErrorAction SilentlyContinue\nExpand-Archive -Path \"app-release.zip\" -DestinationPath $OUT -Force\n\n# Find llvm-readelf from NDK r29 (or r28)\n$readelf = Get-ChildItem \"$SDK\\ndk\\29*\\toolchains\\llvm\\prebuilt\\windows-*\\bin\\llvm-readelf.exe\" -Recurse |\n           Select-Object -First 1 -ExpandProperty FullName\n\n# Show program headers (look at Align column)\nGet-ChildItem \"$OUT\\base\\lib\\arm64-v8a\\*.so\" | ForEach-Object {\n  \"`n== $($_.Name) ==\"; & $readelf -l $_.FullName | Select-String \"LOAD|Align|p_align\"\n}\nExpected output (problem):\n\nlua\nCopy code\n== libimage_processing_util_jni.so ==\n  LOAD ... Align 0x1000   <-- 4 KB (\u274c)\n\n== libtensorflowlite_jni.so ==\n  LOAD ... Align 0x4000   <-- 16 KB (\u2705)\n\n== libtensorflowlite_gpu_jni.so ==\n  LOAD ... Align 0x4000   <-- 16 KB (\u2705)\n```",
  "Requirement ID: ISSUE-101990\nTitle: PR #30127: [ROCM] Splitting gridDim.x for very large reductions / loop fusion kernels, added grid-stride loops for BufferComparator/RedzoneChecker\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nPR #30127: [ROCM] Splitting gridDim.x for very large reductions / loop fusion kernels, added grid-stride loops for BufferComparator/RedzoneChecker\n\nImported from GitHub PR https://github.com/openxla/xla/pull/30127\n\nWe had several failures on ROCM side due to very large gridDim.x values for fusion kernels.\nHere is the proposal to split gridDim.x into gridDim.x/y in these cases.\n\n1. added gridDim.x splitting for reduction and loop fusions to handle large grid sizes\n2. added grid-stride loops to buffer_comparator and redzone checker kernels\n3. extended gpu_kernel_tiling test and buffer_comparator_test\n4. improved error logging in rocm_stream.cc in case of kernel launch failure\n\nWe already had some rocm-related fixes for this issue: with this PR all fixes shall be unified\n\n@xla-rotation: could you have a look please ?\nCopybara import of the project:\n\n--\n9be23224961fe8d0a2ac65e4dc7bebb3bfc078db by Pavel Emeliyanenko <pavel.emeliyanenko@amd.com>:\n\nSplitting gridDim.x into x and y if necessary. Adding grid-stride loops for buffer comparator and redzone checker kernels\n\n--\n10573949c1902f640ad2ffaf49999291e8f1b27e by Pavel Emeliyanenko <pavel.emeliyanenko@amd.com>:\n\ntests fixes\n\n--\n7c5109ca7566f47d2e618f8721f620f41a3dd9ad by Pavel Emeliyanenko <pavel.emeliyanenko@amd.com>:\n\ntest fix\n\n--\nedad1f7249e51e49f094bb081a754c323fc2dce9 by Pavel Emeliyanenko <pavel.emeliyanenko@amd.com>:\n\ncleanup\n\n--\ne3202f3fc0930db02437597859e2a7c2f8819559 by Pavel Emeliyanenko <pavel.emeliyanenko@amd.com>:\n\ntest update\n\n--\n8296a13eb0812521daf7de567f1a9ab2e6be9361 by Pavel Emeliyanenko <pavel.emeliyanenko@amd.com>:\n\nsmall improvement int64_t -> uint64_t\n\n--\n9637ec937360162dae2d25a8a1b6a3ff91fc9daa by Pavel Emeliyanenko <pavel.emeliyanenko@amd.com>:\n\nprevent taking reference to temporary\n\n--\n60330186d3bdcedf98751bfa8f1193b0f9a02815 by Pavel Emeliyanenko <pavel.emeliyanenko@amd.com>:\n\nmore robust fix for buffer_compare test\n\n--\ndd1ee8ff31d580ef8eb300fafa432ae4d0e16261 by Pavel Emeliyanenko <pavel.emeliyanenko@amd.com>:\n\nupdate\n\nMerging this change closes #30127\n\nFUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/30127 from ROCm:ci_reduction_fix_11.08.25 dd1ee8ff31d580ef8eb300fafa432ae4d0e16261",
  "Requirement ID: ISSUE-101989\nTitle: Increase timeout for `gpu_spmd_e2e_compile_test`.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nIncrease timeout for `gpu_spmd_e2e_compile_test`.",
  "Requirement ID: ISSUE-101988\nTitle: Support SparseActivationsUnstack and SparseActivationsUnstackInterleaved custom call always return tuple result\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nSupport SparseActivationsUnstack and SparseActivationsUnstackInterleaved custom call always return tuple result",
  "Requirement ID: ISSUE-101987\nTitle: Add a metric on enqueue request size\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAdd a metric on enqueue request size",
  "Requirement ID: ISSUE-101986\nTitle: Add a CHECK and remove unused parameters from SPMD dot partitioning functions.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAdd a CHECK and remove unused parameters from SPMD dot partitioning functions.",
  "Requirement ID: ISSUE-101985\nTitle: Switch the default nonsymmetric eigendecomposition implementation to use cusolver.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nSwitch the default nonsymmetric eigendecomposition implementation to use cusolver.\n\nThis regresses a test that checks that eigendecompositions behave well if provided NaNs as input. I think this is the right tradeoff, however, since the cusolver implementation should be faster and the old implementation is still available.\n\nDeprecate the use_magma argument to `lax.linalg.eig`, replace it with an `implementation` argument.\n\nThis PR builds on an partial PR from @dfm.\n\nFixes https://github.com/jax-ml/jax/issues/27265\n\nFUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/32389 from Tixxx:tixxx/pipeliner_opt_barrier 4273b194da9ca66b4ebcaa959bcc8119d8c3eff7",
  "Requirement ID: ISSUE-101984\nTitle: Add a unit test for revisiting elements after garbage collection.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAdd a unit test for revisiting elements after garbage collection.",
  "Requirement ID: ISSUE-101983\nTitle: Adds a comment to clarify the case when `job_name` and `dataset_id`  are both present\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAdds a comment to clarify the case when `job_name` and `dataset_id`  are both present",
  "Requirement ID: ISSUE-101982\nTitle: PR #32488: Check against CUDA 13.1. Was missed in the previous merge.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nPR #32488: Check against CUDA 13.1. Was missed in the previous merge.\n\nImported from GitHub PR https://github.com/openxla/xla/pull/32488\n\n\nCopybara import of the project:\n\n--\n73ff211ad0e167e36cec7c24d7868c74aa2dcba1 by Dimitris Vardoulakis <dvardoulakis@nvidia.com>:\n\nCheck against CUDA 13.1. Was missed in the previous merge.\n\nMerging this change closes #32488\n\nFUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/32488 from dimvar:ptx-9-cuda-13 73ff211ad0e167e36cec7c24d7868c74aa2dcba1",
  "Requirement ID: ISSUE-101981\nTitle: Remove MLIR-related unused code from CollectiveThunk\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nRemove MLIR-related unused code from CollectiveThunk\n\nThe removed data fields are not used and always initialized from a nullptr.\nThe removed functions are not called from anywhere. So let's remove all of that.",
  "Requirement ID: ISSUE-101980\nTitle: [xla:cpu] Delete unused APIs from cpu_function_runtime\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[xla:cpu] Delete unused APIs from cpu_function_runtime",
  "Requirement ID: ISSUE-101979\nTitle: Remove unused nvtx_utils target\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nRemove unused nvtx_utils target\n\nThe code is not in use anymore and hasn't been for quite a while. Let's remove it.",
  "Requirement ID: ISSUE-101978\nTitle: [xla:cpu] Create xla::cpu::BufferAllocationInfo from deprecated cpu_function_runtime::BufferInfo\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[xla:cpu] Create xla::cpu::BufferAllocationInfo from deprecated cpu_function_runtime::BufferInfo",
  "Requirement ID: ISSUE-101977\nTitle: [XLA:GPU][codegen] Emit tensor dialect for splat and lower to equivalent triton op.\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[XLA:GPU][codegen] Emit tensor dialect for splat and lower to equivalent triton op.",
  "Requirement ID: ISSUE-101976\nTitle: [TSL] Add cusolverDnXgeev_bufferSize and cusolverDnXgeev to cusolver symbol list.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[TSL] Add cusolverDnXgeev_bufferSize and cusolverDnXgeev to cusolver symbol list.\n\nChange in preparation for adding support for calling this function from JAX.\n\nIssue https://github.com/jax-ml/jax/issues/27265\n\nFUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/32389 from Tixxx:tixxx/pipeliner_opt_barrier 4273b194da9ca66b4ebcaa959bcc8119d8c3eff7",
  "Requirement ID: ISSUE-101975\nTitle: PR #32357: Update gemma2 keras benchmark script - fix ttft, and use tokenizer\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nPR #32357: Update gemma2 keras benchmark script - fix ttft, and use tokenizer\n\nImported from GitHub PR https://github.com/openxla/xla/pull/32357\n\n\ud83d\udcdd Summary of Changes\n\n1. Update calculation for TTFT to be the time to first generated token. This will also impact TPOT calculations.\n2. Use tokenizer to count the number of tokens generated instead of counting words using space\n\n\ud83c\udfaf Justification\nCurrently the script computes TTFT as time to first token which is from the prompt and still in prefill stage.\n\n\ud83d\ude80 Kind of Contribution\n\ud83d\udc1b Bug Fix\nCopybara import of the project:\n\n--\n25178775f936a6f40a205e6969582222f150f0dd by Gauri Deshpande <gauri1.deshpande@intel.com>:\n\nUpdate gemma2 keras benchmark script - fix ttft, and use tokenizer\n\n--\n9b20ead588ad38152e648067c7d34314ba8a5645 by Gauri Deshpande <gauri1.deshpande@intel.com>:\n\naddress review comments\n\nMerging this change closes #32357\n\nFUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/32357 from Intel-tensorflow:gaurides/update_gemma2_benchmark_script 9b20ead588ad38152e648067c7d34314ba8a5645",
  "Requirement ID: ISSUE-101974\nTitle: TFLite: require matching int8 quantization params (scale & zero_point) for Transpose and ResizeBilinear\nState: open\nAuthor: adityajai119\nLabels: ready to pull, size:M\nBody:\n## Summary\r\n\r\nThis PR adds runtime checks to TensorFlow Lite kernels (`Transpose` and `ResizeBilinear`) to ensure that for int8-quantized tensors, the input and output must have identical per-tensor `scale` and `zero_point`.\r\n\r\n- If a mismatch is detected, the kernel logs an explanatory error and returns `kTfLiteError` before executing.\r\n- Includes new negative unit tests for both operators that construct int8 tensors with mismatched quantization and assert the operator fails early.\r\n\r\n## Motivation\r\n\r\n`Transpose` and `ResizeBilinear` rearrange data without rescaling. If quantization parameters differ between input and output, results can be silently incorrect. This change enforces proper model construction by failing fast with a diagnostic error, preventing invalid TFLite models from executing.\r\n\r\n## Files Changed\r\n\r\n**Modified:**\r\n- `transpose.cc`: add runtime check in `Eval` for `kTfLiteInt8`\r\n- `resize_bilinear.cc`: add runtime check in `Eval` for `kTfLiteInt8`\r\n\r\n**Tests added:**\r\n- `transpose_test.cc`: adds `Int8MismatchedQuantizationFails` unit test\r\n- `resize_bilinear_test.cc`: adds `Int8MismatchedQuantizationFails` unit test\r\n\r\n## Behavior & Message\r\n\r\nWhen a mismatch is detected, the kernel will:\r\n- Log via:  \r\n  `TF_LITE_KERNEL_LOG(context, \"Input and output tensors must have the same scale and zero_point for int8 quantized <OpName>.\");`\r\n- Abort operation by returning `kTfLiteError`\r\n\r\n## Testing & Validation\r\n\r\n- Local Bazel tests (run inside Codespace):\r\n  - `//tensorflow/lite/kernels:resize_bilinear_test` \u2014 PASSED\r\n  - `//tensorflow/lite/kernels:transpose_test` \u2014 PASSED\r\n\r\n\r\n## Related Issue\r\nCloses #94333\r\n\r\n## Changelog\r\nRequire matched int8 quantization (scale, zero_point) for TFLite Transpose and ResizeBilinear. Adds tests for error cases.",
  "Requirement ID: ISSUE-101973\nTitle: TFLite: require matching int8 quantization params (scale & zero_point) for Transpose and ResizeBilinear\nState: closed\nAuthor: adityajai119\nLabels: size:M\nBody:\n## Summary\r\n\r\nThis PR adds runtime checks to TensorFlow Lite kernels (`Transpose` and `ResizeBilinear`) to ensure that for int8-quantized tensors, the input and output must have identical per-tensor `scale` and `zero_point`.\r\n\r\n- If a mismatch is detected, the kernel logs an explanatory error and returns `kTfLiteError` before executing.\r\n- Includes new negative unit tests for both operators that construct int8 tensors with mismatched quantization and assert the operator fails early.\r\n\r\n## Motivation\r\n\r\n`Transpose` and `ResizeBilinear` rearrange data without rescaling. If quantization parameters differ between input and output, results can be silently incorrect. This change enforces proper model construction by failing fast with a diagnostic error, preventing invalid TFLite models from executing.\r\n\r\n## Files Changed\r\n\r\n**Modified:**\r\n- `transpose.cc`: add runtime check in `Eval` for `kTfLiteInt8`\r\n- `resize_bilinear.cc`: add runtime check in `Eval` for `kTfLiteInt8`\r\n\r\n**Tests added:**\r\n- `transpose_test.cc`: adds `Int8MismatchedQuantizationFails` unit test\r\n- `resize_bilinear_test.cc`: adds `Int8MismatchedQuantizationFails` unit test\r\n\r\n## Behavior & Message\r\n\r\nWhen a mismatch is detected, the kernel will:\r\n- Log via:  \r\n  `TF_LITE_KERNEL_LOG(context, \"Input and output tensors must have the same scale and zero_point for int8 quantized <OpName>.\");`\r\n- Abort operation by returning `kTfLiteError`\r\n\r\n## Testing & Validation\r\n\r\n- Local Bazel tests (run inside Codespace):\r\n  - `//tensorflow/lite/kernels:resize_bilinear_test` \u2014 PASSED\r\n  - `//tensorflow/lite/kernels:transpose_test` \u2014 PASSED\r\n\r\n\r\n## Related Issue\r\nCloses #94333\r\n\r\n## Changelog\r\nRequire matched int8 quantization (scale, zero_point) for TFLite Transpose and ResizeBilinear. Adds tests for error cases.",
  "Requirement ID: ISSUE-101972\nTitle: PR #32389: [NVIDIA GPU] Change collective pipeliner to honor opt-barrier\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nPR #32389: [NVIDIA GPU] Change collective pipeliner to honor opt-barrier\n\nImported from GitHub PR https://github.com/openxla/xla/pull/32389\n\n\ud83d\udcdd Summary of Changes\nThe current collective pipeliners don't honor opt-barrier in their user/producer chain.\nThis pr changes the behavior to teach collective pipeliners to use acceptable_formatting function to filter out what cannot be pipelined.\n\ud83c\udfaf Justification\nWithout this, opt-barriers connected with target collectives will be peeled and pipelined to other loop iterations which changes the semantic of the whole program.\n\n\ud83d\ude80 Kind of Contribution\n \ud83d\udc1b Bug Fix\n\n\ud83d\udcca Benchmark (for Performance Improvements)\nNA\n\n\ud83e\uddea Unit Tests:\nAdded unit tests\n\n\ud83e\uddea Execution Tests:\nNA\n\nCopybara import of the project:\n\n--\n4273b194da9ca66b4ebcaa959bcc8119d8c3eff7 by TJ Xu <tjx@nvidia.com>:\n\nChange collective pipeliner to honor opt-barrier\n\nMerging this change closes #32389\n\nFUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/32389 from Tixxx:tixxx/pipeliner_opt_barrier 4273b194da9ca66b4ebcaa959bcc8119d8c3eff7",
  "Requirement ID: ISSUE-101971\nTitle: Bug fixes to enable device loop for step tracking\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nBug fixes to enable device loop for step tracking",
  "Requirement ID: ISSUE-101970\nTitle: PR #32336: [ROCm] Move cupti_tracer to cuda dependencies in py_hlo_multihost_runner target\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nPR #32336: [ROCm] Move cupti_tracer to cuda dependencies in py_hlo_multihost_runner target\n\nImported from GitHub PR https://github.com/openxla/xla/pull/32336\n\n\ud83d\udcdd Summary of Changes\nMove cupti_tracer to cuda dependencies in py_hlo_multihost_runner target\n\n\ud83c\udfaf Justification\nThis PR fixes building py_hlo_multihost_runner on ROCm, where CUPTI is not available, missed in #32012\n\n\ud83d\ude80 Kind of Contribution\n\ud83d\udc1b Bug Fix\n\n@xla-rotation could I get a review for this PR, please?\nCopybara import of the project:\n\n--\nfdd0217f90b552767b70447487fbfec87f32cf50 by Eetu Sj\u00f6blom <eetu.sjoblom@amd.com>:\n\nMove cupti_tracer to cuda dependencies\n\nMerging this change closes #32336\n\nFUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/32336 from ROCm:ci_rocm_fix_py_hlo_runner fdd0217f90b552767b70447487fbfec87f32cf50",
  "Requirement ID: ISSUE-101969\nTitle: PR #31472: [XLA:GPU] Adding more debug support for command_buffer_conversion_pass\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nPR #31472: [XLA:GPU] Adding more debug support for command_buffer_conversion_pass\n\nImported from GitHub PR https://github.com/openxla/xla/pull/31472\n\n\ud83d\udcdd Summary of Changes\n Adding more debug support for command_buffer_conversion_pass to help debug what thunks are not lowered to command buffer, and reasons. \n\n\n\ud83c\udfaf Justification\nAssist debug\n\n\ud83d\ude80 Kind of Contribution\n \ud83d\udcda Documentation\n\nCopybara import of the project:\n\n--\ne1c020ddfa79faafcc569edfba6038053ddd61e8 by Shawn Wang <shawnw@nvidia.com>:\n\nadd debug information for command_buffer_conversion_pass\n\n--\n94abb4a5baac343e9c6ee3650657dc6a04d9982a by Shawn Wang <shawnw@nvidia.com>:\n\nfix debug failure\n\nMerging this change closes #31472\n\nFUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/31472 from shawnwang18:shawnw/command_buffer_conversion_pass_debug 94abb4a5baac343e9c6ee3650657dc6a04d9982a",
  "Requirement ID: ISSUE-101968\nTitle: PR #32187: PTX version 9.0 is supported starting with CUDA 13.0.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nPR #32187: PTX version 9.0 is supported starting with CUDA 13.0.\n\nImported from GitHub PR https://github.com/openxla/xla/pull/32187\n\nhttps://docs.nvidia.com/cuda/parallel-thread-execution/#release-notes\n\nAlso a slight refactoring of the code.\n\nCopybara import of the project:\n\n--\nf579d2285563d6b93502f1f2a1dfc9731102eef6 by Dimitris Vardoulakis <dvardoulakis@nvidia.com>:\n\nPTX version 9.0 is supported starting with CUDA 13.0.\nhttps://docs.nvidia.com/cuda/parallel-thread-execution/#release-notes\n\n--\nf585ea4a5470b72835d6ae6b7def54866c4145e5 by Dimitris Vardoulakis <dvardoulakis@nvidia.com>:\n\nFix typo\n\nMerging this change closes #32187\n\nFUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/32187 from dimvar:ptx-9-cuda-13 f585ea4a5470b72835d6ae6b7def54866c4145e5",
  "Requirement ID: ISSUE-101967\nTitle: Assume that CompileAndLoad can change the MLIR.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAssume that CompileAndLoad can change the MLIR.\n\nTo make that more obvious, also use std::move when passing MLIR module from IFRT.",
  "Requirement ID: ISSUE-101966\nTitle: PR #28740: [XLA:GPU] Lowering dynamic update slice thunk into command buffer if it depends on loop iteration. \nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nPR #28740: [XLA:GPU] Lowering dynamic update slice thunk into command buffer if it depends on loop iteration. \n\nImported from GitHub PR https://github.com/openxla/xla/pull/28740\n\nThis is PR tries to lower DynamicSliceThunk into command buffer, even if it depends on the loop iteration. \n\nThe command buffer implementation will also use the same approach (HloEvaluator to get new allocation during runtime) as DynamicSLiceThunk to get the sliced allocations, and for each iteration, CommandBuffer will use HloEvalutor to get the new addresses, and doing graph update with the new address. \n\nThe major changes to custom.cc file is to resolve the issue that when a module has been parsed by command buffer scheduler, it rewrites the module into nested calls, which breaks the while loop analysis pattern, and module extraction pattern, so the fix is trying to introduced a cloned inline module, and perform the loop analysis and module extraction from the inlined module. \n\nCopybara import of the project:\n\n--\n2fe7c75a9fcbc9ade65f5a275aba3a2bc996ba07 by Shawn Wang <shawnw@nvidia.com>:\n\nadd debug information for command_buffer_conversion_pass\n\n--\n88183dd7dc53c2bdc80f3a664a99b50e275311b2 by Shawn Wang <shawnw@nvidia.com>:\n\nLower dynamic update slice thunk into command buffer when its offset\nvalue depends on loop iteraiton.\n\n--\n3cf46be90b3be2185f0b5106ea9eeaa45b088601 by Shawn Wang <shawnw@nvidia.com>:\n\nfix\n\n--\n45b31f69f9299a13bac24a966625190c9e90c91e by Shawn Wang <shawnw@nvidia.com>:\n\nfix\n\n--\nce3af2b9b131c9902b45d6d9934424d861656d32 by Shawn Wang <shawnw@nvidia.com>:\n\nfix\n\n--\na7fc4ab02b5d7dec6d337fcc57bbfd38a3b205ed by Shawn Wang <shawnw@nvidia.com>:\n\nfix\n\n--\n73784aa6530244559c1530b2f922cf81c6d43822 by Shawn Wang <shawnw@nvidia.com>:\n\nchange to gemm command for test\n\n--\n64b1cf454fc360bcc3255f29bd27c01799537e07 by Shawn Wang <shawnw@nvidia.com>:\n\nfix\n\n--\n0a3d7a1b6c142a3c9aa2b299d902520ed7f91515 by Shawn Wang <shawnw@nvidia.com>:\n\nclang format\n\n--\n3105ce82fa3751d73d41b0564402e108328ea147 by Shawn Wang <shawnw@nvidia.com>:\n\nfix\n\n--\n85ce21672052c4bbfd50db54248dbe1ae2494230 by Shawn Wang <shawnw@nvidia.com>:\n\nfix\n\nMerging this change closes #28740\n\nFUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/28740 from shawnwang18:shawnw/dynamic_slice_update_for_while 85ce21672052c4bbfd50db54248dbe1ae2494230",
  "Requirement ID: ISSUE-101965\nTitle: PR #31472: [XLA:GPU] Adding more debug support for command_buffer_conversion_pass\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nPR #31472: [XLA:GPU] Adding more debug support for command_buffer_conversion_pass\n\nImported from GitHub PR https://github.com/openxla/xla/pull/31472\n\n\ud83d\udcdd Summary of Changes\n Adding more debug support for command_buffer_conversion_pass to help debug what thunks are not lowered to command buffer, and reasons. \n\n\n\ud83c\udfaf Justification\nAssist debug\n\n\ud83d\ude80 Kind of Contribution\n \ud83d\udcda Documentation\n\nCopybara import of the project:\n\n--\n59135a8094b6f3ac9d45abb798eb6ca28beb65fb by Shawn Wang <shawnw@nvidia.com>:\n\nadd debug information for command_buffer_conversion_pass\n\n--\n18c639fb6194e2bf430146957cb8589e11347a7d by Shawn Wang <shawnw@nvidia.com>:\n\nfix\n\n--\n3f3aa3c0aa1dbcba75076ae9eac79e80c491aaa4 by Shawn Wang <shawnw@nvidia.com>:\n\nfix\n\n--\nb50e41863d6b8589288ebe137e28834c34b7382c by Shawn Wang <shawnw@nvidia.com>:\n\nclang format\n\nMerging this change closes #31472\n\nFUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/31472 from shawnwang18:shawnw/command_buffer_conversion_pass_debug b50e41863d6b8589288ebe137e28834c34b7382c",
  "Requirement ID: ISSUE-101964\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-101962\nTitle: Adds an option for Hlo Module's CreateFromProto to not preserve instruction unique ids and reassigned them in a compacted way. Options is turned off for now but should be the new default moving forward. Deprecates RemapInstructionIds.\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAdds an option for Hlo Module's CreateFromProto to not preserve instruction unique ids and reassigned them in a compacted way. Options is turned off for now but should be the new default moving forward. Deprecates RemapInstructionIds.",
  "Requirement ID: ISSUE-101961\nTitle: [XLA:GPU] Add SdcThunk\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[XLA:GPU] Add SdcThunk\n\nA Thunk that calculates checksums for all configured buffers and stores them in\nan SdcLog.",
  "Requirement ID: ISSUE-101960\nTitle: Error on load tensorflow\nState: closed\nAuthor: josecabrita\nLabels: stat:awaiting response, type:bug, type:build/install, subtype:windows, TF 2.19\nBody:\n### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.20\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nwindows 11\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.13.7\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nFailed to load the native TensorFlow runtime. Not load with this python and tensorflow version. I need your help.\n\n### Standalone code to reproduce the issue\n\n```shell\nImportError: Traceback (most recent call last):\n  File \"C:\\Users\\PC\\anaconda3\\envs\\spyder-env\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 73, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: Falha numa rotina de inicializa\u00e7\u00e3o de DLL (dynamic-link library).\n```\n\n### Relevant log output\n\n```shell\n\n```",
  "Requirement ID: ISSUE-101959\nTitle: Add proto (de)serialisation for `GpuConvDescriptor`\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAdd proto (de)serialisation for `GpuConvDescriptor`\n\nWe need `GpuConvDescriptor` to be serializable to be able to add (de)serialisation for the `ConvolutionThunk`",
  "Requirement ID: ISSUE-101958\nTitle: [XLA:GPU] Fix channel_ids and use_global_device_ids in RaggedAllToAllMultiHostDecomposer.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[XLA:GPU] Fix channel_ids and use_global_device_ids in RaggedAllToAllMultiHostDecomposer.\n\nThe decomposer now uses `NextChannelId` to ensure unique channel IDs for the generated collectives. Additionally, set `use_global_device_ids=true` for `all-gather` to correctly work in cross-partition case.",
  "Requirement ID: ISSUE-101957\nTitle: Add proto (de)serialisation for `CudnnConvKind`\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAdd proto (de)serialisation for `CudnnConvKind`\n\nFor the `ConvolutionThunk` (de)serialisation we need to make the `GpuConvDescriptor` serializable, and for that we need `CudnnConvKind` too. \n\nI believe handling a `CudnnConvKindProto` that's no longer supported is \"user error\", e.g. a client is using really old thunks that new binary doesn't support. So returning an argument error in that case.\n\nOn the contrary we should always add (or at least map) new `CudnnConvKind` to the proto version, so adding no default case there to enforce that at compile time.",
  "Requirement ID: ISSUE-101956\nTitle: [XLA:GPU] Add SdcLog::ReadProto\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[XLA:GPU] Add SdcLog::ReadProto\n\nA helper that does `SdcLog::ReadFromDevice` and returns the result as\n`SdcLogProto`. The proto will be dumped to log directory for debugging.",
  "Requirement ID: ISSUE-101955\nTitle: [XLA:GPU]: Add support for loading HLO directly from profiler to graphviz\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[XLA:GPU]: Add support for loading HLO directly from profiler to graphviz\n\nNo change to the OSS version for this tool.",
  "Requirement ID: ISSUE-101954\nTitle: Support building XLA with Bzlmod\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nSupport building XLA with Bzlmod\n\n- Added MODULE.bazel and module extensions to introduce external dependencies with Bzlmod.\n- Added a CI config for Linux CPU build with Bzlmod (enabled by `--config=bzlmod`)\n\nTODOs:\n- Support overriding Python runtime\n- Support build with more configs and platforms",
  "Requirement ID: ISSUE-101953\nTitle: PR #32226: Relax the error spec to make BitcastReduceWithStride1Tiling pass on Spark.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nPR #32226: Relax the error spec to make BitcastReduceWithStride1Tiling pass on Spark.\n\nImported from GitHub PR https://github.com/openxla/xla/pull/32226\n\n\nCopybara import of the project:\n\n--\ndf7ce3a07364a1be546e206961b83b8f783894a6 by Dimitris Vardoulakis <dvardoulakis@nvidia.com>:\n\nRelax the error spec to make BitcastReduceWithStride1Tiling pass on Spark.\n\nMerging this change closes #32226\n\nFUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/32226 from dimvar:fusion-emitter-device-test-spark df7ce3a07364a1be546e206961b83b8f783894a6",
  "Requirement ID: ISSUE-101952\nTitle: PR #28740: [XLA:GPU] Lowering dynamic update slice thunk into command buffer if it depends on loop iteration. \nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nPR #28740: [XLA:GPU] Lowering dynamic update slice thunk into command buffer if it depends on loop iteration. \n\nImported from GitHub PR https://github.com/openxla/xla/pull/28740\n\nThis is PR tries to lower DynamicSliceThunk into command buffer, even if it depends on the loop iteration. \n\nThe command buffer implementation will also use the same approach (HloEvaluator to get new allocation during runtime) as DynamicSLiceThunk to get the sliced allocations, and for each iteration, CommandBuffer will use HloEvalutor to get the new addresses, and doing graph update with the new address. \n\nThe major changes to custom.cc file is to resolve the issue that when a module has been parsed by command buffer scheduler, it rewrites the module into nested calls, which breaks the while loop analysis pattern, and module extraction pattern, so the fix is trying to introduced a cloned inline module, and perform the loop analysis and module extraction from the inlined module. \n\nCopybara import of the project:\n\n--\n2fe7c75a9fcbc9ade65f5a275aba3a2bc996ba07 by Shawn Wang <shawnw@nvidia.com>:\n\nadd debug information for command_buffer_conversion_pass\n\n--\n88183dd7dc53c2bdc80f3a664a99b50e275311b2 by Shawn Wang <shawnw@nvidia.com>:\n\nLower dynamic update slice thunk into command buffer when its offset\nvalue depends on loop iteraiton.\n\n--\n3cf46be90b3be2185f0b5106ea9eeaa45b088601 by Shawn Wang <shawnw@nvidia.com>:\n\nfix\n\n--\n45b31f69f9299a13bac24a966625190c9e90c91e by Shawn Wang <shawnw@nvidia.com>:\n\nfix\n\n--\nce3af2b9b131c9902b45d6d9934424d861656d32 by Shawn Wang <shawnw@nvidia.com>:\n\nfix\n\n--\na7fc4ab02b5d7dec6d337fcc57bbfd38a3b205ed by Shawn Wang <shawnw@nvidia.com>:\n\nfix\n\n--\n73784aa6530244559c1530b2f922cf81c6d43822 by Shawn Wang <shawnw@nvidia.com>:\n\nchange to gemm command for test\n\n--\n64b1cf454fc360bcc3255f29bd27c01799537e07 by Shawn Wang <shawnw@nvidia.com>:\n\nfix\n\n--\n0a3d7a1b6c142a3c9aa2b299d902520ed7f91515 by Shawn Wang <shawnw@nvidia.com>:\n\nclang format\n\n--\n3105ce82fa3751d73d41b0564402e108328ea147 by Shawn Wang <shawnw@nvidia.com>:\n\nfix\n\n--\n85ce21672052c4bbfd50db54248dbe1ae2494230 by Shawn Wang <shawnw@nvidia.com>:\n\nfix\n\nMerging this change closes #28740\n\nFUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/28740 from shawnwang18:shawnw/dynamic_slice_update_for_while 85ce21672052c4bbfd50db54248dbe1ae2494230",
  "Requirement ID: ISSUE-101951\nTitle: Fix typo in comment.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nFix typo in comment.\n\nFUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/32459 from ROCm:ci_fix_hermetic_build_add_missing_lib e1766eeee9c88d3408a4de81f98b7454f1d593f5",
  "Requirement ID: ISSUE-101950\nTitle: PR #31300: [XLA:CPU][BugFix] Fix ODR Violation by Renaming Duplicate IsSupportedType Definition\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nPR #31300: [XLA:CPU][BugFix] Fix ODR Violation by Renaming Duplicate IsSupportedType Definition\n\nImported from GitHub PR https://github.com/openxla/xla/pull/31300\n\nTwo different definitions of `xla::cpu::IsSupportedType(...)` in `onednn_util.h` and `convolution_lib.cc` cause ODR (One Definition Rule) violation. This occurs when modifying the `IsSupportedType(...)` function in `onednn_util.h` (e.g., adding logs), which causes the compiler not to inline the function and results in undefined behavior.\n\nThis PR fixes the issue by renaming the `IsSupportedType(...)` function in `convolution_lib.cc` (as changing the name in `onednn_util.h` will need changes in its multiple invocations).\nCopybara import of the project:\n\n--\nc9b2bcde52ac7f0decfd652d0627467f8b31308f by Om Thakkar <om.thakkar@intel.com>:\n\nrename IsSupportedType in convolution_lib.cc\n\n--\n2e2a5159f92d1a474ffb9d5b64ec91af34b0db0d by Om Thakkar <om.thakkar@intel.com>:\n\nmake the IsConvSupportedType inline\n\nMerging this change closes #31300\n\nFUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/31300 from Intel-tensorflow:othakkar/bugfix_conv 2e2a5159f92d1a474ffb9d5b64ec91af34b0db0d",
  "Requirement ID: ISSUE-101949\nTitle: Add visibility rules only for internal users\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAdd visibility rules only for internal users",
  "Requirement ID: ISSUE-101948\nTitle: [XLA:GPU] Remove ConditionalCanonicalizer pass\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[XLA:GPU] Remove ConditionalCanonicalizer pass",
  "Requirement ID: ISSUE-101947\nTitle: [Autotuner] Enable to modify xla_gpu_experimental_autotuner_cache_dir DebugOption via a flag.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[Autotuner] Enable to modify xla_gpu_experimental_autotuner_cache_dir DebugOption via a flag.",
  "Requirement ID: ISSUE-101946\nTitle: PR #32460: [ROCm] Add cuda-only tag to cuda_library\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nPR #32460: [ROCm] Add cuda-only tag to cuda_library\n\nImported from GitHub PR https://github.com/openxla/xla/pull/32460\n\n\ud83d\udcdd Summary of Changes\nFix building xla with rocm.\n\n\ud83c\udfaf Justification\nAdd cuda-only tag to cuda_library targets,\nthat will prevent building them when build with rocm config.\n\n\ud83d\ude80 Kind of Contribution\nPlease remove what does not apply: \ud83d\udc1b Bug Fix\n\n\ud83d\udcca Benchmark (for Performance Improvements)\nNot relevant\n\n\ud83e\uddea Unit Tests:\nCI, build fix\n\n\ud83e\uddea Execution Tests:\nNot relevant\n\nCopybara import of the project:\n\n--\n2698efeb4f8fec85396bad712c86ba59d2a6b1e6 by Alexandros Theodoridis <atheodor@amd.com>:\n\nAdd cuda-only tag to cuda_library\n\nMerging this change closes #32460\n\nFUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/32460 from ROCm:ci_fix_cuda_library_macro_missing_cuda-only_tag 2698efeb4f8fec85396bad712c86ba59d2a6b1e6",
  "Requirement ID: ISSUE-101945\nTitle: Move `GpuConvConfig` creation to the `ConvolutionThunk` constructor\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nMove `GpuConvConfig` creation to the `ConvolutionThunk` constructor\n\nTo (de)serialize this thunk, we'll be using the `GpuConvDescriptor` instead of the `GpuConvConfig`, since its easier to serialize (most of the config fields actually get populated during execution).\n\nSo we move the creation to the Thunk, so that in the next CL we can also store the descriptor to use for (de)serialisation. I didn't add the `GpuConvDescriptor descriptor_` field in this CL, since its technically not needed yet.",
  "Requirement ID: ISSUE-101944\nTitle: Fix includes in convolution_thunk and gpu_conv_runner\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nFix includes in convolution_thunk and gpu_conv_runner\n\nCleaning includes up before touching the files.",
  "Requirement ID: ISSUE-101943\nTitle: PR #32459: [ROCm] Add missing rocradn dependency fix ci build\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nPR #32459: [ROCm] Add missing rocradn dependency fix ci build\n\nImported from GitHub PR https://github.com/openxla/xla/pull/32459\n\n\ud83d\udcdd Summary of Changes\nFix rocm hermetic ci build\n\n\ud83c\udfaf Justification\nAdding missing rocrand dependency to fix heremetic build for rocm\n\n\ud83d\ude80 Kind of Contribution\nPlease remove what does not apply: \ud83d\udc1b Bug Fix\n\n\ud83d\udcca Benchmark (for Performance Improvements)\nNot relevant\n\n\ud83e\uddea Unit Tests:\nNot relevant, ci tests\n\n\ud83e\uddea Execution Tests:\nNot relevant\n\nCopybara import of the project:\n\n--\ne1766eeee9c88d3408a4de81f98b7454f1d593f5 by Alexandros Theodoridis <atheodor@amd.com>:\n\nAdd missing rocradn dependency\n\nMerging this change closes #32459\n\nFUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/32459 from ROCm:ci_fix_hermetic_build_add_missing_lib e1766eeee9c88d3408a4de81f98b7454f1d593f5",
  "Requirement ID: ISSUE-101942\nTitle: [IFRT] Allow IFRT Proxy to use persistence API\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[IFRT] Allow IFRT Proxy to use persistence API\n\nThis changes allows IFRT Proxy client to use persistence API by sending a sidecar via plugin program.",
  "Requirement ID: ISSUE-101940\nTitle: [IFRT] Introduce `MpmdLoadedExecutable` for MPMD-specific interfaces.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[IFRT] Introduce `MpmdLoadedExecutable` for MPMD-specific interfaces.\n\nThis change adds a new `MpmdLoadedExecutable` abstract class, inheriting from `LoadedExecutable`, to provide interfaces specific to Multi-Program Multi-Data (MPMD) execution.",
  "Requirement ID: ISSUE-101939\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-101938\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-101937\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-101936\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-101935\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-101934\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-101933\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-101932\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change\n\nReverts f40ce051cb8a94108ec5aded4f52d4b5a3b1ac3c",
  "Requirement ID: ISSUE-101931\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change\n\nReverts f40ce051cb8a94108ec5aded4f52d4b5a3b1ac3c",
  "Requirement ID: ISSUE-101930\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-101929\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-101928\nTitle: Integrate LLVM at llvm/llvm-project@891f002026df\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nIntegrate LLVM at llvm/llvm-project@891f002026df\n\nUpdates LLVM usage to match\n[891f002026df](https://github.com/llvm/llvm-project/commit/891f002026df)",
  "Requirement ID: ISSUE-101927\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-101926\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-101925\nTitle: Automated Code Change\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAutomated Code Change",
  "Requirement ID: ISSUE-101924\nTitle: Add a lazily populated cache of stream synchronization events.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAdd a lazily populated cache of stream synchronization events.\n\nThis will allow for cheap stream synchronized buffer definition events.",
  "Requirement ID: ISSUE-101923\nTitle: [XLA][Numerics][HLO Value Tracking] Create an original value for compiler-inserted tuples during fusion\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[XLA][Numerics][HLO Value Tracking] Create an original value for compiler-inserted tuples during fusion\n\nThis also removes the code that handles original values in MergeFusionInstructionIntoMultiOutput, as it eventually calls into HloCallableInstruction::CloneAndAppendInstructionIntoCalledComputation to create a tuple result and the corresponding original value.",
  "Requirement ID: ISSUE-101922\nTitle: [tsl:concurrency] Optimize Future::Map for ready futures\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[tsl:concurrency] Optimize Future::Map for ready futures\n\n```\nname                        cpu/op        cpu/op      vs base\nBM_CreateOkFuture           1.948n \u00b1 0%   1.627n \u00b1 0%  -16.50% (n=80)\nBM_CopyFuture               1.946n \u00b1 0%   1.625n \u00b1 0%  -16.52% (n=80)\nBM_MapStatelessFuture       37.16n \u00b1 0%   15.90n \u00b1 0%  -57.21% (n=80)\nBM_TryMapStatelessFuture    36.03n \u00b1 1%   15.88n \u00b1 0%  -55.93% (n=80)\nBM_MapToFromStatelessFuture 38.04n \u00b1 0%   15.91n \u00b1 0%  -58.19% (n=80)\nBM_MapStatefulFuture        38.43n \u00b1 0%   16.45n \u00b1 0%  -57.18% (n=80)\nBM_TryMapStatefulFuture     37.50n \u00b1 0%   16.45n \u00b1 0%  -56.13% (n=80)\ngeomean                     16.08n        8.368n       -47.97%\n```",
  "Requirement ID: ISSUE-101921\nTitle: Refactor the H2D transfer for user input in TFRT/IFRT.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nRefactor the H2D transfer for user input in TFRT/IFRT.",
  "Requirement ID: ISSUE-101920\nTitle: [PjRT-IFRT] Add caching to `xla::ifrt::PjRtClient::GetDefaultPjRtLayout()`\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[PjRT-IFRT] Add caching to `xla::ifrt::PjRtClient::GetDefaultPjRtLayout()`\n\n`xla::ifrt::Client::GetDefaultPjRtLayout()` may be called more frequently once `xla::ifrt::Array::pjrt_layout()` returns `nullptr` to indicate a default layout. To prepare for it, this change adds caching to the method.",
  "Requirement ID: ISSUE-101919\nTitle: Pass a Future<std::string> through PjRtCApiBuffer::CopyToRemoteDevice instead of awaiting the string.\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nPass a Future<std::string> through PjRtCApiBuffer::CopyToRemoteDevice instead of awaiting the string.",
  "Requirement ID: ISSUE-101918\nTitle: PR #32430: [GPU] Use intrinsics to accelerate f4e2m1fn conversions on Blackwell.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nPR #32430: [GPU] Use intrinsics to accelerate f4e2m1fn conversions on Blackwell.\n\nImported from GitHub PR https://github.com/openxla/xla/pull/32430\n\n\ud83d\udcdd Summary of Changes\nUse intrinsics to accelerate f4e2m1fn conversions on Blackwell.\n\n\ud83c\udfaf Justification\nAccelerates type conversions.\n\n\ud83d\ude80 Kind of Contribution\nPerformance Improvement.\n\n\ud83d\udcca Benchmark (for Performance Improvements)\nPublic HLOs in `compiler/xla/tools/benchmarks/hlo/` don't use f4e2m1fn.\nOn a trivial microbenchmark (bf16->f4e2m1fn conversion of a large buffer) leads to ~2x speedup.\n\n\ud83e\uddea Unit Tests: Yes.\n\n\ud83e\uddea Execution Tests: Yes.\nCopybara import of the project:\n\n--\nd40e86ccb4568f6b515543e9c3bfe2425e48d964 by Ilia Sergachev <isergachev@nvidia.com>:\n\n[GPU] Use intrinsics to accelerate f4e2m1fn conversions on Blackwell.\n\n--\na0e9e4ddfc2ddee6e57cffffca4c5278279e1c76 by Ilia Sergachev <isergachev@nvidia.com>:\n\nAddress review comment.\n\n--\nc8bb0a81efda7b7ffb41542d4a8b112796eb218a by Ilia Sergachev <isergachev@nvidia.com>:\n\nAddress review comment.\n\nMerging this change closes #32430\n\nFUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/32430 from openxla:fast_f4_convert c8bb0a81efda7b7ffb41542d4a8b112796eb218a",
  "Requirement ID: ISSUE-101917\nTitle: Add `PJRT_Promise` and related functions so that valid `xla::Future`s can be passed into C API client methods.\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAdd `PJRT_Promise` and related functions so that valid `xla::Future`s can be passed into C API client methods.",
  "Requirement ID: ISSUE-101916\nTitle: Add missing function handler to nvml impl lib and use it\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAdd missing function handler to nvml impl lib and use it",
  "Requirement ID: ISSUE-101915\nTitle: Refactor CMV1 in spmd/dot_handler.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nRefactor CMV1 in spmd/dot_handler.",
  "Requirement ID: ISSUE-101914\nTitle: Reverts f40ce051cb8a94108ec5aded4f52d4b5a3b1ac3c\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nReverts f40ce051cb8a94108ec5aded4f52d4b5a3b1ac3c",
  "Requirement ID: ISSUE-101913\nTitle: Added `no_windows` tags to //tensorflow/core/kernels/batching_util:shared_batch_scheduler_test\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAdded `no_windows` tags to //tensorflow/core/kernels/batching_util:shared_batch_scheduler_test",
  "Requirement ID: ISSUE-101912\nTitle: Add testDifferentDatasetIdsForSameJob test.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAdd testDifferentDatasetIdsForSameJob test.\n\nThis verifies behavior when creating iterators with the same job_name\nbut different dataset IDs.",
  "Requirement ID: ISSUE-101911\nTitle: fix and reenable analytical_latency_estimator_test\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nfix and reenable analytical_latency_estimator_test",
  "Requirement ID: ISSUE-101910\nTitle: [JAX] Add `jax_check_ifrt_user_context` config and enable it for JAX tests\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[JAX] Add `jax_check_ifrt_user_context` config and enable it for JAX tests\n\nWhen JAX config `jax_check_ifrt_user_context` is true, JAX will require some `xla::ifrt::UserContext` to be set up for IFRT values and executables when wrapping them as JAX objects.\n\nThis replaces `-DIFRT_REQUIRE_USER_CONTEXT` for checking if IFRT user\ncontext setups is done correctly.\n\nLimitation: This check does not cover the paths that do not wrap any IFRT\nvalues and executables, notably blocking on an array and fetching an\narray to host.",
  "Requirement ID: ISSUE-101909\nTitle: Fix protobuf.patch label in tensorflow repository.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nFix protobuf.patch label in tensorflow repository.",
  "Requirement ID: ISSUE-101908\nTitle: Fix usage of hermetic tar and xz tools on aarch64 machines.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nFix usage of hermetic tar and xz tools on aarch64 machines.\n\nThe previous tool packages were built on the machine with GLIBC 2.34, which caused errors on the runners with GLIBC 2.31.",
  "Requirement ID: ISSUE-101907\nTitle: [XLA] Restrict `HloPassPipeline::RunOnModuleGroup` to single-module groups.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[XLA] Restrict `HloPassPipeline::RunOnModuleGroup` to single-module groups.\n\nThis is in preparation to retiring RunOnModuleGroup completely.",
  "Requirement ID: ISSUE-101906\nTitle: [XLA] Clean up AnnotateHostComputeOffloadTest.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[XLA] Clean up AnnotateHostComputeOffloadTest.\n\nAnnotateHostComputeOffload is no longer a module-group pass, so the test doesn't need to use module groups.",
  "Requirement ID: ISSUE-101905\nTitle: Fix typos: separate, occurred, the, and improve issue template\nState: open\nAuthor: sivamurthy30\nLabels: awaiting review, comp:xla, size:S\nBody:\n- Fix 'Seperate' \u2192 'Separate' in mhlo_extensions_test.mlir\r\n- Fix 'occured' \u2192 'occurred' in gemm_fusion_autotuner.cc\r\n- Fix 'teh' \u2192 'the' in gpu_p2p_pipeliner_test.cc\r\n- Fix 'SEPERATE' \u2192 'SEPARATE' in polymorphic_function attributes\r\n- Improve GitHub issue template formatting and structure",
  "Requirement ID: ISSUE-101904\nTitle: [xla:gpu] Fix our convert integer to pred in our Triton emitter\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[xla:gpu] Fix our convert integer to pred in our Triton emitter\n\n`arith.trunci` for i1 will simply take the last bit, but HLO expects convert to i1 to be value != 0. Emit this conversion a a compare not equal to 0 instead. This is already done correctly for floats.",
  "Requirement ID: ISSUE-101903\nTitle: Encapsulate XLA compilation error payload handling.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nEncapsulate XLA compilation error payload handling.\n\nFUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/32389 from Tixxx:tixxx/pipeliner_opt_barrier 4273b194da9ca66b4ebcaa959bcc8119d8c3eff7",
  "Requirement ID: ISSUE-101902\nTitle: Add a getter method to access the custom call handler from an HloEvaluator.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nAdd a getter method to access the custom call handler from an HloEvaluator.",
  "Requirement ID: ISSUE-101901\nTitle: [TFRT]Update batch size rounding to consider batch priority.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[TFRT]Update batch size rounding to consider batch priority.\n\n- The two places where call the `RoundToLowestAllowedBatchSize` function now include a call to `IsLowPriorityBatch`, ensuring the processed batch size is consistent with the batch size generated in ConcatInputTensors and SplitOutputTensors.",
  "Requirement ID: ISSUE-101900\nTitle: Use `ABSL_ATTRIBUTE_LIFETIME_BOUND` on `GetTensorData` to detect potential dangling pointers.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\nUse `ABSL_ATTRIBUTE_LIFETIME_BOUND` on `GetTensorData` to detect potential dangling pointers.",
  "Requirement ID: ISSUE-101899\nTitle: [TFRT]Fix scope of batcher queue options initialization.\nState: open\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[TFRT]Fix scope of batcher queue options initialization.\n\nThe logic for setting `max_execution_batch_size` and `allowed_batch_sizes` was incorrectly nested within the scope of the `split_input_task_func` lambda definition. This change moves the `if/else` block to the correct scope within `GetBatcherQueueOptions`.",
  "Requirement ID: ISSUE-101898\nTitle: [XLA] Solidify the invariants of `TiledHloSchedule`.\nState: closed\nAuthor: copybara-service[bot]\nLabels: \nBody:\n[XLA] Solidify the invariants of `TiledHloSchedule`.\n\nIn order to support arbitrary schedules with a predictable number of\nparameters, we produce a map with a single parameter representing the\nentire iteration space---circumventing otherwise limiting divisibility\nconstraints (e.g. given two axis of prime length, there are only two\nvalid schedules that can be expressed with 2 parameters, but many more 1D\nschedules).\n\nFollow-up changes will focus on implementing other schedules and integrating\nit as a parameter to `SymbolicTileAnalysis::ComputeTiledHloInstructions`."
]