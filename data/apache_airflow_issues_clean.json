[
  "Requirement ID: ISSUE-56699\nTitle: [v3-1-test] Fix custom timetable generate_run_id not called for manual triggers (#56373)\nState: open\nAuthor: github-actions[bot]\nLabels: area:API\nBody:\n(cherry picked from commit c28b21178b9c8c299a8ec5c74eff39d7e1da99b9)\n\nCo-authored-by: Nils Werner <nils@eochgroup.com>",
  "Requirement ID: ISSUE-56698\nTitle: Enable PT011 rule to prvoider tests\nState: open\nAuthor: xchwan\nLabels: provider:amazon, area:providers\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\nissue: Enable Even More PyDocStyle Checks #40567\r\n@ferruzzi\r\nThis PR is for enable PT011 rule:\r\nPT011: all instances of pytest.raises should include a match term to make sure they are catching the right error.\r\nhttps://docs.astral.sh/ruff/rules/pytest-raises-too-broad/\r\nThere are 102 files changes is need.\r\nSo, I separate to many PR, which contains about 5 file changes for easy review.\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56697\nTitle: Migrate CreateAssetEventsBody to Pydantic v2 ConfigDict\nState: open\nAuthor: choo121600\nLabels: area:API\nBody:\nThe `CreateAssetEventsBody` model currently uses Pydantic v1 `Config` class,\r\nwhich triggers DeprecationWarnings in Pydantic v2.\r\n\r\n```\r\n/opt/airflow/airflow-core/src/airflow/api_fastapi/core_api/datamodels/assets.py:172 PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/\r\n```\r\n\r\n<img width=\"622\" height=\"127\" alt=\"image\" src=\"https://github.com/user-attachments/assets/096f78fa-07e2-4411-ba9c-842177feb1d7\" />\r\n\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56696\nTitle: refactor: rename deprecated `HTTP_422_UNPROCESSABLE_ENTITY` to `HTTP_422_UNPROCESSABLE_CONTENT`\nState: closed\nAuthor: choo121600\nLabels: \nBody:\nThis PR replaces the deprecated constant `HTTP_422_UNPROCESSABLE_ENTITY` with `HTTP_422_UNPROCESSABLE_CONTENT`\r\n\r\n\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56695\nTitle: Fix memory leak in remote logging connection cache\nState: open\nAuthor: kaxil\nLabels: area:task-sdk\nBody:\nThe remote logging connection cache was using `@lru_cache` with the API client instance as a parameter. This caused client references to be retained in the cache indefinitely, preventing garbage collection and causing memory leaks when tasks created multiple client instances.\r\n\r\nThe new implementation ensures connection details are cached for performance while allowing client instances to be properly garbage collected after use.\r\n\r\nPart of https://github.com/apache/airflow/issues/56641\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56694\nTitle: Optimize fail-fast check to avoid loading ``SerializedDAG``\nState: open\nAuthor: kaxil\nLabels: area:serialization, area:API, kind:documentation, area:DAG-processing, area:db-migrations\nBody:\nWhen a task fails and ``fail_fast`` is enabled, the API-server needs to stop remaining tasks. Previously, this required loading the entire 5-50 MB SerializedDAG for every task failure (although it comes from cache -- but it is likely that if multiple replicas are run -- it might not have it in local cache) to check the ``fail_fast`` setting.\r\n\r\nThis change adds `fail_fast` column to the dag table and checks it with a simple database lookup first. The `SerializedDAG` is only loaded when `fail_fast=True` (affecting ~1% of DAGs), avoiding unnecessary memory and I/O overhead in 99% of cases.\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56693\nTitle: KubernetesJobOperator does not recover when pods are deleted on completion\nState: open\nAuthor: pmcquighan-camus\nLabels: kind:bug, area:providers, provider:cncf-kubernetes, needs-triage\nBody:\n### Apache Airflow Provider(s)\n\ncncf-kubernetes\n\n### Versions of Apache Airflow Providers\n\napache-airflow-providers-cncf-kubernetes==10.7.0\n\n### Apache Airflow version\n\n3.0.6\n\n### Operating System\n\ndebian 12\n\n### Deployment\n\nOfficial Apache Airflow Helm Chart\n\n### Deployment details\n\nRunning on GKE , kubernetes version 1.33\n\n### What happened\n\nA job with parallelism 1 and 1 completion (i.e. just running a single pod to completion) completed successfully.  The triggerer detected the job completion, but before the task was restarted GKE deleted the pod for a node scaling event.  Since the pod is `Complete` the Job is also considered `Complete` and so kubernetes will not retry the pod or anything.  Then, when the task wakes up, it fails when trying to `resume_execution`, notably when trying to fetch logs.  The worst part is that on *task retries* the operator sees \"job is completed\" and tries to resume from `execute_complete` and hits the same pod not found error again (instead of perhaps retrying the Job from the start).\n\n```\n[2025-10-15, 09:48:22] ERROR - Task failed with exception: source=\"task\"\nApiException: (404)\nReason: Not Found\n\nFile \"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py\", line 920 in run\nFile \"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py\", line 1215 in _execute_task\nFile \"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/bases/operator.py\", line 1606 in resume_execution\nFile \"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/cncf/kubernetes/operators/job.py\", line 276 in execute_complete\nFile \"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/cncf/kubernetes/hooks/kubernetes.py\", line 470 in get_pod\nFile \"/home/airflow/.local/lib/python3.12/site-packages/kubernetes/client/api/core_v1_api.py\", line 23999 in read_namespaced_pod\nFile \"/home/airflow/.local/lib/python3.12/site-packages/kubernetes/client/api/core_v1_api.py\", line 24086 in read_namespaced_pod_with_http_info\nFile \"/home/airflow/.local/lib/python3.12/site-packages/kubernetes/client/api_client.py\", line 348 in call_api\nFile \"/home/airflow/.local/lib/python3.12/site-packages/kubernetes/client/api_client.py\", line 180 in __call_api\nFile \"/home/airflow/.local/lib/python3.12/site-packages/kubernetes/client/api_client.py\", line 373 in request\nFile \"/home/airflow/.local/lib/python3.12/site-packages/kubernetes/client/rest.py\", line 244 in GET\nFile \"/home/airflow/.local/lib/python3.12/site-packages/kubernetes/client/rest.py\", line 238 in request\n```\n\nI think a primary workaround is to set `get_logs=False`, but I'm not totally certain that this workaround fixes all cases where a PodNotFound might occur.\n\nAlso note that the None-check on getting the pod [here](https://github.com/apache/airflow/blob/providers-cncf-kubernetes/10.7.0/providers/cncf/kubernetes/src/airflow/providers/cncf/kubernetes/operators/job.py#L276-L278) is not hit since the method `get_pod` ends up throwing a kubernetes.client.ApiException.  I tried patching the code to catch that exception and rethrow the `PodNotFoundException`, but that had no effect.\n\nThis feels similar to, but not fixed by https://github.com/apache/airflow/issues/39239, notably a task retry does not result in a successful execution.\n\n### What you think should happen instead\n\nI think failing with PodNotFoundException for the task when `get_logs=True` is reasonable, however it seems like a task retry should then result in the full task being retried instead of just re-running `execute_complete` and failing on the same exception multiple times.  This behavior seemed to occur regardless of if the kubernetes Job object still remained either.\n\n### How to reproduce\n\nRun a KubernetesJobOperator that does anything, and once the pod completes (but prior to airflow fetching logs/marking the task complete), manually delete the pod.  In an actual cloud-hosted Kubernetes environment, a cluster-autoscaling component might result in the pod being deleted, but it is hard to rely on that so a manual delete mimics the same behavior. \n\n### Anything else\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56692\nTitle: Prevent unnecessary kubernetes client imports in workers\nState: open\nAuthor: kaxil\nLabels: area:serialization\nBody:\nWorkers no longer import the full kubernetes client library (~32-42 MB) when performing routine operations like secret masking and DAG serialization. The kubernetes client is only imported when actually processing kubernetes objects.\r\n\r\nWith the default 32 LocalExecutor workers, this could reduce memory usage by approximately 1 GB in deployments that don't all use k8s.\r\n\r\nPart of #56641 (Kudos to @wjddn279 for investigation)\r\n\r\n```py\r\nimport sys\r\nimport tracemalloc\r\n\r\nassert 'kubernetes' not in sys.modules\r\n\r\ntracemalloc.start()\r\nsnapshot_before = tracemalloc.take_snapshot()\r\n\r\nfrom kubernetes.client import V1EnvVar\r\n\r\nsnapshot_after = tracemalloc.take_snapshot()\r\n\r\ntop_stats = snapshot_after.compare_to(snapshot_before, 'traceback')\r\nprint(\"[ Top 10 differences ]\")\r\nfor stat in top_stats[:10]:\r\n    print(stat)\r\n\r\ntotal = sum(stat.size_diff for stat in top_stats)\r\nprint(f\"\\nTotal memory increase: {total / 1024 / 1024:.2f} MB\")\r\n```\r\nOutput: Total memory increase: 41.62 MB\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56691\nTitle: Asset-Aware scheduling has a weird behavior: a child DAG references a single Asset object, but they get grouped\nState: open\nAuthor: Ferdinanddb\nLabels: kind:bug, area:Scheduler, area:core, needs-triage\nBody:\n### Apache Airflow version\n\n3.1.0\n\n### If \"Other Airflow 2/3 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\nI have the following DAG that emits Assets on a daily basis:\n```python\nimport os\nfrom datetime import datetime, timedelta\nfrom pathlib import Path\n\nfrom airflow.providers.google.cloud.operators.gcs import GCSListObjectsOperator\nfrom airflow.providers.google.cloud.transfers.gcs_to_gcs import GCSToGCSOperator\nfrom airflow.providers.google.cloud.transfers.sftp_to_gcs import SFTPToGCSOperator\nfrom airflow.providers.sftp.sensors.sftp import SFTPSensor\nfrom airflow.providers.standard.operators.python import PythonOperator, ShortCircuitOperator\nfrom airflow.sdk import DAG, Asset, AssetAlias, task\nfrom airflow_richfox.custom.utils.check_date import should_run_mon_to_fri\n\nPROJECT_ID = os.environ.get(\"PROJECT_ID\")\nGCS_LANDING_ZONE_BUCKET = f\"{PROJECT_ID}-landing-zone-euw4\"\nGCS_STAGING_BUCKET = f\"{PROJECT_ID}-staging-euw4\"\nGCS_ARCHIVE_BUCKET = f\"{PROJECT_ID}-archive-euw4\"\n\nSFTP_FILE_NAME = \"IVYDB.{{ data_interval_start | ds_nodash }}D.zip\"\n\nBLOB_DESTINATION_PATH: str = Path(\n    \"{{ dag.dag_id }}\", \"{{ data_interval_start | ds_nodash }}\", SFTP_FILE_NAME\n).as_posix()\n\nSFTP_FILE_PATH = Path(\"/IvyDBUS/v6.0/Update/\", SFTP_FILE_NAME).as_posix()\n\nGCP_CONN_ID = \"google_cloud_default\"\n\n\ndefault_args = {\n    \"owner\": \"airflow\",\n    \"depends_on_past\": False,\n    \"email_on_failure\": False,\n    \"email_on_retry\": False,\n    \"retries\": 1,\n    \"start_date\": datetime(2024, 9, 7),\n    \"retry_delay\": timedelta(minutes=1),\n}\n\n\n# Update your DAG to use parallel processing\nwith DAG(\n    dag_id=Path(__file__).parent.name,\n    doc_md=__doc__,\n    schedule=\"00 7 * * *\",\n    default_args=default_args,\n    max_active_runs=1,\n    catchup=False,\n    on_failure_callback=default_dag_failure_slack_webhook_notification,\n) as dag:\n    list_files_in_staging = GCSListObjectsOperator(\n        task_id=\"list_files_in_staging\",\n        bucket=GCS_STAGING_BUCKET,\n        prefix=\"{{ dag.dag_id }}/{{ data_interval_start | ds_nodash }}/\",\n        gcp_conn_id=GCP_CONN_ID,\n    )\n\n    @task(outlets=[AssetAlias(\"opentmetrics-update-iceberg-bronze-tables-dags-to-trigger\")])\n    def emit_airflow_assets(file_paths: list[str], outlet_events, **context):  # noqa: ANN001, ANN201\n        \"\"\"Emit Assets for all GCS files.\"\"\"\n        for file_path in file_paths:\n            optionmetrics_table_name = Path(file_path).name.split(\".\")[0].lower()\n            asset_name = f\"staging-file.optionmetrics_{optionmetrics_table_name}_update\"\n            outlet_events[\n                AssetAlias(\"opentmetrics-update-iceberg-bronze-tables-dags-to-trigger\")\n            ].add(\n                Asset(asset_name),\n                extra={\n                    \"data_interval_start\": context[\"data_interval_start\"],\n                    \"data_interval_end\": context[\"data_interval_end\"],\n                    \"ds\": context[\"ds\"],\n                    \"ds_nodash\": context[\"ds_nodash\"],\n                    \"dag_id\": context[\"dag\"].dag_id,\n                    \"gcs_staging_file_path\": f\"{file_path}\",\n                },\n            )\n\n    emit_airflow_assets_task = emit_airflow_assets(list_files_in_staging.output)\n\n    # Set task dependencies\n    list_files_in_staging >> emit_airflow_assets_task\n```\n\nEach Asset emitted by the DAG above will trigger a 'child' DAG, an example of such DAG is:\n\n```python\nimport os\nfrom datetime import datetime, timedelta\nfrom pathlib import Path\n\nfrom airflow.providers.cncf.kubernetes.operators.spark_kubernetes import SparkKubernetesOperator\nfrom airflow.providers.google.cloud.transfers.gcs_to_gcs import GCSToGCSOperator\nfrom airflow.sdk import DAG, Asset, Metadata, task\nfrom airflow_richfox.custom.utils.gcs import upload_spark_config_to_gcs\nfrom airflow_richfox.custom.utils.slack import default_dag_failure_slack_webhook_notification\n\nPROJECT_ID = os.environ.get(\"PROJECT_ID\")\nGCS_LANDING_ZONE_BUCKET = f\"{PROJECT_ID}-landing-zone-euw4\"\nGCS_STAGING_BUCKET = f\"{PROJECT_ID}-staging-euw4\"\nGCS_ARCHIVE_BUCKET = f\"{PROJECT_ID}-archive-euw4\"\n\nOPTIONMETRICS_TABLENAME = \"optionmetrics_ivyopprc_update\"\n\nICEBERG_CATALOG_NAME = \"biglakeCatalog\"\nICEBERG_TABLE_REF = f\"bronze.{OPTIONMETRICS_TABLENAME}\"\nICEBERG_FULL_TABLE_REF = f\"{ICEBERG_CATALOG_NAME}.{ICEBERG_TABLE_REF}\"\n\n# Define the Asset for this DAG\nasset_staging_file_optionmetrics_ivyopprc_update = Asset(f\"staging-file.{OPTIONMETRICS_TABLENAME}\")\nasset_table_bronze_optionmetrics_ivyopprc_update = Asset(f\"x-iceberg://{ICEBERG_FULL_TABLE_REF}\")\n\n\ndefault_args = {\n    \"owner\": \"airflow\",\n    \"depends_on_past\": False,\n    \"email_on_failure\": False,\n    \"email_on_retry\": False,\n    \"retries\": 2,\n    \"start_date\": datetime(2024, 9, 7),\n    \"retry_delay\": timedelta(minutes=1),\n    \"weight_rule\": \"upstream\",\n    \"max_active_tis_per_dag\": 2,\n}\n\n\n# Update your DAG to use parallel processing\nwith DAG(\n    dag_id=Path(__file__).parent.name,\n    dag_display_name=\"\ud83d\udfe4 \" + ICEBERG_FULL_TABLE_REF,\n    doc_md=__doc__,\n    schedule=asset_staging_file_optionmetrics_ivyopprc_update,\n    default_args=default_args,\n    max_active_runs=20,\n    tags=[\n        \"optionmetrics\",\n        \"bronze\",\n        OPTIONMETRICS_TABLENAME,\n    ],\n    params={\n        \"full_reload\": False,\n    },\n    catchup=False,\n    on_failure_callback=default_dag_failure_slack_webhook_notification,\n) as dag:\n    daily_changes_job = SparkKubernetesOperator(\n        task_id=\"daily_changes_job\",\n        namespace=\"spark-operator\",\n        application_file=\"spark_app/daily_changes_job/spark_application_config.yml\",\n        kubernetes_conn_id=\"kubernetes_default\",\n        random_name_suffix=True,\n        get_logs=True,\n        reattach_on_restart=True,\n        delete_on_termination=True,\n        do_xcom_push=False,\n        deferrable=False,\n        retries=2,\n        on_execute_callback=upload_spark_config_to_gcs,\n    )\n\n    archive_staging_file = GCSToGCSOperator(\n        task_id=\"archive_staging_file\",\n        source_bucket=GCS_STAGING_BUCKET,\n        source_object=\"{{ (triggering_asset_events.values() | first | last).extra['gcs_staging_file_path'] }}\",  # noqa: E501\n        destination_bucket=GCS_ARCHIVE_BUCKET,\n        destination_object=\"bronze.optionmetrics_ivyopprc_update/\",\n        move_object=True,\n        replace=True,\n        gcp_conn_id=\"google_cloud_default\",\n    )\n\n    @task(outlets=[asset_table_bronze_optionmetrics_ivyopprc_update])\n    def emit_assets_if_none_failed(**context):  # noqa: ANN201\n        \"\"\"Emit assets if no tasks have failed.\"\"\"\n        yield Metadata(\n            asset_table_bronze_optionmetrics_ivyopprc_update,\n            extra={\n                \"data_interval_start\": context[\"triggering_asset_events\"][\n                    asset_staging_file_optionmetrics_ivyopprc_update\n                ][-1].extra[\"data_interval_start\"],\n                \"ds\": context[\"triggering_asset_events\"][\n                    asset_staging_file_optionmetrics_ivyopprc_update\n                ][-1].extra[\"ds\"],\n            },\n        )\n\n    emit_assets = emit_assets_if_none_failed()\n\n    daily_changes_job >> [emit_assets, archive_staging_file]\n```\n\nAs one can see, there is only one Asset object referenced in the `schedule` parameter of the DAG, so I expect that one Asset will trigger one DagRun. But sometimes, a DagRun has more than one Source Asset being referenced, as in the following screenshot:\n\n<img width=\"1827\" height=\"863\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/26410430-b6c6-44bc-a661-ab1170b72f3d\" />\n\nThis is a bit random, or at least from my perspective. I wonder if this could be because the scheduler or DAG processor takes too much time to do its loop, and then some Assets get grouped together?\n\nThank you if you can help.\n\n### What you think should happen instead?\n\nOne Asset event should trigger one DagRun, when the DAG only depends on a single Asset. The DAG should not consume more than one Asset events.\n\n### How to reproduce\n\nCreate a parent DAG which emits an Asset event via an AssetAlias object, with the extra field always being unique.\n\nCreate a child DAG which is scheduled using the Asset from the parent DAG.\n\nBackfill the parent DAG (my backfill created 500 DagRuns) with the Max Active Runs parameter set to 1.\n\nMost of the DAG will have a unique source asset, but some of them will have many.\n\n### Operating System\n\nOfficial airflow image\n\n### Versions of Apache Airflow Providers\n\n_No response_\n\n### Deployment\n\nOfficial Apache Airflow Helm Chart\n\n### Deployment details\n\n_No response_\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56690\nTitle: Add Task-Level Resource Usage Metrics (CPU and Memory)\nState: open\nAuthor: HsiuChuanHsu\nLabels: kind:documentation, area:task-sdk\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n### Description\r\nThis PR adds resource usage monitoring at the task level, tracking CPU percentage and memory consumption during task execution.\r\n> Just find out CPU & Memory usage metrics are removed in latest version. These two metrics are definitly needed.\r\n\r\n### Changes\r\n**The measurement works as follows:**\r\n- `psutil.cpu_percent(interval=0)` returns the CPU usage since the last call\r\n- First call establishes the baseline (typically returns 0.0)\r\n- Second call returns the average CPU usage across the entire task execution\r\n- Memory is a point-in-time snapshot, not an average\r\n```bash\r\nTime  Action                          CPU%    Memory\r\n----  ------------------------------  ------  ------\r\n0s    First _get_resource_usage()     0.0     20MB   (establish baseline)\r\n      \u2193\r\n1s    _execute_task() starts            \r\n2s    Task running...                  80%     50MB\r\n3s    Task running...                  90%     100MB\r\n4s    Task running...                  85%     120MB\r\n...\r\n10s   _execute_task() completes\r\n      \u2193\r\n10s   Second _get_resource_usage()     75%     80MB   (recorded!)\r\n      \r\n      cpu_percent = 75%  \u2190 average CPU usage from 0s to 10s\r\n      memory_mb = 80MB   \u2190 instantaneous memory at 10s\r\n```\r\n\r\nRelated: #51602\r\n\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56689\nTitle: EKSPodOperator doesnt use AWS Connection\nState: open\nAuthor: raynorelyp\nLabels: kind:bug, provider:amazon, area:providers, area:core, needs-triage\nBody:\n### Apache Airflow version\n\n2.11.0\n\n### If \"Other Airflow 2/3 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\nTried using an AWS Connection with EKSPodOperator and it didn't work. Tried setting the environment vars and it did work. \n\n### What you think should happen instead?\n\nUsing an AWS Connection should work for the EKSPodOperator\n\n### How to reproduce\n\nSet up and AWS EKS connection.\nTry using EKSPodOperator\n\n### Operating System\n\nMac\n\n### Versions of Apache Airflow Providers\n\n_No response_\n\n### Deployment\n\nOfficial Apache Airflow Helm Chart\n\n### Deployment details\n\nNothing relevant.\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56685\nTitle: feat: async slack notifier\nState: open\nAuthor: dondaum\nLabels: area:providers, provider:slack\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\nAdd an asynchronous version of `SlackNotifier`. \r\n\r\nrelated: https://github.com/apache/airflow/issues/55237\r\n\r\nI sucessfully tested the SlackNotifier with the following Dag:\r\n\r\n```Python\r\nfrom datetime import timedelta\r\nfrom airflow import DAG\r\nfrom airflow.sdk.definitions.deadline import AsyncCallback, DeadlineAlert, DeadlineReference\r\nfrom airflow.providers.slack.notifications.slack import SlackNotifier\r\nfrom airflow.providers.standard.operators.empty import EmptyOperator\r\nfrom airflow.sdk import task\r\n\r\nwith DAG(\r\n    dag_id=\"deadline_slack_alert_example\",\r\n    deadline=DeadlineAlert(\r\n        reference=DeadlineReference.DAGRUN_QUEUED_AT,\r\n        interval=timedelta(seconds=20),\r\n        callback=AsyncCallback(\r\n            SlackNotifier,\r\n            kwargs={\r\n                \"text\": \"\ud83d\udea8 Dag {{ dag_run.dag_id }} missed deadline at {{ deadline.deadline_time }}. DagRun: {{ dag_run }}\",\r\n                \"channel\": \"#allgemein\",\r\n                \"username\": \"Test App\",\r\n            },\r\n        ),\r\n    ),\r\n):\r\n    c = EmptyOperator(task_id=\"example_task\")\r\n\r\n    @task()\r\n    def wait():\r\n        import time\r\n        time.sleep(60*5)\r\n\r\n    \r\n    c >> wait()\r\n```\r\n\r\n\r\n\r\n\r\n<img width=\"1501\" height=\"512\" alt=\"Screenshot 2025-10-15 224828\" src=\"https://github.com/user-attachments/assets/88891fe2-7e8f-4c42-b5a2-230918ca83cb\" />\r\n\r\n<img width=\"1375\" height=\"342\" alt=\"Screenshot 2025-10-15 224858\" src=\"https://github.com/user-attachments/assets/6e7d7a20-36cc-48ff-a218-e4f8e7c0f815\" />\r\n\r\n\r\n\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56682\nTitle: [v3-1-test] Add cleanup of free space for provider tests (#56681)\nState: closed\nAuthor: github-actions[bot]\nLabels: area:dev-tools, backport-to-v3-1-test\nBody:\n(cherry picked from commit 88175f8365e7543c8a7b968d668a2ff6470e90ae)\n\nCo-authored-by: Jarek Potiuk <jarek@potiuk.com>",
  "Requirement ID: ISSUE-56681\nTitle: Add cleanup of free space for provider tests\nState: closed\nAuthor: potiuk\nLabels: area:dev-tools, backport-to-v3-1-test\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56680\nTitle: fix(docs): remove mention of deprecated operator in docs\nState: open\nAuthor: marianore-muttdata\nLabels: area:providers, kind:documentation, provider:databricks\nBody:\nVersion [7.0.0](https://github.com/apache/airflow/blob/66d5e72a7117d3f7fb8b32a61f8215016facee57/providers/databricks/docs/changelog.rst#700) of the provider `apache-airflow-providers-databricks` deprecates the operator and the latest version of the docs still contain it as usable.\r\n\r\n<img width=\"2506\" height=\"1241\" alt=\"image\" src=\"https://github.com/user-attachments/assets/eedf3f46-e397-47f9-9f7e-e0467fe94ae5\" />\r\n\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56679\nTitle: Remove `pytest.importorskip(\"flask_appbuilder\")` from tests\nState: closed\nAuthor: vincbeck\nLabels: provider:microsoft-azure, area:providers, area:serialization, area:plugins, area:API, provider:databricks\nBody:\n`Flask-appbuilder` was migrated to version 5 so these statements can be removed.\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56678\nTitle: fix: handle unmapped task deadlock when upstream tasks are removed\nState: open\nAuthor: kevinhongzl\nLabels: \nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\ncloses: #48816\r\nrelated: #26518\r\n\r\nWhile the issue of deadlock caused by removing upstream mapped tasks has been addressed in #26518, that PR only handled cases where the downstream tasks were also mapped. This PR addresses the task deadlock occurring on downstream unmapped tasks.\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56677\nTitle: [v3-1-test] Update authentication to handle JWT token in backend\nState: closed\nAuthor: vincbeck\nLabels: provider:amazon, area:providers, area:dev-tools, area:API, kind:documentation, area:UI, provider:fab, backport-to-v3-1-test, provider:keycloak\nBody:\nBackport of https://github.com/apache/airflow/pull/56633\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56675\nTitle: Add DdlOperator to execute Data Definition Language (DDL) statements on Teradata databases using TTU tbuild utility\nState: open\nAuthor: sc250072\nLabels: area:providers, kind:documentation, provider:teradata\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\nThis PR introduces a new feature in the Airflow Teradata provider to support execution of Teradata DDL scripts as part of Airflow DAGs. The new operator enhances integration with Teradata by enabling flexible orchestration of DDL workloads both locally and remotely via SSH.\r\n\r\n**Key Features:**\r\n\r\n- Executes DDL SQL statements (CREATE, ALTER, DROP, etc.)\r\n- Works with single statements or batches of multiple DDL operations\r\n- Integrates with Airflow's connection management for secure database access\r\n- Provides comprehensive logging of execution results\r\n- Supports both local and remote execution via SSH\r\n\r\n\r\n\ud83d\udee0\ufe0f Additional Enhancements:\r\nIncludes utility functions for:\r\n\r\nFile validation and encoding checks\r\n\r\n- Script preparation for remote/local execution- \r\n- Robust error handling and comprehensive logging  \r\n- System tests are failing due to issue https://github.com/apache/airflow/issues/56287.\r\n- \r\n- Teradata Provider documentation build status: https://github.com/Teradata/airflow/actions/runs/15688902771\r\n- \r\n- Teradata Provider Unit tests build status: https://github.com/Teradata/airflow/actions/runs/15688585657\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56672\nTitle: [v3-1-test] Update bulk API permission check to handle `action_on_existence`\nState: closed\nAuthor: vincbeck\nLabels: area:API\nBody:\nBackport of #56666.\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56670\nTitle: MySql Deadlock in check_trigger_timeouts\nState: open\nAuthor: punx120\nLabels: kind:bug, area:core, area:Triggerer, needs-triage\nBody:\n### Apache Airflow version\n\nOther Airflow 2/3 version (please specify below)\n\n### If \"Other Airflow 2/3 version\" selected, which one?\n\n2.10.5\n\n### What happened?\n\nHi,\n\nWe got the exact same error as: #41429, it looks to me that we should wrap the `execute` method in `check_trigger_timeouts` so we catch the exception and can actually retry? like `adopt_or_reset_orphaned_tasks` does.\n\nThis happened for the first time shortly after we added a second triggerer instance, so this could make this issue more likely.\n\nThanks\nSylvain\n\n### What you think should happen instead?\n\n_No response_\n\n### How to reproduce\n\nrunning a scheduler and multiple triggerer.\n\n### Operating System\n\nFedora 8.3\n\n### Versions of Apache Airflow Providers\n\n_No response_\n\n### Deployment\n\nOfficial Apache Airflow Helm Chart\n\n### Deployment details\n\n_No response_\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [x] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56669\nTitle: Dag schedules with longer asset names overlap with the entry in the latest run field in dag list\nState: open\nAuthor: shri-astro\nLabels: kind:bug, area:core, area:UI, needs-triage\nBody:\n### Apache Airflow version\n\n3.1.0\n\n### If \"Other Airflow 2/3 version\" selected, which one?\n\n3.0.0 onwards\n\n### What happened?\n\nFor the airflow 3 versions, when the dag schedule is based on an asset with the longer name, the name of the asset extends to overlap with the last run field and makes it hard to read the timestamp of the last run , for the user .\n\n<img width=\"1728\" height=\"154\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/13091cd4-c72a-4ca3-bffe-8a70539fb8e0\" />\n\n### What you think should happen instead?\n\nThe name of the assets should be handled in such a way that the field values don't overlap with each other\n\n### How to reproduce\n\n1. Deploy the dags with asset based schedule to airflow version 3.0.0 based deployment\n2. Run the dags and verify if the name of the asset overlaps the last run field\n\n### Operating System\n\nos\n\n### Versions of Apache Airflow Providers\n\nOfficial Apache Airflow Helm Chart\n\n### Deployment\n\nOfficial Apache Airflow Helm Chart\n\n### Deployment details\n\n_No response_\n\n### Anything else?\n\nRelated to https://github.com/apache/airflow/issues/55721\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56668\nTitle: add statsd service monitor into helm chart\nState: open\nAuthor: ido177\nLabels: area:helm-chart\nBody:\ncloses: [#56664](https://github.com/apache/airflow/issues/56664)\r\n\r\n\r\n\r\n\r\n---\r\nAdd ServiceMonitor for StatsD to enable Prometheus monitoring. This allows Prometheus to scrape metrics from the StatsD service",
  "Requirement ID: ISSUE-56667\nTitle: API-Server occurs memory alarm in K8S Env.\nState: closed\nAuthor: bh7274\nLabels: kind:feature, area:API, needs-triage\nBody:\n### Description\n\nIn the case of the Airflow API server running on Kubernetes, there are situations where memory usage exceeds 75%, 80%, or 85% \u2014 triggering alerts \u2014 regardless of the configured memory limit.\nWould it be possible to provide a configuration option to set the maximum memory usage threshold (as a percentage)?\n\n### Use case/motivation\n\nIn on-premises environments, resource alerts often indicate serious issues.\nProviding such a configuration would help prevent these problems.\n\n### Related issues\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [ ] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56666\nTitle: Update bulk API permission check to handle `action_on_existence`\nState: closed\nAuthor: vincbeck\nLabels: area:API, backport-to-v3-1-test\nBody:\nWhen performing a bulk request, you have the option to pass `action_on_existence` = `overwrite` along a create request. When doing so, if the resource already exists in the DB, the resource gets overridden, in other words, the resource gets updated. Therefore, when a user specifies a create request as part of a bulk API call with `action_on_existence` = `overwrite`, we need to check whether the user has `PUT` permission as well.\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56665\nTitle: fix: corrects otel serialization of file paths in dag processor\nState: open\nAuthor: codecae\nLabels: area:DAG-processing\nBody:\nSerialization of OpenTelemetry stats in the dag processor is attempting to serialize the `file_path` tag using instances of `DagFileInfo` instead of the string version of the filename.  Consequently, stats are not emitted and stack traces are thrown in the dag processor stdout.\r\n\r\nThis PR adjusts the tag to use `str(file.rel_path)` instead of `file`.",
  "Requirement ID: ISSUE-56664\nTitle: Add serviceMonitor for statsd into helm chart\nState: open\nAuthor: ido177\nLabels: kind:bug, kind:feature, area:helm-chart, needs-triage\nBody:\n### Official Helm Chart version\n\n1.18.0 (latest released)\n\n### Apache Airflow version\n\n3.1.0\n\n### Kubernetes Version\n\n1.31.6\n\n### Helm Chart configuration\n\n_No response_\n\n### Docker Image customizations\n\n_No response_\n\n### What happened\n\nIn the current chart version, we have the option to enable statsd, but not serviceMonitor for metrics collection. \n\n### What you think should happen instead\n\nMany production Airflow deployments run in Kubernetes and use Prometheus Operator as the standard for observability. We need to add the ability to enable serviceMonitor for statsd. Otherwise, collecting metrics in Prometheus becomes more difficult.\n\n### How to reproduce\n\nSimply run the chart\n\n### Anything else\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [x] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56663\nTitle: Add new arguments to db_clean to explicitly include or exclude DAGs\nState: open\nAuthor: mattusifer\nLabels: area:CLI\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\ncloses: https://github.com/apache/airflow/issues/24828\r\n\r\nThis PR adds two new arguments to `db_clean`:\r\n\r\n- `dag_ids`: Only remove rows related to the given DAG ids\r\n- `exclude_dag_ids`: Only remove rows NOT related to the given DAG ids\r\n\r\nI started the work [here](https://github.com/apache/airflow/pull/46876), but let the PR turn stale.\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56662\nTitle: [v3-1-test] chore: add credentials to access local airflow instance (#56636)\nState: closed\nAuthor: github-actions[bot]\nLabels: area:dev-tools, backport-to-v3-1-test\nBody:\n(cherry picked from commit 947f4b980137d75530a60edf8af91e38074da2ae)\n\nCo-authored-by: John Nguyen <55610216+nguy4130@users.noreply.github.com>",
  "Requirement ID: ISSUE-56661\nTitle: [Providers][CNCF-Kubernetes] Move Hook functions to PodManager\nState: closed\nAuthor: AutomationDev85\nLabels: area:providers, provider:cncf-kubernetes\nBody:\n# Overview\r\n\r\nWe are preparing an update to the KubernetesPodTriggerer workflow to align its startup behavior with that of the synchronous workflow. As part of this effort, we are introducing this preparation PR:\r\n\r\nThis PR relocates certain \"higher-level\" functions from the Hook to the PodManager. Currently, the Hook imports functions from the PodManager, but we believe the PodManager should instead use the Hook as its abstraction layer to the Kubernetes API. To keep this PR focused and manageable, we are only retaining self._client in the PodManager.\r\n\r\nIn subsequent PRs, we plan to unify the code paths so that both the Triggerer (async) and synchronous workflows use the same logic to start Pods. Without this change, we would encounter circular import issues.\r\n\r\nWe welcome your feedback on this change.\r\n\r\n# Details of change:\r\n\r\n* Move functions from Hook code to the PodManager which match the context of the PodManager\r\n* Remove import from PodManager in the Hook code.",
  "Requirement ID: ISSUE-56660\nTitle: Do not require serialized dag for dags test\nState: open\nAuthor: ephraimbuddy\nLabels: area:CLI, area:serialization, backport-to-v3-1-test\nBody:\nDAG test should be able to run without serialized dag. This PR is to ensure serialized dag is not required when running dag test.\r\n\r\nI used the triggering_user_name to exclude the check for serdag but I wonder if we should have a specific name to exclude serdag in the dagrun creation other than triggering_user_name. Please let me know\r\n\r\nCloses: https://github.com/apache/airflow/issues/56657",
  "Requirement ID: ISSUE-56659\nTitle: [v3-1-test] Bump hatch version to 1.15.0 (#56652)\nState: closed\nAuthor: github-actions[bot]\nLabels: area:dev-tools, backport-to-v3-1-test\nBody:\n(cherry picked from commit 002ce93f6a8d33a935a9c05d2b1fbc9d3c582bfa)\n\nCo-authored-by: Amogh Desai <amoghrajesh1999@gmail.com>",
  "Requirement ID: ISSUE-56658\nTitle: [v3-1-test] Adding amoghrajesh as code owner for task sdk integration testing (#56656)\nState: closed\nAuthor: github-actions[bot]\nLabels: area:dev-tools, backport-to-v3-1-test\nBody:\n(cherry picked from commit b17864e2b0b6b78d598a9dc29fe9e0ab583b5ab8)\n\nCo-authored-by: Amogh Desai <amoghrajesh1999@gmail.com>",
  "Requirement ID: ISSUE-56657\nTitle: Airflow `dag.test()` raises `AirflowException: Cannot create DagRun for DAG because the dag is not serialized`\nState: open\nAuthor: tatiana\nLabels: kind:bug, area:core, affected_version:3.1\nBody:\n### Apache Airflow version\n\n3.1.0\n\n### If \"Other Airflow 2/3 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\nAirflow `dag.test()` stopped working in Airflow 3.1.\n\n### What you think should happen instead?\n\nThe command `dag.test()`  should work as in Airlfow 3.0.\n\n### How to reproduce\n\nCreate this DAG file as `example_bug.py`:\n\n```\nfrom airflow import DAG\nfrom airflow.decorators import dag, task\n\n@dag(\n    dag_id=\"simple_dag\",\n)\ndef simple_dag():\n    \"\"\"A simple DAG with two Python tasks.\"\"\"\n\n    @task\n    def start():\n        print(\"Starting the DAG...\")\n        return \"Hello from start!\"\n\n    @task\n    def end(message: str):\n        print(f\"Received message: {message}\")\n        print(\"Ending the DAG!\")\n\n    msg = start()\n    end(msg)\n\n\ndag = simple_dag()\n\nif __name__ == \"__main__\":\n    dag.test()\n```\n\nAnd run in a Python virtual environment with:\n```\n python example_bug.py\n```\n\nObserve stacktrace similar to:\n```\nTraceback (most recent call last):\n  File \"/Users/tatiana.alchueyr/Code/astronomer-cosmos/example_bug.py\", line 27, in <module>\n    dag.test()\n  File \"/Users/tatiana.alchueyr/Library/Application Support/hatch/env/virtual/astronomer-cosmos/4lAAXoPP/tests.py3.10-3.1-1.10/lib/python3.10/site-packages/airflow/sdk/definitions/dag.py\", line 1202, in test\n    dr: DagRun = get_or_create_dagrun(\n  File \"/Users/tatiana.alchueyr/Library/Application Support/hatch/env/virtual/astronomer-cosmos/4lAAXoPP/tests.py3.10-3.1-1.10/lib/python3.10/site-packages/airflow/models/dagrun.py\", line 2170, in get_or_create_dagrun\n    dr = dag.create_dagrun(\n  File \"/Users/tatiana.alchueyr/Library/Application Support/hatch/env/virtual/astronomer-cosmos/4lAAXoPP/tests.py3.10-3.1-1.10/lib/python3.10/site-packages/airflow/utils/session.py\", line 98, in wrapper\n    return func(*args, **kwargs)\n  File \"/Users/tatiana.alchueyr/Library/Application Support/hatch/env/virtual/astronomer-cosmos/4lAAXoPP/tests.py3.10-3.1-1.10/lib/python3.10/site-packages/airflow/serialization/serialized_objects.py\", line 3257, in create_dagrun\n    orm_dagrun = _create_orm_dagrun(\n  File \"/Users/tatiana.alchueyr/Library/Application Support/hatch/env/virtual/astronomer-cosmos/4lAAXoPP/tests.py3.10-3.1-1.10/lib/python3.10/site-packages/airflow/utils/session.py\", line 98, in wrapper\n    return func(*args, **kwargs)\n  File \"/Users/tatiana.alchueyr/Library/Application Support/hatch/env/virtual/astronomer-cosmos/4lAAXoPP/tests.py3.10-3.1-1.10/lib/python3.10/site-packages/airflow/serialization/serialized_objects.py\", line 2314, in _create_orm_dagrun\n    raise AirflowException(f\"Cannot create DagRun for DAG {dag.dag_id} because the dag is not serialized\")\nairflow.exceptions.AirflowException: Cannot create DagRun for DAG simple_dag because the dag is not serialized\n```\n\n### Operating System\n\nMacOS\n\n### Versions of Apache Airflow Providers\n\n_No response_\n\n### Deployment\n\nOther\n\n### Deployment details\n\nThis issue happens in the local environment, not in a deployed Airflow environment.\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56656\nTitle: Adding amoghrajesh as code owner for task sdk integration testing\nState: closed\nAuthor: amoghrajesh\nLabels: area:dev-tools, backport-to-v3-1-test\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\nAdding self as codeowner to get notified for reviews.\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56655\nTitle: Copy task SDK integration test dags to compose location before job start\nState: closed\nAuthor: amoghrajesh\nLabels: full tests needed\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\nDue to recent merge of https://github.com/apache/airflow/pull/56139, some changes came in that lead to failures in dag processor finding dags at the right location.\r\n\r\nThis solution ensures that:\r\n* The test_dag.py file is available in the correct location for docker-compose to mount\r\n* The docker-compose volume mount `${PWD}/dags:/opt/airflow/dags` will find the DAGs in the temporary directory\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56654\nTitle: Fix Managed Kafka system tests to use correct network name when creating a cluster\nState: closed\nAuthor: VladaZakharova\nLabels: provider:google, area:providers\nBody:\nThis PR adds logic to determine and use the correct network name for cluster creation, since cluster for Kafka should be created in the same network as the machine it is running in.\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56653\nTitle: [v3-1-test] Fix task SDK connection error handling to match airflow-core behavior (#56650)\nState: closed\nAuthor: github-actions[bot]\nLabels: area:task-sdk\nBody:\n(cherry picked from commit 7a834d6e86666e8f9b0ec53b1bc2c1236770d663)\n\nCo-authored-by: Amogh Desai <amoghrajesh1999@gmail.com>",
  "Requirement ID: ISSUE-56652\nTitle: Bump hatch to 1.15.0\nState: closed\nAuthor: amoghrajesh\nLabels: area:dev-tools, backport-to-v3-1-test\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\nFix to broken CI: https://github.com/apache/airflow/actions/runs/18521323945/job/52781859769\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56651\nTitle: Add retry mechanism and error handling to DBT Hook\nState: closed\nAuthor: AardJan\nLabels: area:providers, provider:dbt-cloud\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n**Description**\r\n\r\nThis PR enhances error handling and retry mechanisms in the DBT Hook and its integration with DBT operators.\r\n\r\n## Changes Made\r\n\r\n### DBT Hook\r\n- Implemented comprehensive error handling for both synchronous and asynchronous requests\r\n- Replaced the `run` method with `run_with_advanced_retry` for improved request reliability\r\n- Added `\"reraise\": True` parameter to maintain backward compatibility by raising the original exception after retry threshold is exceeded, rather than a retry-specific error\r\n\r\n### DBT Operator Integration\r\n- Updated DBT operators to utilize the enhanced hook functionality through the `hook_params` parameter\r\n\r\n## Backward Compatibility\r\nThe implementation maintains full backward compatibility with existing code through:  \r\n- Default parameter values that preserve previous behavior\r\n- Use of `reraise` to maintain consistent exception handling\r\n\r\n## Testing\r\nAdded comprehensive test coverage for the new retry functionality.\r\n\r\ncloses: #51801 \r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56650\nTitle: Fix task SDK connection error handling to match airflow-core behavior\nState: closed\nAuthor: amoghrajesh\nLabels: area:task-sdk, backport-to-v3-1-test\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\nUnusual, but when `Connection.get(None)` is called, the stacktrace looks like:\r\n\r\n```\r\n[2025-09-30 16:58:30] WARNING - Skipping masking for a secret as it's too short (<5 chars) source=airflow._shared.secrets_masker.secrets_masker loc=secrets_masker.py:546\r\n[2025-09-30 16:58:30] ERROR - Unable to retrieve connection from secrets backend (EnvironmentVariablesBackend). Checking subsequent secrets backend. source=task loc=context.py:162\r\nAttributeError: 'NoneType' object has no attribute 'upper'\r\nFile \"/opt/airflow/task-sdk/src/airflow/sdk/execution_time/context.py\", line 156 in _get_connection\r\n\r\nFile \"/opt/airflow/airflow-core/src/airflow/secrets/base_secrets.py\", line 76 in get_connection\r\n\r\nFile \"/opt/airflow/airflow-core/src/airflow/secrets/environment_variables.py\", line 34 in get_conn_value\r\n\r\n[2025-09-30 16:58:30] ERROR - Task failed with exception source=task loc=task_runner.py:993\r\nValidationError: 1 validation error for GetConnection\r\nconn_id\r\n  Input should be a valid string [type=string_type, input_value=None, input_type=NoneType]\r\n    For further information visit https://errors.pydantic.dev/2.11/v/string_type\r\nFile \"/opt/airflow/task-sdk/src/airflow/sdk/execution_time/task_runner.py\", line 919 in run\r\n\r\nFile \"/opt/airflow/task-sdk/src/airflow/sdk/execution_time/task_runner.py\", line 1306 in _execute_task\r\n\r\nFile \"/opt/airflow/task-sdk/src/airflow/sdk/bases/operator.py\", line 416 in wrapper\r\n\r\nFile \"/opt/airflow/providers/standard/src/airflow/providers/standard/operators/python.py\", line 216 in execute\r\n\r\nFile \"/opt/airflow/providers/standard/src/airflow/providers/standard/operators/python.py\", line 239 in execute_callable\r\n\r\nFile \"/opt/airflow/task-sdk/src/airflow/sdk/execution_time/callback_runner.py\", line 82 in run\r\n\r\nFile \"/files/dags/print_sensitive_data.py\", line 13 in print_sensitive\r\n\r\nFile \"/opt/airflow/task-sdk/src/airflow/sdk/definitions/connection.py\", line 226 in get\r\n\r\nFile \"/opt/airflow/task-sdk/src/airflow/sdk/execution_time/context.py\", line 183 in _get_connection\r\n\r\nFile \"/usr/python/lib/python3.10/site-packages/pydantic/main.py\", line 253 in __init__\r\n```\r\n\r\n\r\n\r\n\r\nThis is because when a connection is not found, the secrets backend failures log verbosely (log.exception) and continues to API call, allowing cryptic errors to bubble up. Changed that to `log.debug`, so that the task SDK now raises clean `AirflowNotFoundException` instead of cryptic `AttributeError`, matching airflow-core's  error handling.\r\n\r\nExample after:\r\n\r\n```\r\nfrom airflow.sdk import Connection as SDKConnection\r\nSDKConnection.get(None)\r\nTraceback (most recent call last):\r\n  File \"/Users/amoghdesai/Documents/OSS/repos/airflow/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3699, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-9-21c62a62b135>\", line 1, in <module>\r\n    SDKConnection.get(None)\r\n  File \"/Users/amoghdesai/Documents/OSS/repos/airflow/task-sdk/src/airflow/sdk/definitions/connection.py\", line 226, in get\r\n    return _get_connection(conn_id)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/amoghdesai/Documents/OSS/repos/airflow/task-sdk/src/airflow/sdk/execution_time/context.py\", line 172, in _get_connection\r\n    raise AirflowNotFoundException(f\"The conn_id `{conn_id}` isn't defined\")\r\nairflow.exceptions.AirflowNotFoundException: The conn_id `None` isn't defined\r\n```\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56649\nTitle: [v3-1-test] Fix AutoRefresh when only 1 dag run is running #56623\nState: closed\nAuthor: pierrejeambrun\nLabels: area:UI\nBody:\nCherry picked from: 1115cdf\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56648\nTitle: [v3-1-test] Add optional pending dag runs check to auto refresh (#56014)\nState: closed\nAuthor: pierrejeambrun\nLabels: area:UI\nBody:\n* Add optional pending dag runs check to auto refresh\r\n\r\n* Readd hasActiveRun check for structure\r\n\r\n(cherry picked from commit a6506f2b4681a0eceddfc68f82ebaa51c7cb85bc)\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56647\nTitle: Db migration error when upgrading to Airflow 3.1.0\nState: closed\nAuthor: Zingeryo\nLabels: kind:bug, area:core, needs-triage, area:db-migrations\nBody:\n### Apache Airflow version\n\n3.1.0\n\n### If \"Other Airflow 2/3 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\nI have an issue when trying to upgrade my Airflow 2.10.5 to 3.1.0. When i rub `airflow db migrate` i get the following error:\n```\nRunning airflow config list to create default config file if missing.\n\nThe container is run as root user. For security, consider using a regular user account.\n/home/airflow/.local/lib/python3.12/site-packages/airflow/configuration.py:2251 FutureWarning: The 'log_filename_template' setting in [logging] has the old default value of 'dag_id={{ ti.dag_id }}/run_id={{ ti.run_id }}/task_id={{ ti.task_id }}/{% if ti.map_index >= 0 %}map_index={{ ti.map_index }}/{% endif %}attempt={{ try_number }}.log'. This value has been changed to 'dag_id={{ ti.dag_id }}/run_id={{ ti.run_id }}/task_id={{ ti.task_id }}/{% if ti.map_index >= 0 %}map_index={{ ti.map_index }}/{% endif %}attempt={{ try_number|default(ti.try_number) }}.log' in the running config, but please update your config.\n/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py:591 DeprecationWarning: The web_server_port option in [webserver] has been moved to the port option in [api] - the old setting has been used, but please update your config.\n/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py:597 DeprecationWarning: The workers option in [webserver] has been moved to the workers option in [api] - the old setting has been used, but please update your config.\n/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py:609 DeprecationWarning: The web_server_host option in [webserver] has been moved to the host option in [api] - the old setting has been used, but please update your config.\n/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py:614 DeprecationWarning: The access_logfile option in [webserver] has been moved to the access_logfile option in [api] - the old setting has been used, but please update your config.\n/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py:629 DeprecationWarning: The web_server_ssl_cert option in [webserver] has been moved to the ssl_cert option in [api] - the old setting has been used, but please update your config.\n/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py:634 DeprecationWarning: The web_server_ssl_key option in [webserver] has been moved to the ssl_key option in [api] - the old setting has been used, but please update your config.\nTraceback (most recent call last):\n  File \"/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py\", line 1910, in _execute_context\n    self.dialect.do_execute(\n  File \"/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py\", line 736, in do_execute\n    cursor.execute(statement, parameters)\npsycopg2.errors.InvalidTextRepresentation: invalid input syntax for type json\nDETAIL:  Unicode low surrogate must follow a high surrogate.\nCONTEXT:  JSON data, line 1: \"\\udc94...\n\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/airflow/.local/bin/airflow\", line 7, in <module>\n    sys.exit(main())\n             ^^^^^^\n  File \"/home/airflow/.local/lib/python3.12/site-packages/airflow/__main__.py\", line 55, in main\n    args.func(args)\n  File \"/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py\", line 48, in command\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py\", line 111, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/providers_configuration_loader.py\", line 55, in wrapped_function\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/db_command.py\", line 197, in migratedb\n    run_db_migrate_command(args, db.upgradedb, _REVISION_HEADS_MAP)\n  File \"/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/db_command.py\", line 125, in run_db_migrate_command\n    command(\n  File \"/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py\", line 101, in wrapper\n    return func(*args, session=session, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/db.py\", line 1142, in upgradedb\n    command.upgrade(config, revision=to_revision or \"heads\")\n  File \"/home/airflow/.local/lib/python3.12/site-packages/alembic/command.py\", line 483, in upgrade\n    script.run_env()\n  File \"/home/airflow/.local/lib/python3.12/site-packages/alembic/script/base.py\", line 549, in run_env\n    util.load_python_file(self.dir, \"env.py\")\n  File \"/home/airflow/.local/lib/python3.12/site-packages/alembic/util/pyfiles.py\", line 116, in load_python_file\n    module = load_module_py(module_id, path)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/airflow/.local/lib/python3.12/site-packages/alembic/util/pyfiles.py\", line 136, in load_module_py\n    spec.loader.exec_module(module)  # type: ignore\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<frozen importlib._bootstrap_external>\", line 999, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n  File \"/home/airflow/.local/lib/python3.12/site-packages/airflow/migrations/env.py\", line 138, in <module>\n    run_migrations_online()\n  File \"/home/airflow/.local/lib/python3.12/site-packages/airflow/migrations/env.py\", line 132, in run_migrations_online\n    context.run_migrations()\n  File \"<string>\", line 8, in run_migrations\n  File \"/home/airflow/.local/lib/python3.12/site-packages/alembic/runtime/environment.py\", line 946, in run_migrations\n    self.get_context().run_migrations(**kw)\n  File \"/home/airflow/.local/lib/python3.12/site-packages/alembic/runtime/migration.py\", line 627, in run_migrations\n    step.migration_fn(**kw)\n  File \"/home/airflow/.local/lib/python3.12/site-packages/airflow/migrations/versions/0049_3_0_0_remove_pickled_data_from_xcom_table.py\", line 101, in upgrade\n    op.execute(\n  File \"<string>\", line 8, in execute\n  File \"<string>\", line 3, in execute\n  File \"/home/airflow/.local/lib/python3.12/site-packages/alembic/operations/ops.py\", line 2591, in execute\n    return operations.invoke(op)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/airflow/.local/lib/python3.12/site-packages/alembic/operations/base.py\", line 441, in invoke\n    return fn(self, operation)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/home/airflow/.local/lib/python3.12/site-packages/alembic/operations/toimpl.py\", line 240, in execute_sql\n    operations.migration_context.impl.execute(\n  File \"/home/airflow/.local/lib/python3.12/site-packages/alembic/ddl/impl.py\", line 253, in execute\n    self._exec(sql, execution_options)\n  File \"/home/airflow/.local/lib/python3.12/site-packages/alembic/ddl/impl.py\", line 246, in _exec\n    return conn.execute(construct, params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/future/engine.py\", line 286, in execute\n    return self._execute_20(\n           ^^^^^^^^^^^^^^^^^\n  File \"/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py\", line 1710, in _execute_20\n    return meth(self, args_10style, kwargs_10style, execution_options)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/sql/elements.py\", line 334, in _execute_on_connection\n    return connection._execute_clauseelement(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py\", line 1577, in _execute_clauseelement\n    ret = self._execute_context(\n          ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py\", line 1953, in _execute_context\n    self._handle_dbapi_exception(\n  File \"/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py\", line 2134, in _handle_dbapi_exception\n    util.raise_(\n  File \"/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py\", line 211, in raise_\n    raise exception\n  File \"/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py\", line 1910, in _execute_context\n    self.dialect.do_execute(\n  File \"/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py\", line 736, in do_execute\n    cursor.execute(statement, parameters)\nsqlalchemy.exc.DataError: (psycopg2.errors.InvalidTextRepresentation) invalid input syntax for type json\nDETAIL:  Unicode low surrogate must follow a high surrogate.\nCONTEXT:  JSON data, line 1: \"\\udc94...\n\n[SQL: \n            ALTER TABLE xcom\n            ALTER COLUMN value TYPE JSONB\n            USING CASE\n                WHEN value IS NOT NULL THEN CAST(CONVERT_FROM(value, 'UTF8') AS JSONB)\n                ELSE NULL\n            END\n            ]\n(Background on this error at: https://sqlalche.me/e/14/9h9h)\n```\nMy db is postgres:15\n\n### What you think should happen instead?\n\nSuccessful migration \n\n### How to reproduce\n\nRun `airflow db migrate` from airflow 2.10.5 db to airflow 3.1.0 db\n\n### Operating System\n\nUbuntu 24.04.3 LTS\n\n### Versions of Apache Airflow Providers\n\n_No response_\n\n### Deployment\n\nDocker-Compose\n\n### Deployment details\n\nMy docker-compose file:\n```\n---\nx-airflow-common:\n  &airflow-common\n  image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:3.1.0}\n  env_file:\n    - path: ./.env\n  volumes:\n    - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags\n    - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs\n    - ${AIRFLOW_PROJ_DIR:-.}/config:/opt/airflow/config\n    - ${AIRFLOW_PROJ_DIR:-.}/plugins:/opt/airflow/plugins\n    - ${AIRFLOW_PROJ_DIR:-.}/dbt:/opt/airflow/dbt\n  user: \"${AIRFLOW_UID:-50000}:0\"\n  depends_on:\n    &airflow-common-depends-on\n    redis:\n      condition: service_healthy\n\nservices:\n  redis:\n    image: redis:7.2-bookworm\n    ports:\n      - \"6379:6379\"\n    healthcheck:\n      test: [\"CMD\", \"redis-cli\", \"ping\"]\n      interval: 10s\n      timeout: 30s\n      retries: 50\n      start_period: 30s\n    restart: always\n\n  airflow-webserver:\n    <<: *airflow-common\n    command: config update\n    network_mode: host\n    volumes:\n      - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags\n      - ${AIRFLOW_PROJ_DIR:-.}/webserver_config.py:/opt/airflow/webserver_config.py\n      - ${AIRFLOW_PROJ_DIR:-.}/config:/opt/airflow/config\n      - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs\n      - ${AIRFLOW_PROJ_DIR:-.}/dbt:/opt/airflow/dbt\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"--fail\", \"http://localhost:8080/api/v2/version\"]\n      interval: 30s\n      timeout: 10s\n      retries: 5\n      start_period: 30s\n    restart: always\n    depends_on:\n      <<: *airflow-common-depends-on\n      airflow-init:\n        condition: service_completed_successfully\n\n\n  airflow-scheduler:\n    <<: *airflow-common\n    command: scheduler\n    network_mode: host\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"--fail\", \"http://localhost:8974/health\"]\n      interval: 30s\n      timeout: 10s\n      retries: 5\n      start_period: 30s\n    restart: always\n    depends_on:\n      <<: *airflow-common-depends-on\n      airflow-init:\n        condition: service_completed_successfully\n\n  airflow-dag-processor:\n    <<: *airflow-common\n    command: dag-processor\n    healthcheck:\n      test: [\"CMD-SHELL\", 'airflow jobs check --job-type DagProcessorJob --hostname \"$${HOSTNAME}\"']\n      interval: 30s\n      timeout: 10s\n      retries: 5\n      start_period: 30s\n    restart: always\n    depends_on:\n      <<: *airflow-common-depends-on\n      airflow-init:\n        condition: service_completed_successfully\n\n  airflow-triggerer:\n    <<: *airflow-common\n    command: triggerer\n    network_mode: host\n    healthcheck:\n      test: [\"CMD-SHELL\", 'airflow jobs check --job-type TriggererJob --hostname \"$${HOSTNAME}\"']\n      interval: 30s\n      timeout: 10s\n      retries: 5\n      start_period: 30s\n    restart: always\n\n  airflow-init:\n    <<: *airflow-common\n    entrypoint: /bin/bash\n    network_mode: host\n    command:\n      - -c\n      - |\n        if [[ -z \"${AIRFLOW_UID}\" ]]; then\n          echo\n          echo -e \"\\033[1;33mWARNING!!!: AIRFLOW_UID not set!\\e[0m\"\n          echo \"If you are on Linux, you SHOULD follow the instructions below to set \"\n          echo \"AIRFLOW_UID environment variable, otherwise files will be owned by root.\"\n          echo \"For other operating systems you can get rid of the warning with manually created .env file:\"\n          echo \"    See: https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#setting-the-right-airflow-user\"\n          echo\n          export AIRFLOW_UID=$$(id -u)\n        fi\n        one_meg=1048576\n        mem_available=$$(($$(getconf _PHYS_PAGES) * $$(getconf PAGE_SIZE) / one_meg))\n        cpus_available=$$(grep -cE 'cpu[0-9]+' /proc/stat)\n        disk_available=$$(df / | tail -1 | awk '{print $$4}')\n        warning_resources=\"false\"\n        if (( mem_available < 4000 )) ; then\n          echo\n          echo -e \"\\033[1;33mWARNING!!!: Not enough memory available for Docker.\\e[0m\"\n          echo \"At least 4GB of memory required. You have $$(numfmt --to iec $$((mem_available * one_meg)))\"\n          echo\n          warning_resources=\"true\"\n        fi\n        if (( cpus_available < 2 )); then\n          echo\n          echo -e \"\\033[1;33mWARNING!!!: Not enough CPUS available for Docker.\\e[0m\"\n          echo \"At least 2 CPUs recommended. You have $${cpus_available}\"\n          echo\n          warning_resources=\"true\"\n        fi\n        if (( disk_available < one_meg * 10 )); then\n          echo\n          echo -e \"\\033[1;33mWARNING!!!: Not enough Disk space available for Docker.\\e[0m\"\n          echo \"At least 10 GBs recommended. You have $$(numfmt --to iec $$((disk_available * 1024 )))\"\n          echo\n          warning_resources=\"true\"\n        fi\n        if [[ $${warning_resources} == \"true\" ]]; then\n          echo\n          echo -e \"\\033[1;33mWARNING!!!: You have not enough resources to run Airflow (see above)!\\e[0m\"\n          echo \"Please follow the instructions to increase amount of resources available:\"\n          echo \"   https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#before-you-begin\"\n          echo\n        fi\n        echo\n        echo \"Creating missing opt dirs if missing:\"\n        echo\n        mkdir -v -p /opt/airflow/{logs,dags,plugins,config}\n        echo\n        echo \"Airflow version:\"\n        /entrypoint airflow version\n        echo\n        echo \"Files in shared volumes:\"\n        echo\n        ls -la /opt/airflow/{logs,dags,plugins,config}\n        echo\n        echo \"Running airflow config list to create default config file if missing.\"\n        echo\n        /entrypoint airflow config list >/dev/null\n        echo\n        echo \"Files in shared volumes:\"\n        echo\n        ls -la /opt/airflow/{logs,dags,plugins,config}\n        echo\n        echo \"Change ownership of files in /opt/airflow to ${AIRFLOW_UID}:0\"\n        echo\n        chown -R \"${AIRFLOW_UID}:0\" /opt/airflow/\n        echo\n        echo \"Change ownership of files in shared volumes to ${AIRFLOW_UID}:0\"\n        echo\n        chown -v -R \"${AIRFLOW_UID}:0\" /opt/airflow/{logs,dags,plugins,config}\n        echo\n        echo \"Files in shared volumes:\"\n        echo\n        ls -la /opt/airflow/{logs,dags,plugins,config}\n    # yamllint enable rule:line-length\n    environment:\n      _AIRFLOW_DB_MIGRATE: 'true'\n      _AIRFLOW_WWW_USER_CREATE: 'true'\n      _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}\n      _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow}\n      _PIP_ADDITIONAL_REQUIREMENTS: ''\n    user: \"0:0\"\n\n    volumes:\n      - ${AIRFLOW_PROJ_DIR:-.}:/sources\n      - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags\n      - ${AIRFLOW_PROJ_DIR:-.}/webserver_config.py:/opt/airflow/webserver_config.py\n      - ${AIRFLOW_PROJ_DIR:-.}/config:/opt/airflow/config\n      - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs\n      - ${AIRFLOW_PROJ_DIR:-.}/dbt:/opt/airflow/dbt\n\n  airflow-cli:\n    <<: *airflow-common\n    profiles:\n      - debug\n    environment:\n      CONNECTION_CHECK_MAX_COUNT: \"0\"\n    # Workaround for entrypoint issue. See: https://github.com/apache/airflow/issues/16252\n    command:\n      - bash\n      - -c\n      - airflow\n    depends_on:\n      <<: *airflow-common-depends-on\n```\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56646\nTitle: Airflow 3.0.6 ignoring max_active_runs_per_dag\nState: open\nAuthor: jtreffler\nLabels: kind:bug, area:Scheduler, area:core, needs-triage, affected_version:3.0, affected_version:3.1\nBody:\n### Apache Airflow version\n\nOther Airflow 2/3 version (please specify below)\n\n### If \"Other Airflow 2/3 version\" selected, which one?\n\n3.0.6\n\n### What happened?\n\nIf I configure max_active_runs_per_dag = 1 , catchup = true and schedule a dag in short cycles i see following behaviour. On first run it starts one dag run and the creates the rest of the runs which are up to come as queued. \nOnce the first run is finished it starts all runs which are not yet started in parallel (see screenshot)\n\n<img width=\"2229\" height=\"1207\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/4a9bb799-dc1c-450b-8d8a-66b1656be9f4\" />\n\nWe have same behaviour if we clear tasks backwards to recalculate some days.\n\n### What you think should happen instead?\n\nthe Dag runs should be triggered one after another in correct order, otherwise you can produce huge data issues in cases you have calculations which needs to go in correct timed order\n\n### How to reproduce\n\ncreate a dag which will trigger every hour, catchup true, start date should be today or yesterday, configure max_active_runs_per_dag = 1 and enable the dag...\n\n### Operating System\n\nubi9/python312\n\n### Versions of Apache Airflow Providers\n\nn/a (pip contract enforced)\n\n### Deployment\n\nOfficial Apache Airflow Helm Chart\n\n### Deployment details\n\nwe are using self written helm charts for deployment\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56645\nTitle: Refactor RenderedTaskInstanceFields.write to be an instance method\nState: open\nAuthor: humit0\nLabels: \nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\nhttps://github.com/apache/airflow/blob/ec42f30c9dd338655ac6c6f277f8c7d66c11d4f5/airflow-core/src/airflow/models/renderedtifields.py#L203-L206\r\n\r\nThe `write` method was being called on the `RenderedTaskInstanceFields` class.\r\n\r\nI refactored it to be a direct instance method call for adherence to OOP principles.\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56644\nTitle: [v3-1-test] bump zizmor and python 3.13 patch level version in global_constants (#56639)\nState: closed\nAuthor: github-actions[bot]\nLabels: area:dev-tools, backport-to-v3-1-test\nBody:\n(cherry picked from commit ec42f30c9dd338655ac6c6f277f8c7d66c11d4f5)\n\nCo-authored-by: GPK <gopidesupavan@gmail.com>",
  "Requirement ID: ISSUE-56643\nTitle: Fix task sdk integration tests, copy dags to working dir\nState: closed\nAuthor: gopidesupavan\nLabels: full tests needed\nBody:\nhttps://github.com/apache/airflow/actions/runs/18515458033/job/52767293548\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56642\nTitle: Enable PT011 rule to prvoider tests\nState: closed\nAuthor: xchwan\nLabels: provider:amazon, area:providers\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\nissue: Enable Even More PyDocStyle Checks #40567\r\n@ferruzzi\r\nThis PR is for enable PT011 rule:\r\nPT011: all instances of pytest.raises should include a match term to make sure they are catching the right error.\r\nhttps://docs.astral.sh/ruff/rules/pytest-raises-too-broad/\r\nThere are 102 files changes is need.\r\nSo, I separate to many PR, which contains about 5 file changes for easy review.\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56641\nTitle: Root Cause Investigation: Memory Growth in LocalExecutor Workers (Scheduler Subprocesses)\nState: open\nAuthor: wjddn279\nLabels: kind:bug, area:Scheduler, priority:high, area:core, needs-triage, affected_version:3.0, affected_version:3.1\nBody:\n### Apache Airflow version\n\n3.1.0\n\n### If \"Other Airflow 2/3 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\nrelated to: https://github.com/apache/airflow/issues/55768#issuecomment-3402928673\nMemory occupancy continues to rise in scheduler containers\n\n### What you think should happen instead?\n\nMemory usage must not rise in airflow deployed by docker compose.\n\n\n### How to reproduce\n\nTest enviroment:\n- airflow 3.1.0 official docker image\n- deployed by docker compose (api-server, scheduler, dag-processor)\n- 100 Dags with 5 PythonOperator running every minute\n\n### Operating System\n\ndocker container operated in macOs \n\n### Versions of Apache Airflow Providers\n\n_No response_\n\n### Deployment\n\nDocker-Compose\n\n### Deployment details\n\n````\nx-airflow-common:\n  &airflow-common\n  image: apache/airflow:3.1.0\n  environment:\n    &airflow-common-env\n    AIRFLOW__CORE__EXECUTOR: LocalExecutor\n    AIRFLOW__CORE__AUTH_MANAGER: airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager\n    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow\n    AIRFLOW__CORE__FERNET_KEY: ''\n    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'\n    AIRFLOW__CORE__EXECUTION_API_SERVER_URL: 'http://airflow-apiserver:8080/execution/'\n    AIRFLOW__API__SECRET_KEY: 'abc'\n    AIRFLOW__API_AUTH__JWT_SECRET: 'asdasd'\n    AIRFLOW__SCHEDULER__ENABLE_TRACEMALLOC: 'false'\n  volumes:\n    - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags\n  depends_on:\n    &airflow-common-depends-on\n    postgres:\n      condition: service_healthy\n\nservices:\n  postgres:\n    image: postgres:13\n    ports:\n      - \"5432:5432\"\n    environment:\n      POSTGRES_USER: airflow\n      POSTGRES_PASSWORD: airflow\n      POSTGRES_DB: airflow\n    healthcheck:\n      test: [\"CMD\", \"pg_isready\", \"-U\", \"airflow\"]\n      interval: 10s\n      retries: 5\n      start_period: 5s\n    restart: always\n\n  airflow-init:\n    <<: *airflow-common\n    entrypoint: /bin/bash\n    command:\n      - -c\n      - |\n        echo \"Creating missing opt dirs if missing:\"\n        mkdir -v -p /opt/airflow/{logs,dags,plugins,config}\n        echo \"Airflow version:\"\n        /entrypoint airflow version\n        echo \"Running airflow config list to create default config file if missing.\"\n        /entrypoint airflow config list >/dev/null\n        echo \"Change ownership of files in /opt/airflow to ${AIRFLOW_UID}:0\"\n        chown -R \"${AIRFLOW_UID}:0\" /opt/airflow/\n    environment:\n      <<: *airflow-common-env\n      _AIRFLOW_DB_MIGRATE: 'true'\n      _AIRFLOW_WWW_USER_CREATE: 'true'\n      _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}\n      _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow}\n      _PIP_ADDITIONAL_REQUIREMENTS: ''\n    user: \"0:0\"\n    depends_on:\n      <<: *airflow-common-depends-on\n\n  airflow-apiserver:\n    <<: *airflow-common\n    command: api-server\n    ports:\n      - \"8080:8080\"\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"--fail\", \"http://localhost:8080/api/v2/version\"]\n      interval: 30s\n      timeout: 10s\n      retries: 5\n      start_period: 30s\n    restart: always\n    depends_on:\n      <<: *airflow-common-depends-on\n      airflow-init:\n        condition: service_completed_successfully\n\n  airflow-scheduler:\n    <<: *airflow-common\n    command: scheduler\n    healthcheck:\n      test: [\"CMD\", \"airflow\", \"jobs\", \"check\", \"--job-type\", \"SchedulerJob\"]\n      interval: 30s\n      timeout: 10s\n      retries: 5\n    restart: always\n    depends_on:\n      postgres:\n        condition: service_healthy\n      airflow-init:\n        condition: service_completed_successfully\n\n\n  airflow-dag-processor:\n    <<: *airflow-common\n    command: dag-processor\n    healthcheck:\n      test: [\"CMD-SHELL\", 'airflow jobs check --job-type DagProcessorJob --hostname \"$${HOSTNAME}\"']\n      interval: 30s\n      timeout: 10s\n      retries: 5\n      start_period: 30s\n    restart: always\n    depends_on:\n      <<: *airflow-common-depends-on\n      airflow-init:\n        condition: service_completed_successfully\n````\n\n### Anything else?\n\nAs mentioned earlier https://github.com/apache/airflow/issues/55768#issuecomment-3374894204, the observed memory increase originates from both the scheduler process and its subprocesses \u2014 the LocalExecutor workers.\nThe scheduler\u2019s own memory growth has already been analyzed and discussed by @kaxil https://github.com/apache/airflow/issues/55768#issuecomment-3353598174, so I will not cover it here.\n\nWhen running with the LocalExecutor, the default number of worker processes is 32.\nSince any memory increase per worker is multiplied across all 32 workers, even small leaks can have a critical impact on overall memory usage.\n\nI used Memray to analyze the worker processes (which are child processes of the scheduler) and identified three main causes of excessive memory allocation within them.\n\n### 1. Importing the k8s client object\n\nFirst, here is the result of analyzing a single worker process:\n[memray-flamegraph-output-111.html](https://github.com/user-attachments/files/22916972/memray-flamegraph-output-111.html)\n\n[In this section](https://github.com/apache/airflow/blob/main/shared/secrets_masker/src/airflow_shared/secrets_masker/secrets_masker.py#L164C10-L164C47), I confirmed that approximately 32 MB of memory is allocated per worker.\nAlthough the code only appears to reference the object\u2019s type, it actually triggers imports of all underlying submodules.\nSince each worker imports these modules independently, this results in an additional ~1 GB of total memory allocation across all workers.\n\n### 2. Increasing memory from client SSL objects\n\nAfter modifying the problematic code in (1) to prevent the import, I ran memory profiling again.\nWhile the initial memory footprint per worker was significantly reduced, I still observed gradual memory growth over time. (0928 means the stats is reported in 09:28)\n[remove-k8s-0928.html](https://github.com/user-attachments/files/22916986/remove-k8s-0928.html)\n[remove-k8s-1001.html](https://github.com/user-attachments/files/22916988/remove-k8s-1001.html)\n[remove-k8s-1035.html](https://github.com/user-attachments/files/22916990/remove-k8s-1035.html)\n\n[In the following section](https://github.com/apache/airflow/blob/main/task-sdk/src/airflow/sdk/api/client.py#L828), the SSL initialization appears not to properly release memory.\nWithin about 30 minutes, a single worker\u2019s memory grew from 8 MB \u2192 23 MB, later exceeding 50 MB, and continued to increase steadily thereafter.\n\n### 3. Memory inheritance from the parent process due to lazy forking\n\nAfter addressing issues (1) and (2), I verified that the overall memory consumption remained stable and did not exhibit continuous growth.\nHowever, I noticed that while initial PSS values were low, they gradually increased to relatively high levels over time.  \n[memory_smem.txt](https://github.com/user-attachments/files/22917005/memory_smem.txt)\n\nIt was difficult to track the exact distribution using Memray due to extensive shared memory usage \u2014 very little heap memory remained in the workers themselves.\n\nMy hypothesis is as follows:\nUnlike Airflow 2.x, version 3.x introduced lazy worker initialization.\nAs a result, when the scheduler (already holding significant memory) forks a new worker, Copy-on-Write (CoW) causes shared pages to be duplicated across workers, leading to increased per-process memory consumption.\n\n### Conclusion\n\nTo verify this hypothesis, I modified the code to eagerly spawn worker processes before the scheduler enters its scheduling loop \u2014 effectively disabling lazy forking.\nThe experiment showed that worker memory usage remained stable and no longer exhibited the previous pattern of gradual growth.\n\n[memory_smem_2.txt](https://github.com/user-attachments/files/22917012/memory_smem_2.txt)\n\n### Are you willing to submit PR?\n\n- [x] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56640\nTitle: Add Audit Logs detailed documentation\nState: open\nAuthor: KoviAnusha\nLabels: kind:documentation, backport-to-v3-1-test\nBody:\ncloses: #29961\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #29961\r\nrelated: #29961\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\nAdded detailed documentation page explaining Airflow Audit Logs \u2014 their purpose, event catalog, and how to query them via UI, REST API, or SQL. Also distinguishes audit logs from event logs and includes key usage examples.\r\n\r\n<img width=\"715\" height=\"587\" alt=\"image\" src=\"https://github.com/user-attachments/assets/0f3eaac6-fdca-4f1b-ae60-6180c486509e\" />\r\n<img width=\"715\" height=\"657\" alt=\"image\" src=\"https://github.com/user-attachments/assets/1a568d99-824b-411a-a9f9-0df5a21f8f90\" />\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56639\nTitle: bump zizmor and python 3.13 patch level version in global_constants\nState: closed\nAuthor: gopidesupavan\nLabels: area:dev-tools, backport-to-v3-1-test\nBody:\nhttps://github.com/apache/airflow/actions/runs/18507857252/job/52741317469\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56638\nTitle: [v3-1-test] Improve UI retry strategy on client errors (#56625)\nState: closed\nAuthor: github-actions[bot]\nLabels: area:UI\nBody:\n(cherry picked from commit 7939692b266a20b340f133f998612f083c585b6c)\n\nCo-authored-by: Pierre Jeambrun <pierrejbrun@gmail.com>",
  "Requirement ID: ISSUE-56637\nTitle: [v3-1-test] Finalising Core Documentation Pages Screenshot Update (#56167)\nState: closed\nAuthor: github-actions[bot]\nLabels: kind:documentation\nBody:\ncloses https://github.com/apache/airflow/issues/55521?\n(cherry picked from commit d2df89e795216f279a0c584f2c3e22b89dab8948)\n\nCo-authored-by: Kavya Katal <KAVYAKATAL09@GMAIL.COM>",
  "Requirement ID: ISSUE-56636\nTitle: chore: add credentials to access local airflow webserver\nState: closed\nAuthor: nguy4130\nLabels: area:dev-tools, backport-to-v3-1-test\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n\r\nDuring the workshop at the Airflow Summit I had to ask for the credentials to access the local Airflow Webserver UI. Adding this to the the breeze output to make it explicit and easier for beginners.\r\n\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).\r\n\r\nBefore \r\n<img width=\"686\" height=\"397\" alt=\"Screenshot 2025-10-14 at 4 07 50\u202fPM\" src=\"https://github.com/user-attachments/assets/40681621-ec27-4340-91c5-a8c5bc35e7a8\" />\r\n\r\nAfter\r\n<img width=\"693\" height=\"255\" alt=\"Screenshot 2025-10-14 at 4 07 32\u202fPM\" src=\"https://github.com/user-attachments/assets/f07e261a-52d1-43b5-9c13-4b5052b97dbb\" />",
  "Requirement ID: ISSUE-56635\nTitle: Slow API server website load times\nState: open\nAuthor: trau-sca\nLabels: kind:bug, area:performance, area:API, area:core, needs-triage\nBody:\n### Apache Airflow version\n\n3.1.0\n\n### If \"Other Airflow 2/3 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\nHi, we've been battling issues for the past few weeks on our api-server pod being very slow to display the website information (up to a minute to load the dags card screen). Sometimes the api server pod even restarts due to the liveness probe. I don't see any errors when it happens either.\n\nQuite a few requests look long, but the biggest one is `dags?dag_runs_limit=14&limit=50&offset=0&exclude_stale=true&order_by=-dag_display_name` taking 51s, but even `dags?is_favorite=true` takes 4 seconds.\n\nWhen remoting into the dag-processor and doing a curl command for the first one - it also takes 51s. I used the private endpoint assigned to the api-server while I was in the dag-processor.\n\nWe are running on airflow 3.1.0 and the 1.18.0 helm chart in an Azure Kubernetes cluster with an Azure PostgreSQL database.\nNo pods are going past their resource requests when this issue occurs, and the resources for the database are very low in usage.\n\nI have disabled sqlalchemy pooling, and use pgbouncer in airflow as well as in the PostgreSQL database.\nI have the following configuration for airflow:\n```\npgbouncer:\n    enabled: true\n    maxClientConn: 500\n    metadataPoolSize: 50\n    resultBackendPoolSize: 10\n    extraIni: |\n      pool_mode = transaction\n```\nOn PostgreSQL, I have pgbouncer's pool_mode set to session, and the default_pool_size is 100.\n\nWe have 16 DAGs, and have a daily job that runs a db clean for anything older than 90 days.\n\n### What you think should happen instead?\n\n_No response_\n\n### How to reproduce\n\nClicking on DAGs card screen - I have 16 DAGs running with an Azure PostgreSQL backend.\n\n### Operating System\n\nAzure Kubernetes\n\n### Versions of Apache Airflow Providers\n\n_No response_\n\n### Deployment\n\nOfficial Apache Airflow Helm Chart\n\n### Deployment details\n\nAzure PostgreSQL 16.9, Azure Kubernetes v1.32.7\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56634\nTitle: feat: adds support for OpenTelemetry standard environment variables f\u2026\nState: open\nAuthor: codecae\nLabels: kind:documentation\nBody:\nThe current OpenTelemetry implementation has a fairly rigid configuration specification for endpoint host/port and export interval. While this is probably okay for most configurations, there are cases in which endpoints do not conform with the standard implied by Airflow's implementation. See https://prometheus.io/docs/guides/opentelemetry/#send-opentelemetry-metrics-to-the-prometheus-server for an example of how the API URL can stray from the basic \"/v1/metrics\" route, yet still be entirely valid.\r\n\r\nInstead of adding new configuration values to the metrics and traces sections, this PR adds transparent support for all standard OpenTelemetry environment variables (so far) that would otherwise be overridden by Airflow's configuration.\r\n\r\nThis change includes the support for:\r\n\r\n- OTEL_EXPORTER_OTLP_ENDPOINT\r\n- OTEL_EXPORTER_OTLP_METRICS_ENDPOINT\r\n- OTEL_EXPORTER_OTLP_TRACES_ENDPOINT\r\n- OTEL_METRIC_EXPORT_INTERVAL",
  "Requirement ID: ISSUE-56633\nTitle: Update UI authentication to handle JWT token in backend\nState: closed\nAuthor: vincbeck\nLabels: provider:amazon, area:providers, area:API, kind:documentation, area:UI, provider:fab, backport-to-v3-1-test, provider:keycloak\nBody:\nResolves https://github.com/apache/airflow/issues/55143.\r\n\r\nFor better simplicity and better security this PR updates the Airflow UI authentication. With this change the front-end no longer needs to handle the JWT token, it makes API calls to Airflow API (public and UI APIs) with no authentication. A fallback mechanism is added to the back-end so that, if no authentication is provided as part of a request, it fetches the JWT token from the cookies (if it exists). This cookie is saved by auth managers like today, the only change is now this cookie is httpOnly, so Javascript cannot read it.\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56632\nTitle: Dynamic DAG parameter options based on external configuration (dropdown / radio support)\nState: open\nAuthor: rolfschleiss\nLabels: kind:feature, good first issue, area:UI\nBody:\n### Description\n\nWhen triggering a DAG in the Airflow web UI, all parameters defined in the params section are currently rendered as plain text input fields.\nIt would be highly useful if Airflow supported dynamic UI field types (e.g. dropdowns or radio buttons) that can populate their options from external configuration files, such as INI, YAML or JSON.\n\n### Use case/motivation\n\nIn many environments, DAGs depend on configuration files that define available interfaces, systems, or targets.\nFor example, an INI file might describe multiple interfaces with different types:\n\n```\n[InterfaceA]\nTYPE = Script\n\n[InterfaceB]\nTYPE = EBICS\n\n[InterfaceC]\nTYPE = Script\n```\n\nWhen triggering the DAG, users should be able to select from all sections that match a given condition (e.g. TYPE = Script), using a dropdown or radio button instead of free-text entry.\n\nExample goal:\n\nShow a dropdown listing InterfaceA and InterfaceC, because both are defined as TYPE = Script in the INI file.\n\nThis would make DAG triggering:\n\n- safer (no typos in interface names),\n- more dynamic (values adapt automatically to config changes),\n- and much more user-friendly for non-technical operators.\n\nAlternatively, Airflow could provide a plugin interface to populate parameter choices dynamically,\nso that DAG authors can implement their own logic for which options should appear.\n\nBenefits\n\n- Greatly improves safety and UX for parameterized DAGs.\n- Keeps Airflow configuration-driven (no need to hardcode options).\n- Enables dynamic parameter lists that reflect live configuration.\n- Avoids invalid values and reduces operational mistakes.\n\nPossible Implementation Notes\n\n- Extend the Web UI (React) to render dropdowns/radio fields when ui metadata is provided.\n- Allow a DAG to expose a small API or callable hook to fetch dynamic choices.\n- Could reuse existing Airflow variable/config reading mechanisms.\n\nProposed Change\n\nAllow params definitions to reference an external source for valid options.\n\nExample:\n\n```\nwith DAG(\n    \"interface_trigger_dag\",\n    params={\n        \"interface_name\": {\n            \"source\": \"config/interfaces.ini\",\n            \"filter\": {\"TYPE\": \"Script\"},\n            \"key\": \"section_name\",\n            \"ui\": \"dropdown\"  # or \"radio\"\n        }\n    },\n) as dag:\n    ...\n```\n\nLabels:\nenhancement, UI, params, feature-request, dynamic-config\n\n### Related issues\n\n#56427 \u2013 [DAG Params Enum not works as expected](https://github.com/apache/airflow/issues/56427?utm_source=chatgpt.com)\n: Dropdown rendering for enum params not functioning correctly.\n\n#42524 \u2013 [DAG Param incorrectly convert enum objects to strings](https://github.com/apache/airflow/issues/42524?utm_source=chatgpt.com)\n: Enum parameters are displayed as plain strings instead of UI choices.\n\n#31399 \u2013 [Trigger UI Form Dropdowns with enums do not set default correctly](https://github.com/apache/airflow/issues/31399?utm_source=chatgpt.com)\n: Dropdown default value not applied properly in DAG trigger form.\n\n#48481 \u2013 [Multiple-choice Params (discussion)](https://github.com/apache/airflow/discussions/48481?utm_source=chatgpt.com)\n: Community discussion about extending enum params to support richer UI controls.\n\n#39904 \u2013 [Dynamic DAG Params behave differently in manually triggered runs](https://github.com/apache/airflow/issues/39904?utm_source=chatgpt.com)\n: Dynamic parameter behavior inconsistency between manual and scheduled runs.\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56631\nTitle: Use ValueError instead of RuntimeError when resolving SparkSubmitHook connection\nState: closed\nAuthor: ephraimbuddy\nLabels: area:providers, provider:apache-spark\nBody:\nIn the future, connection error would raise RuntimeError instead of AirflowException\r\n and in that case, if we also have the RuntimeError in the Spark connection,\r\nwe won't be able to raise it as we would like to catch the RuntimeError from connection.\r\n\r\nAlso, looking at what the code does, ValueError is more appropriate",
  "Requirement ID: ISSUE-56630\nTitle: [v3-1-test] Docs: update CI/CD workflows documentation (Closes #41933) (#56610)\nState: closed\nAuthor: github-actions[bot]\nLabels: area:dev-tools, backport-to-v3-1-test\nBody:\n* fix formatting in 03a_contributors_quick_start_beginners.rst filre\n\n* docs: fix formatting in Codespaces and Next Steps sections\n\n* Fix formatting for steps 4-6 in development setup guide\n\n* Update documentation for CI/CD to include composite workflows #41933\n\n* Update documentation for CI/CD to include composite workflows\n\n---------\n(cherry picked from commit cc6760c70694192f640bdf6f7c44815104f8cb09)\n\nCo-authored-by: Anusha Kovi <parvathi.kovi@gmail.com>\nCo-authored-by: Anusha Kovi <anushakovi@Anushas-Air.lan>",
  "Requirement ID: ISSUE-56628\nTitle: [v3-1-test] Improve api doc for ordering params\nState: closed\nAuthor: pierrejeambrun\nLabels: area:API, area:UI\nBody:\nManual backport for:\r\n- https://github.com/apache/airflow/pull/55988\r\n- https://github.com/apache/airflow/pull/56617\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56626\nTitle: [v3-1-test] Fix UI keeps poking pools API when no permission (#56163)\nState: closed\nAuthor: pierrejeambrun\nLabels: area:UI\nBody:\n(cherry picked from commit 9ed441418e42833ad48a1137587108dcb598f710)\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56625\nTitle: Improve UI retry strategy on client errors\nState: closed\nAuthor: pierrejeambrun\nLabels: area:UI, backport-to-v3-1-test\nBody:\nThis modify the query client to not retry request (both gets and mutations) when we receive a 4xx response from the server. (Client error), unless this is rate limiting.\r\n\r\nWill prevent extra useless and allow UI error handling code to fail early, beside staying in 'isLoading' mode until all retries are exhausted.",
  "Requirement ID: ISSUE-56624\nTitle: Fix: Implement token refresh for Keycloak provider (#56614)\nState: closed\nAuthor: fediabdelhedi\nLabels: area:providers, provider:keycloak\nBody:\n## What\r\nImplements automatic token refresh when Keycloak access token expires.\r\n\r\n## Why\r\nFixes #56614 - Users receive 500 Internal Server Error after token lifetime (5 minutes) expires. The Keycloak provider was not implementing OAuth2 refresh token flow.\r\n\r\n## How\r\n1. **Added `_refresh_access_token()` method** that:\r\n   - Retrieves refresh token from the user object\r\n   - Calls Keycloak token endpoint with `grant_type=refresh_token`\r\n   - Updates user's access token and refresh token\r\n   - Includes comprehensive error handling and logging\r\n\r\n2. **Modified `_is_authorized()` to**:\r\n   - Catch 401 Unauthorized responses (token expired)\r\n   - Attempt token refresh using `_refresh_access_token()`\r\n   - Retry the authorization check with the new token\r\n   - Provide clear error messages if refresh fails\r\n\r\n3. **Modified `_is_batch_authorized()` to**:\r\n   - Apply the same token refresh logic for batch authorization checks\r\n   - Ensure consistency across authorization methods\r\n\r\n## Testing\r\n- [x] Code follows Airflow coding standards\r\n- [x] Logic verified against OAuth2 refresh token specification\r\n- [ ] Manual testing with Keycloak (requires dedicated test environment)\r\n\r\n**Note:** I don't currently have a Keycloak test environment set up. The implementation follows the OAuth2 standard refresh token flow and matches the pattern used in similar auth providers. Happy to add tests or make adjustments based on reviewer feedback!\r\n\r\n## Impact\r\n- Users will remain logged in after token expiration\r\n- No more 500 errors after 5 minutes of inactivity\r\n- Seamless user experience with automatic token renewal\r\n\r\nCloses: #56614\r\n\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56623\nTitle: Fix AutoRefresh when only 1 dag run is running\nState: closed\nAuthor: pierrejeambrun\nLabels: area:UI, backport-to-v3-1-test\nBody:\nWe should do AutoRefresh as long as at least 1 DagRun is in the running state. \r\n\r\nAutoRefresh was not working when only 1 DagRun was running.",
  "Requirement ID: ISSUE-56622\nTitle: [v3-1-test] Upgrade zizmor and uv to latest versions (#56620)\nState: closed\nAuthor: potiuk\nLabels: area:dev-tools, backport-to-v3-1-test\nBody:\n(cherry picked from commit 80bf5405b9ccdd2e3f80121c562497218eab5e6e)\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56621\nTitle: [v3-1-test] Remove ``dagReports`` API endpoint (#56609)\nState: closed\nAuthor: potiuk\nLabels: area:API, area:UI\nBody:\nThe `/api/v2/dagReports` endpoint loaded user DAG files directly in the API server process via DagBag, violating Airflow's core architectural principle that the API server must never execute user code.\r\n\r\nThe endpoint was not used by the UI and had no known consumers. Users needing DAG loading reports should use the `airflow dags report` CLI command instead, which runs in an isolated process designed to safely execute user code.\r\n(cherry picked from commit 828aaa0b1d95caf90612a648867c17aec7e87874)\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56620\nTitle: Upgrade zizmor and uv to latest versions\nState: closed\nAuthor: potiuk\nLabels: area:dev-tools, backport-to-v3-1-test\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56619\nTitle: [v3-1-test] Complete `snowflake-snowpark-python` pip resolver hint (#\u2026\nState: closed\nAuthor: potiuk\nLabels: area:providers, provider:snowflake\nBody:\n\u202656606)\r\n\r\n* Complete `snowflake-snowpark-python` pip resolver hint\r\n\r\n* Update providers/snowflake/pyproject.toml\r\n\r\n* Update providers/snowflake/pyproject.toml\r\n\r\n---------\r\n(cherry picked from commit fe5d582)\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56618\nTitle: [v3-1-test] Upgrade to latest versions of prek, uv, golang\nState: closed\nAuthor: potiuk\nLabels: area:dev-tools, backport-to-v3-1-test\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56617\nTitle: Improve API sort documentation\nState: closed\nAuthor: pierrejeambrun\nLabels: area:API, area:UI, backport-to-v3-1-test\nBody:\nValues from the \"replace\" mapping are also valid for sorting. Update the documentation to reflect that.",
  "Requirement ID: ISSUE-56616\nTitle: Move timeout test to the SDK\nState: closed\nAuthor: ephraimbuddy\nLabels: area:task-sdk\nBody:\nThe timeout test doesn't belong to the core, it belongs to the SDK. This PR moves the test to the appropriate section in SDK.",
  "Requirement ID: ISSUE-56614\nTitle: Keycloak provider not refreshing auth token properly\nState: open\nAuthor: xartii\nLabels: kind:bug, area:providers, area:auth, needs-triage, provider:keycloak\nBody:\n### Apache Airflow Provider(s)\n\nkeycloak\n\n### Versions of Apache Airflow Providers\n\napache-airflow-providers-keycloak==0.1.0\n\n### Apache Airflow version\n\n3.1.0\n\n### Operating System\n\nMacOS 15.7.1\n\n### Deployment\n\nDocker-Compose\n\n### Deployment details\n\nDocker file used to build container image\n```\nARG AIRFLOW_VERSION=3.1.0\nFROM apache/airflow:${AIRFLOW_VERSION}\n\nRUN pip install --no-cache-dir \\\n    apache-airflow-providers-keycloak \\\n    psycopg2-binary\n```\nAirflow parameters set in docker compose (raw values are provided via the `.env` file)\n```\n    # Airflow Core DB Connection (uses Postgres service name)\n    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}\n    # Airflow Core Configuration\n    AIRFLOW__CORE__EXECUTOR: LocalExecutor\n    AIRFLOW__CORE__FERNET_KEY: ${_AIRFLOW_FERNET_KEY}\n    AIRFLOW__WEBSERVER__SECRET_KEY: ${_AIRFLOW_SECRET_KEY}\n    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'\n    AIRFLOW__WEBSERVER__RBAC: 'true'\n    # Keycloak Authorization Manager Configuration\n    AIRFLOW__CORE__AUTH_MANAGER: 'airflow.providers.keycloak.auth_manager.keycloak_auth_manager.KeycloakAuthManager'\n    AIRFLOW__KEYCLOAK_AUTH_MANAGER__CLIENT_ID: ${KC_CLIENT_ID}\n    AIRFLOW__KEYCLOAK_AUTH_MANAGER__CLIENT_SECRET: ${KC_CLIENT_SECRET}\n    AIRFLOW__KEYCLOAK_AUTH_MANAGER__REALM: ${KC_REALM}\n    # Point the Airflow containers to the Keycloak service name\n    AIRFLOW__KEYCLOAK_AUTH_MANAGER__SERVER_URL: 'http://10.10.0.141:8080'\n    # Required for Docker file permissions\n    AIRFLOW_UID: ${AIRFLOW_UID}\n    AIRFLOW_GID: ${AIRFLOW_GID}\n    # Authentication must be disabled locally, as it's delegated to Keycloak\n    _AIRFLOW_WWW_USER_USERNAME: ''\n    _AIRFLOW_WWW_USER_PASSWORD: ''\n```\n\n### What happened\n\nThe keycloak token lifetime is set to 5 minutes. I can log in properly to the Airflow, it requests correct permissions but after the token lifetime expires the UI starts to show `500 Internal server error`. What I see in the logs is the message with `Invalid bearer token`\n\nWhole stack trace from the logs\n```\nINFO:     192.168.65.1:39410 - \"GET /ui/dashboard/historical_metrics_data?start_date=2025-10-13T09%3A41%3A55.870Z HTTP/1.1\" 500 Internal Server Error\nERROR:    Exception in ASGI application\nTraceback (most recent call last):\n  File \"/home/airflow/.local/lib/python3.12/site-packages/uvicorn/protocols/http/httptools_impl.py\", line 409, in run_asgi\n    result = await app(  # type: ignore[func-returns-value]\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/airflow/.local/lib/python3.12/site-packages/fastapi/applications.py\", line 1082, in __call__\n    await super().__call__(scope, receive, send)\n  File \"/home/airflow/.local/lib/python3.12/site-packages/starlette/applications.py\", line 113, in __call__\n    await self.middleware_stack(scope, receive, send)\n  File \"/home/airflow/.local/lib/python3.12/site-packages/starlette/middleware/errors.py\", line 186, in __call__\n    raise exc\n  File \"/home/airflow/.local/lib/python3.12/site-packages/starlette/middleware/errors.py\", line 164, in __call__\n    await self.app(scope, receive, _send)\n  File \"/home/airflow/.local/lib/python3.12/site-packages/starlette/middleware/gzip.py\", line 29, in __call__\n    await responder(scope, receive, send)\n  File \"/home/airflow/.local/lib/python3.12/site-packages/starlette/middleware/gzip.py\", line 130, in __call__\n    await super().__call__(scope, receive, send)\n  File \"/home/airflow/.local/lib/python3.12/site-packages/starlette/middleware/gzip.py\", line 46, in __call__\n    await self.app(scope, receive, self.send_with_compression)\n  File \"/home/airflow/.local/lib/python3.12/site-packages/starlette/middleware/cors.py\", line 85, in __call__\n    await self.app(scope, receive, send)\n  File \"/home/airflow/.local/lib/python3.12/site-packages/starlette/middleware/exceptions.py\", line 63, in __call__\n    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n  File \"/home/airflow/.local/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    raise exc\n  File \"/home/airflow/.local/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    await app(scope, receive, sender)\n  File \"/home/airflow/.local/lib/python3.12/site-packages/starlette/routing.py\", line 716, in __call__\n    await self.middleware_stack(scope, receive, send)\n  File \"/home/airflow/.local/lib/python3.12/site-packages/starlette/routing.py\", line 736, in app\n    await route.handle(scope, receive, send)\n  File \"/home/airflow/.local/lib/python3.12/site-packages/starlette/routing.py\", line 290, in handle\n    await self.app(scope, receive, send)\n  File \"/home/airflow/.local/lib/python3.12/site-packages/starlette/routing.py\", line 78, in app\n    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n  File \"/home/airflow/.local/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    raise exc\n  File \"/home/airflow/.local/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    await app(scope, receive, sender)\n  File \"/home/airflow/.local/lib/python3.12/site-packages/starlette/routing.py\", line 75, in app\n    response = await f(request)\n               ^^^^^^^^^^^^^^^^\n  File \"/home/airflow/.local/lib/python3.12/site-packages/fastapi/routing.py\", line 298, in app\n    solved_result = await solve_dependencies(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/airflow/.local/lib/python3.12/site-packages/fastapi/dependencies/utils.py\", line 648, in solve_dependencies\n    solved = await run_in_threadpool(call, **solved_result.values)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/airflow/.local/lib/python3.12/site-packages/starlette/concurrency.py\", line 38, in run_in_threadpool\n    return await anyio.to_thread.run_sync(func)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/airflow/.local/lib/python3.12/site-packages/anyio/to_thread.py\", line 56, in run_sync\n    return await get_async_backend().run_sync_in_worker_thread(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/airflow/.local/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 2485, in run_sync_in_worker_thread\n    return await future\n           ^^^^^^^^^^^^\n  File \"/home/airflow/.local/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 976, in run\n    result = context.run(func, *args)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/airflow/.local/lib/python3.12/site-packages/airflow/api_fastapi/core_api/security.py\", line 125, in inner\n    _requires_access(\n  File \"/home/airflow/.local/lib/python3.12/site-packages/airflow/api_fastapi/core_api/security.py\", line 462, in _requires_access\n    if not is_authorized_callback():\n           ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/airflow/.local/lib/python3.12/site-packages/airflow/api_fastapi/core_api/security.py\", line 126, in <lambda>\n    is_authorized_callback=lambda: get_auth_manager().is_authorized_dag(\n                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/keycloak/auth_manager/keycloak_auth_manager.py\", line 152, in is_authorized_dag\n    return self._is_authorized(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/keycloak/auth_manager/keycloak_auth_manager.py\", line 307, in _is_authorized\n    raise AirflowException(f\"Unexpected error: {resp.status_code} - {resp.text}\")\nairflow.exceptions.AirflowException: Unexpected error: 401 - {\"error\":\"invalid_grant\",\"error_description\":\"Invalid bearer token\"}\n```\n\nIt looks like for some reasons Airflow is not using a refresh token to get a new authorization token\n\n### What you think should happen instead\n\nThe token should be renewed using refresh token mechanism\n\n### How to reproduce\n\nDeploy airflow with keycloak provider.\nConfigure keycloak with Standard flow, needed resources and simple policies that grant access to everything.\n\n### Anything else\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56612\nTitle: Bump uv to 0.9.2, prek to 0.2.5, golang to 1.25.3\nState: closed\nAuthor: amoghrajesh\nLabels: area:dev-tools, area:production-image, backport-to-v3-1-test\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\nFix failing main: https://github.com/apache/airflow/actions/runs/18484705518/job/52666113925\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56611\nTitle: [v3-1-test] Use name passed to `@asset` decorator when fetching the asset. (#56434)\nState: closed\nAuthor: github-actions[bot]\nLabels: area:task-sdk\nBody:\n(cherry picked from commit 1cbed3d0833d89ee83017caf0af90a8df17c1e5e)\n\nCo-authored-by: Karthikeyan Singaravelan <tir.karthi@gmail.com>",
  "Requirement ID: ISSUE-56610\nTitle: Docs: update CI/CD workflows documentation (Closes #41933)\nState: closed\nAuthor: KoviAnusha\nLabels: area:dev-tools, backport-to-v3-1-test\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #41933\r\nrelated: #41933\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\nExpand and clarify the content in dev/breeze/doc/ci/05_workflows.md to\r\ndescribe Airflow\u2019s composite workflow architecture, branch-specific\r\ntesting behavior, and workflow grouping in more detail.\r\n\r\nChanges include:\r\n- Added explanation of composite workflows and their purpose\r\n- Detailed test flow from build-info through finalization\r\n- Clarified branch-specific test coverage differences\r\n- Updated tables for workflow groups and individual jobs\r\n- Improved structure for readability and consistency\r\n\r\n<img width=\"460\" height=\"602\" alt=\"image\" src=\"https://github.com/user-attachments/assets/92d3a0fb-7700-41e9-af5c-c26b3bc7de39\" />\r\n\r\n<img width=\"476\" height=\"646\" alt=\"image\" src=\"https://github.com/user-attachments/assets/49b26978-7bc7-40c6-8caa-c7d0e4a22268\" />\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56609\nTitle: Remove ``dagReports`` API endpoint\nState: closed\nAuthor: kaxil\nLabels: area:API, area:UI, backport-to-v3-1-test\nBody:\nThe `/api/v2/dagReports` endpoint loaded user DAG files directly in the API server process via DagBag, violating Airflow's core architectural principle that the API server must never execute user code.\r\n\r\nThe endpoint was not used by the UI and had no known consumers. Users needing DAG loading reports should use the `airflow dags report` CLI command instead, which runs in an isolated process designed to safely execute user code.\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56608\nTitle: Enable PT011 rule to prvoider tests\nState: closed\nAuthor: xchwan\nLabels: provider:amazon, area:providers, provider:apache-hive\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\nissue: Enable Even More PyDocStyle Checks #40567\r\n@ferruzzi\r\nThis PR is for enable PT011 rule:\r\nPT011: all instances of pytest.raises should include a match term to make sure they are catching the right error.\r\nhttps://docs.astral.sh/ruff/rules/pytest-raises-too-broad/\r\nThere are 102 files changes is need.\r\nSo, I separate to many PR, which contains about 5 file changes for easy review.\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56607\nTitle: [v3-1-test] Fix retry callbacks not executing for externally killed tasks (#56586)\nState: closed\nAuthor: github-actions[bot]\nLabels: area:Scheduler\nBody:\n* Fix retry callbacks not executing for externally killed tasks\n\nWhen tasks with remaining retries were killed externally the\n`on_failure_callback` was incorrectly executed\ninstead of `on_retry_callback`.\n\nThe scheduler now correctly sets the callback type to `UP_FOR_RETRY` when\ntasks are eligible to retry, ensuring proper callback and email routing.\nFor heartbeat timeouts, the task is loaded before evaluating retry\neligibility to access the task's retry configuration.\n\nFixes #56196\n\n* fixup! Fix retry callbacks not executing for externally killed tasks\n(cherry picked from commit 04d3c440e3c741b0f2d9ac4d34d2f09f8b41a38b)\n\nCo-authored-by: Kaxil Naik <kaxilnaik@gmail.com>",
  "Requirement ID: ISSUE-56606\nTitle: Complete `snowflake-snowpark-python` pip resolver hint\nState: closed\nAuthor: notatallshaw\nLabels: area:providers, provider:snowflake\nBody:\nThis extendings the existing resolvelib hint to all of `snowflake-snowpark-python`, the resolver knot that pip gets into is now happening for snowflake-snowpark-python on every version of Python",
  "Requirement ID: ISSUE-56605\nTitle: Fix KeyError when accessing retry_delay on MappedOperator without explicit value\nState: closed\nAuthor: dheerajturaga\nLabels: \nBody:\nWhen querying the API endpoint for mapped tasks (e.g., /api/v2/dags/{dag_id}/tasks/{task_id}),\r\nthe server returned a 500 Internal Server Error if the mapped task didn't have an explicit\r\nretry_delay set. The retry_delay property was using direct dictionary access which raised\r\na KeyError during serialization.\r\n\r\nChanged to use .get() with the default value from SerializedBaseOperator (300 seconds)\r\n\r\n```\r\nINFO:     172.18.0.1:47834 - \"GET /api/v2/dags/example_dv_simulation_flow/tasks/run_simulation HTTP/1.1\" 500 Internal Server Error\r\nERROR:    Exception in ASGI application\r\n\r\nraise ResponseValidationError(\r\nfastapi.exceptions.ResponseValidationError: 1 validation errors:\r\n  {'type': 'get_attribute_error', 'loc': ('response', 'retry_delay'), 'msg': \"Error extracting attribute: KeyError: 'retry_delay'\", 'input': <SerializedMappedTask(_PythonDecoratedOperator): run_simulation>, 'ctx': {'error': \"KeyError: 'retry_delay'\"}}\r\n```\r\n\r\ncc: @kaxil , @vatsrahul1001",
  "Requirement ID: ISSUE-56604\nTitle: [v3-1-test] Add triggering_user_name to DagRunProtocol interface (#56193)\nState: open\nAuthor: github-actions[bot]\nLabels: area:API, area:task-sdk\nBody:\n* Add triggering_user_name to DagRunProtocol interface\n\n  Enable tasks running in isolated environments to access the username\n  that triggered a DAG run without requiring direct database access.\n\n  Changes:\n  - Add triggering_user_name field to DagRunProtocol interface\n  - Update task-sdk DagRun data model to include triggering_user_name\n  - Update execution API DagRun model to include triggering_user_name\n  - Ensure field flows from database through API to task context\n\n  This allows developers to identify DAG run triggers from within task\n  code using the existing DagRunProtocol interface.\n\n* Fix tests!\n\n* Add Cadwyn migration for triggering_user_name field in DagRunProtocol\n\n  Add backward compatibility migration for the new triggering_user_name field\n  added to the DagRun model in the Execution API. This ensures older API\n  clients continue to work seamlessly with newer API servers.\n\n* Add tests for older versions where triggering user is missing\n\n* Remove triggering user from v2025-04-28\n\n* Add a test with triggering_user_name set\n(cherry picked from commit 3120146f0a2720d186513282f9e8d807f635434f)\n\nCo-authored-by: Dheeraj Turaga <dheerajturaga@gmail.com>",
  "Requirement ID: ISSUE-56603\nTitle: [v3-1-test] chore: rename 0ne-stone to onestn for consistency (#56574)\nState: closed\nAuthor: github-actions[bot]\nLabels: area:dev-tools, backport-to-v3-1-test\nBody:\n(cherry picked from commit f65d012fa036bf55aeb0ce170bcd1121ada6289a)\n\nCo-authored-by: Wonseok Yang <deadline.yang@gmail.com>",
  "Requirement ID: ISSUE-56602\nTitle: Fix Connection or Variable access in Server context\nState: closed\nAuthor: kaxil\nLabels: area:secrets, area:task-sdk\nBody:\nHooks used in API server contexts (plugins, middlewares, log handlers) previously failed with `ImportError` for `SUPERVISOR_COMMS` because it only exists in worker Task execution contexts (Worker, Dag processor, Trigger). This prevented using hooks like GCSHook or S3Hook in plugins and broke log retrieval.\r\n\r\nImplemented automatic context detection using separate secrets backend chains for client and server processes:\r\n\r\n- Client contexts (workers, DAG processors, triggerers) are detected via `SUPERVISOR_COMMS` presence and use `ExecutionAPISecretsBackend` to route through the Execution API\r\n- Server contexts (API server, scheduler, plugins) are detected when `SUPERVISOR_COMMS` is unavailable and use `MetastoreBackend` for direct database access\r\n\r\nFixes #56120\r\nFixes #56583\r\n\r\nPS: I do not love the way we deal with secrets backends in airflow/configuration, even before this PR -- but not going to handle it in this PR.\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56601\nTitle: [v3-1-test] Update ASF logos in documentation to the new Oak logo (#56598)\nState: closed\nAuthor: github-actions[bot]\nLabels: kind:documentation\nBody:\n(cherry picked from commit 2eed93b3ea7f3084b8dc1906321a895d22e1dbd0)\n\nCo-authored-by: Jarek Potiuk <jarek@potiuk.com>",
  "Requirement ID: ISSUE-56600\nTitle: [v3-1-test] Remove some irrelevant TODOs in task sdk (#56506)\nState: closed\nAuthor: github-actions[bot]\nLabels: area:task-sdk\nBody:\n(cherry picked from commit 69536b8b40baf899025dfd3045c81b9d5efea8f2)\n\nCo-authored-by: Amogh Desai <amoghrajesh1999@gmail.com>",
  "Requirement ID: ISSUE-56599\nTitle: [v3-1-test] Fix: Correctly parse JSON for --dag_run_conf in airflow dags backfill CLI (#56380)\nState: closed\nAuthor: github-actions[bot]\nLabels: area:CLI\nBody:\n* feat: Parse and validate JSON for --dag-run-conf in backfill command\nAdd import json and update the argument parsing in create_backfill.\n\n* unit tests covering valid JSON, invalid JSON, and empty JSON cases\n(cherry picked from commit 6bcf3e8168e566373392fe9cf049ab58b8f5014a)\n\nCo-authored-by: Gary Hsu <hchsu2106@gmail.com>",
  "Requirement ID: ISSUE-56598\nTitle: Update ASF logos in documentation to the new Oak logo\nState: closed\nAuthor: potiuk\nLabels: kind:documentation, backport-to-v3-1-test\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56596\nTitle: Race Condition in KubernetesJobOperator with XCom\nState: open\nAuthor: pmcquighan-camus\nLabels: kind:bug, area:providers, provider:cncf-kubernetes, needs-triage\nBody:\n### Apache Airflow Provider(s)\n\ncncf-kubernetes\n\n### Versions of Apache Airflow Providers\n\napache-airflow-providers-cncf-kubernetes==10.7.0\n\n### Apache Airflow version\n\n3.0.6\n\n### Operating System\n\ndebian 12\n\n### Deployment\n\nOfficial Apache Airflow Helm Chart\n\n### Deployment details\n\nRunning on GKE , kubernetes version 1.33\n\n### What happened\n\nOccasionally, the KubernetesJobOperator  ends up creating both a Job object and a Pod object.  Kubernetes' controller-manager also creates a pod, so there are then 2 pods running for this airflow task.  One will have a prefix like `job-` created by the job from the KubernetesJobOperator, and the other will not.  This *could* be okay, except that if you have `do_xcom_push=True` on the job, then there is the XCom sidecar container running which will indefinitely until it is [terminated by airflow](https://github.com/apache/airflow/blob/ca9f3f0557bb93e63fc2403a4fd5c17d816f5f8f/providers/cncf/kubernetes/src/airflow/providers/cncf/kubernetes/utils/pod_manager.py#L973) by doing an `exec` and sending a signal to the main process.  So, when there are 2 pods that exist, the query to find the appropriate pod to exec into possibly choose the wrong pod, and the main job will run indefinitely (or until kubernetes kills it due to an `active_deadline_seconds`) or other airflow timeout and ultimately fail the task.  \n\n\nSide note, the operator [does not sleep in between retries](https://github.com/apache/airflow/blob/ca9f3f0557bb93e63fc2403a4fd5c17d816f5f8f/providers/cncf/kubernetes/src/airflow/providers/cncf/kubernetes/operators/job.py#L457-L462) when trying to find pods for the job, and does `len(pod_list) != self.parallelism`, instead of a `len(pod_list) > self.parallelism` (or something) since a kubernetes Job can have retries on Pods and so there might be 3 pods for a job with parallelism 1 (if the first 2 are `Failed`).\n\n### What you think should happen instead\n\nThe problem, I believe is that *if paralellism is None* the operator will call a method [get_or_create_pod](https://github.com/apache/airflow/blob/ca9f3f0557bb93e63fc2403a4fd5c17d816f5f8f/providers/cncf/kubernetes/src/airflow/providers/cncf/kubernetes/operators/job.py#L208-L217) which looks for an existing pod (by label).  However, the job controller in k8s might not have created a matching pod yet, and so this returns `None`, and a second pod gets created (not associated to the job).\n\nI think I have been able to work around this race condition by always setting `parallelism` flag on the Job, but this seems like unexpected behavior, and the job operator should never result in launching a `pod`, and should always launch a `job`.\n\n### How to reproduce\n\nUsing a simple DAG that just writes something to the XCom file and then tries to read it, if I trigger this 10 times it will typically fail about 3-4 of them (i.e. the task will run indefinitely until being killed by k8s)\n\n```python\nwith DAG(\n    dag_id=\"k8s-test\",\n    schedule=None,\n    catchup=False,\n    start_date=datetime(2025, 8, 1, 0, 0, 0, 0, timezone.utc),\n    max_active_runs=2,\n    default_args={\n        \"retries\": 2,\n        \"retry_delay\": timedelta(seconds=30),\n    },\n) as dag:\n    k8s_job = GKEStartJobOperator(\n        # Task config\n        task_id=\"k8s_output\",\n        cluster_name=\"xx\",\n        location=\"xx\",\n        deferrable=True,\n        poll_interval=30.0,\n        backoff_limit=3,  # Number of times pod will be retried (independent of task being retried)\n        do_xcom_push=True,  # Need to push the output file paths on to later stages\n        base_container_name=\"k8s-output\",\n        # Don't mark this stage as complete until the job is actually done\n        wait_until_job_complete=True,\n        # Job config - write out a file with a string-wrapped dictionary\n        arguments=[\n            \"python3\",\n            \"-c\",\n            \"\"\"from pathlib import Path\nimport time\nimport json\nPath(\"/airflow/xcom/return.json\").parent.mkdir(parents=True, exist_ok=True)\nwith open(\"/airflow/xcom/return.json\", \"w\") as f:\n  f.write(json.dumps({\"hello\":\"world\"}))\n\"\"\",\n        ],\n        image=\"python:3.12.10-alpine\",\n        namespace=\"default\",\n    )\n\n    parse_xcom = GKEStartJobOperator(\n        # Task config\n        task_id=\"parse_xcom\",\n        cluster_name=\"xx\",\n        location=\"xx\",\n        deferrable=True,\n        poll_interval=30.0,\n        backoff_limit=3,\n        base_container_name=\"parse-xcom\",\n        wait_until_job_complete=True,\n        arguments=[\n            \"echo\",\n            \"{{ ti.xcom_pull('k8s_output')\",\n        ],\n        image=\"python:3.12.10-alpine\",\n        namespace=\"default\",\n        ttl_seconds_after_finished=180,\n    )\n\n    _ = parse_xcom << k8s_job\n```\n\n### Anything else\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56593\nTitle: [Docs]updates the Spark base image\nState: open\nAuthor: kyungjunleeme\nLabels: area:providers, kind:documentation, provider:cncf-kubernetes\nBody:\n## \u2728 Summary\r\n\r\nThis PR updates the Spark base image reference used in the Airflow Spark-on-Kubernetes example to align with the latest officially maintained Apache Spark images and Python version requirements.\r\n\r\n---\r\n\r\n## \ud83e\udde9 Changes\r\n\r\n| Item | Old | New | Python Version |\r\n|------|------|------|----------------|\r\n| Base Image | `gcr.io/spark-operator/spark-py:v3.1.1` | `apache/spark-py:v3.4.0` | 3.10.6 |\r\n\r\n- The legacy image (gcr.io/spark-operator/spark-py:v3.1.1) was built on an outdated Spark and Python base, and attempts to pull it from /artifacts/tags/spark-operator/us/gcr.io/spark-py/v3.1.1 failed due to registry access issues during migration.\r\n- The updated image (`v3.4.0`) is based on **Ubuntu 22.04 (Jammy)** and ships with **Python 3.10.6**.  \r\n- This update ensures compatibility with the latest **`apache-airflow-providers-cncf-kubernetes`**, which now requires **Python >= 3.10**.  \r\n\r\n> Verified with:\r\n> ```bash\r\n> curl -s https://pypi.org/pypi/apache-airflow-providers-cncf-kubernetes/json \\\r\n>   | jq -r '.info.requires_python'\r\n> # >=3.10\r\n> ```\r\n\r\n---\r\n\r\n## \ud83e\udde0 Rationale\r\n\r\n- The older `gcr.io/spark-operator/spark-py:v3.1.1` image (Python 3.8.x) is deprecated and no longer maintained on GCR.  \r\n- The official Apache image (`apache/spark-py`) on Docker Hub is now the actively maintained and recommended base for Spark 3.4+.  \r\n- Verified the runtime environments manually:\r\n\r\n```bash\r\ndocker run --rm apache/spark-py:v3.2.4 python3 -V\r\n# Python 3.9.2\r\n\r\ndocker run --rm apache/spark-py:v3.4.0 python3 -V\r\n# Python 3.10.6\r\n```\r\n\r\n<img width=\"1346\" height=\"1878\" alt=\"image\" src=\"https://github.com/user-attachments/assets/678d9868-b63b-4aa5-952e-b321ef1d0f1f\" />\r\n\r\n\r\n<img width=\"716\" height=\"979\" alt=\"Screenshot 2025-10-14 at 12 05 35\u202fAM\" src=\"https://github.com/user-attachments/assets/1f0856ee-de59-410e-9a24-cb62fe091d74\" />\r\n\r\n<img width=\"3152\" height=\"132\" alt=\"image\" src=\"https://github.com/user-attachments/assets/63f104dc-107b-49fe-a2dc-d03967ef4ef3\" />\r\n\r\n\r\nhttps://hub.docker.com/r/apache/spark-py/tags\r\n\r\n`I checked all image tag`\r\n\r\n<img width=\"1336\" height=\"1576\" alt=\"image\" src=\"https://github.com/user-attachments/assets/970b6fc7-af36-4d1d-b45f-2ecbef21d7c4\" />\r\n\r\n<img width=\"1430\" height=\"1658\" alt=\"image\" src=\"https://github.com/user-attachments/assets/fb3ba801-7c0e-46b5-ad7d-50fb38775c20\" />\r\n\r\n\r\n\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56592\nTitle: [v3-1-test] Added missing babel dependency in Task SDK (#56553)\nState: open\nAuthor: github-actions[bot]\nLabels: area:task-sdk\nBody:\nAfter the [move of macros to the Task SDK](https://github.com/apache/airflow/pull/46867/files#diff-854d19db18bae58289f4ce996ca0fb34341bc0f22930620627afccbd9d6facfcL30), the babel dependency is missing.  This is probably due to the fact that in the past, the babel dependency was available through the flask-babel dependency.  This PR add the babel dependency to the Task SDK so that macro's depending on it can work again.\n(cherry picked from commit f96bb22d684d2d72ec9acb48a9e8b666d89f47a1)\n\nCo-authored-by: David Blain <info@dabla.be>\ncloses: https://github.com/apache/airflow/issues/56552",
  "Requirement ID: ISSUE-56591\nTitle: Add support for multiple Celery worker groups with queue-specific configurations\nState: open\nAuthor: HsiuChuanHsu\nLabels: kind:feature, needs-triage\nBody:\n### Description\n\nWe are using Apache Airflow with the Celery Executor and need to configure the system to allow tasks in different queues to be executed on specific nodes. \n> The goal is to achieve fine-grained control over task distribution by assigning queues to designated worker nodes. \n\n### Use case/motivation\n\nAssign each queue to run on specific worker nodes to optimize resource allocation and task execution.\n1. Celery queue groups for different queues and node assignments\n2. Each worker group creates a separate Deployment assigned to specific nodes and queues\n\n**Use case**:\n1. **Node-specific scheduling**: Route certain queues to specialized node pools (GPU, high-memory, etc.)\n2. **High-priority vs. low-priority tasks**: Allocate more resources to critical queues\n3. **Resource-intensive workloads**: Dedicate workers with higher memory/CPU to specific queues\n\n### Related issues\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [x] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56589\nTitle: Add support for multiple Celery worker groups with queue-specific configurations\nState: open\nAuthor: HsiuChuanHsu\nLabels: area:helm-chart\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n## Summary\r\nThis PR adds support for multiple Celery worker groups. Each group can now be independently configured to target specific queues and use distinct resource settings\r\n> The goal is to achieve **fine-grained** control over task distribution by assigning queues to specific worker nodes. \r\n\r\n## Changes\r\n**New Configuration**: `workers.celeryQueueGroups`\r\nAdded support for defining multiple worker deployments through the `workers.celeryQueueGroups` configuration. Each group can specify:\r\n- `name`: (required) Unique name for the worker group\r\n- `queues`: (required) Comma-separated list of Celery queues\r\n- `replicas`: (required) Number of worker replicas\r\n- `nodeSelector`: (optional)\r\n- `affinity`: (optional)\r\n- `tolerations`: (optional)\r\n- `topologySpreadConstraints`: (optional)\r\n- `resources`: (optional) \r\n- `labels`: (optional)\r\n- `podAnnotations`: (optional)\r\n- `priorityClassName`: (optional)\r\n- `env`: (optional)\r\n\r\n**`worker-deployment.yaml` Logic**\r\n- Modified the worker deployment template to loop through all configured worker groups\r\n- For default behavior (no `celeryQueueGroups` defined), the template maintains original setup using existing `workers.* configuration`\r\n- Each worker group creates a separate Deployment/StatefulSet with:\r\n  - Unique name suffix based on group name (e.g., airflow-worker-high-priority)\r\n  -  **worker-group** label for identification\r\n  - **Queue-specific command arguments**: `exec airflow celery worker --queues <group-queues>`\r\n  -  Custom resource configurations override default settings per group\r\n\r\n**`worker-service.yaml` Logic**\r\n- Extended service template to create separate services for each worker group\r\n- Each service is scoped to its corresponding worker group using the `worker-group` label selector\r\n- Service naming follows the pattern: `{{ airflow.fullname }}-worker-<group-name>`\r\n\r\ncloses: #56591\r\n\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56588\nTitle: Fix Advanced Search button overlap in DAG List View\nState: open\nAuthor: jakuborlowski\nLabels: area:UI\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\nThis PR fixes a layout issue in the DAG List View where the Advanced Search button overlaps the Clear Search button when the label text is longer (e.g., in certain translations).\r\nThe fix ensures that both buttons remain visible and properly spaced regardless of language or text length.\r\n\r\nChanges:\r\n\r\n- Adjusted button to handle variable text width.\r\n- Ensured responsive spacing between Advanced Search and Clear Search buttons.\r\n- Verified UI in multiple languages.\r\n\r\n<img width=\"1512\" height=\"731\" alt=\"polish\" src=\"https://github.com/user-attachments/assets/c8f4293d-7469-4537-80df-8b74aaf5706a\" />\r\n\r\n<img width=\"1463\" height=\"545\" alt=\"polish-fixed\" src=\"https://github.com/user-attachments/assets/40a2eec8-5b8c-4199-9188-248f97d9a020\" />\r\n\r\ncloses: #56587\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56587\nTitle: \"Advanced Search\" button overlaps \"Clear Search\" button in DAG List View when UI language is set to Polish\nState: open\nAuthor: jakuborlowski\nLabels: kind:bug, area:core, area:UI, needs-triage\nBody:\n### Apache Airflow version\n\nmain (development)\n\n### If \"Other Airflow 2/3 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\nWhen switching the Airflow UI language to Polish, the label for the \u201cAdvanced Search\u201d button (\u201cWyszukiwanie zaawansowane\u201d) becomes too long and overlaps the \u201cClear Search\u201d (X) button in the DAG List View.\nAs a result, the clear button becomes partially or completely inaccessible, making it difficult to reset the search query.\n\nSee attached screenshots for reference.\n\n<img width=\"1512\" height=\"730\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/db86586a-59e7-4e29-8767-0e7008c3b1c7\" />\n<img width=\"1512\" height=\"731\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/e460f1b6-2e75-4813-8113-12abe7f7902a\" />\n\n### What you think should happen instead?\n\nThe \u201cAdvanced Search\u201d button and the \u201cClear Search\u201d button should be properly aligned and visible.\nThe layout should automatically adapt to longer text strings in translated UI labels.\n\n### How to reproduce\n\n1. Open DAG List View in the Airflow UI.\n2. Go to User \u2192 Select Language \u2192 Polish.\n3. Type any text into the search bar.\n\n### Operating System\n\nmacOS 15.6.1\n\n### Versions of Apache Airflow Providers\n\n_No response_\n\n### Deployment\n\nDocker-Compose\n\n### Deployment details\n\n_No response_\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [x] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56586\nTitle: Fix retry callbacks not executing for externally killed tasks\nState: closed\nAuthor: kaxil\nLabels: area:Scheduler, backport-to-v3-1-test\nBody:\nWhen tasks with remaining retries were killed externally the `on_failure_callback` was incorrectly executed\r\ninstead of `on_retry_callback`.\r\n\r\nThe scheduler now correctly sets the callback type to `UP_FOR_RETRY` when tasks are eligible to retry, ensuring proper callback and email routing. For heartbeat timeouts, the task is loaded before evaluating retry eligibility to access the task's retry configuration.\r\n\r\nFixes #56196\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56584\nTitle: Fixed incorrect path in EMR notebook waiter\nState: closed\nAuthor: ellisms\nLabels: provider:amazon, area:providers\nBody:\n---\r\nFixed incorrect status path in EMR notebook waiter.\r\nCloses: #50587",
  "Requirement ID: ISSUE-56583\nTitle: Plugins cannot import 'SUPERVISOR_COMMS'\nState: closed\nAuthor: pierrejeambrun\nLabels: kind:bug, area:plugins, AIP-84, area:task-execution-interface-aip72\nBody:\nServer side plugins (fastapi apps, middlewares) do not have any supervisor_comms context setup. Any attempt to use sdk calls will fail, for instance retrieving a connection, which is problematic because by ricochet it also prevents re-using operators hooks in plugins.\n\n### Committer\n\n- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",
  "Requirement ID: ISSUE-56582\nTitle: Move natsort dependency to airflow-core\nState: closed\nAuthor: potiuk\nLabels: backport-to-v3-1-test\nBody:\nThe #53782 added natsort dependency to main pyproject.toml, where it should be in airflow-core.\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56581\nTitle: [v3-1-test] Fix migration errors for Pydantic 2.12.0 (#56579)\nState: closed\nAuthor: github-actions[bot]\nLabels: area:dev-tools, backport-to-v3-1-test\nBody:\nPydantic 2.12.0 implemented experimental Sentinel that requires\nnewer version of typing extensions. Our migration scripts however\ndowngrade airflow to 2.11.0 and since airflow 2.11 does not have\npydantic specified as required dependency, it does not downgrade\nit - but it downgrades typing-extensions that are airflow dependency.\n\nThis causes a mismatch between expected version of typing extensions\nby Pydantic (4.14.1) and the one that we have installed in airflow\n2.11 (4.13.1). However - in fact - pydantic is a dependency of\nAirflow 2.11 - becuase serialization uses pydantic serializer\nin 2.11 and it fails being imported if typing extensions is too low.\n\nThis is only a problem when downgrading to Airflow 2.11 with constraints\nwhen you do not specify pydantic as extra. This should be fixed in\n2.11.1 as there constraints should include latest version of\ntyping-extension and pydantic.\n\nFor now - the fix is to add pydantic as extra when downgrading\nairflow to 2.11.0\n(cherry picked from commit dfb24d74b968d9362d68db5c46b2e789584480fb)\n\nCo-authored-by: Jarek Potiuk <jarek@potiuk.com>",
  "Requirement ID: ISSUE-56579\nTitle: Fix migration errors for Pydantic 2.12.0\nState: closed\nAuthor: potiuk\nLabels: area:dev-tools, full tests needed, backport-to-v3-1-test\nBody:\nPydantic 2.12.0 implemented experimental Sentinel that requires newer version of typing extensions. Our migration scripts however downgrade airflow to 2.11.0 and since airflow 2.11 does not have pydantic specified as required dependency, it does not downgrade it - but it downgrades typing-extensions that are airflow dependency.\r\n\r\nThis causes a mismatch between expected version of typing extensions by Pydantic (4.14.1) and the one that we have installed in airflow 2.11 (4.13.1). However - in fact - pydantic is a dependency of Airflow 2.11 - becuase serialization uses pydantic serializer in 2.11 and it fails being imported if typing extensions is too low.\r\n\r\nThis is only a problem when downgrading to Airflow 2.11 with constraints when you do not specify pydantic as extra. This should be fixed in 2.11.1 as there constraints should include latest version of typing-extension and pydantic.\r\n\r\nFor now - the fix is to add pydantic as extra when downgrading airflow to 2.11.0\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56578\nTitle: Enable PT011 rule to provider tests\nState: open\nAuthor: xchwan\nLabels: area:providers, provider:cncf-kubernetes, provider:asana, provider:apache-spark, provider:apache-livy\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\nissue: Enable Even More PyDocStyle Checks #40567\r\n@ferruzzi\r\nThis PR is for enable PT011 rule:\r\nPT011: all instances of pytest.raises should include a match term to make sure they are catching the right error.\r\nhttps://docs.astral.sh/ruff/rules/pytest-raises-too-broad/\r\nThere are 102 files changes is need.\r\nSo, I separate to many PR, which contains about 5 file changes for easy review.\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56577\nTitle: [v3-1-test] Upgrade important dependencies for 3-1 branch\nState: closed\nAuthor: potiuk\nLabels: area:dev-tools, kind:documentation, area:production-image, backport-to-v3-1-test\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56576\nTitle: [v3-1-test] Fix typo in contributing-docs/05_pull_requests.rst (#56573)\nState: closed\nAuthor: github-actions[bot]\nLabels: area:dev-tools, backport-to-v3-1-test\nBody:\n(cherry picked from commit db4b534a29abe5a990d776b311f0dcb80631d65b)\n\nCo-authored-by: Jakub Or\u0142owski <jakub.orlowski@gmail.com>",
  "Requirement ID: ISSUE-56575\nTitle: Grid View with grid_view_sorting_order=topological does not reflect true topological order\nState: open\nAuthor: rebel-wansoo\nLabels: kind:bug, area:core, area:UI, needs-triage\nBody:\n### Apache Airflow version\n\n3.1.0\n\n### If \"Other Airflow 2/3 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\nWebserver setting: AIRFLOW__WEBSERVER__GRID_VIEW_SORTING_ORDER=topological\nDAG structure: multi-level DAG using @task_group\n\nEven when the configuration option grid_view_sorting_order=topological is set, the vertical order of tasks and task groups in Grid View does not match the actual topological (execution) order of the DAG.\n\nThe Graph View and execution dependencies are correct, but the Grid View\u2019s left-hand task list appears to be sorted alphabetically by task_id / group_id.\n\nExample DAG structure:\n\nstart >> step_a >> step_b >> finish\n\n\nExpected topological order (execution order):\n\nstart \u2192 step_a \u2192 step_b \u2192 finish\n\n\nActual order shown in Grid View:\n\nfinish\nstart\nstep_a\nstep_b\n\nExpected Behavior\n\nWhen grid_view_sorting_order=topological is used,\nthe vertical task order in Grid View should match the DAG\u2019s topological execution order.\n\nExpected display:\n\nstart\nstep_a\nstep_b\nfinish\n\n\nGrid View appears to sort sibling tasks/groups alphabetically rather than following topological order.\nGraph View and runtime behavior are correct; only the Grid View ordering is inconsistent.\n\n### What you think should happen instead?\n\n_No response_\n\n### How to reproduce\n\nCreate a simple DAG:\n\nstart >> step_a >> step_b >> finish\n\n\nSet environment variable:\n\nAIRFLOW__WEBSERVER__GRID_VIEW_SORTING_ORDER=topological\n\n\nRun the DAG and open Grid View.\n\nObserve that the left-hand task list does not follow topological order.\n\n### Operating System\n\nUbuntu 22.04.5 LTS\n\n### Versions of Apache Airflow Providers\n\n_No response_\n\n### Deployment\n\nOfficial Apache Airflow Helm Chart\n\n### Deployment details\n\n_No response_\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56574\nTitle: chore: rename 0ne-stone to onestn for consistency\nState: closed\nAuthor: onestn\nLabels: area:dev-tools, backport-to-v3-1-test\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56573\nTitle: Fix typo in contributing-docs/05_pull_requests.rst\nState: closed\nAuthor: jakuborlowski\nLabels: area:dev-tools, backport-to-v3-1-test\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\nThis pull request corrects a small typo in the contributor documentation (`contributing-docs/05_pull_requests.rst`).\r\nNo content or structural changes were made \u2014 only a minor spelling correction.\r\n\r\nThis change was made while following the steps in [Your First Airflow Pull Request \u2014 5-Minute Guide](https://github.com/apache/airflow/blob/main/contributing-docs/03a_contributors_quick_start_beginners.rst)\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56572\nTitle: Contributors quickstart: fix RST style\nState: closed\nAuthor: ecodina\nLabels: area:dev-tools, backport-to-v3-1-test\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\nI was looking at the new contributors quick start guide and I saw that option B was following MD style instead of RST, so it was not rendering correctly on Github.\r\n\r\nThis PR modifies the style so that is RST compliant.\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56571\nTitle: Instability at large scales - workers get ReadTimeout when calling api-server\nState: open\nAuthor: ron-gaist\nLabels: kind:bug, area:performance, kind:feature, area:API, area:core, needs-triage, provider:celery\nBody:\n### Apache Airflow version\n\n3.1.0\n\n### If \"Other Airflow 2/3 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\nOur large scale setup includes\n* ~1000 celery executor workers,\n* 15 api servers - 64 worker processes each (with enough resources - having checked utilization)\n\nAlso, maybe relevantly,\n* 6 scheduler replicas\n* 2 dag processors\n* a pgbouncer with a large enough `airflow` connection pool size (doesn't reach maximum)\n* dags with up to 8k tasks (in parallel) and a final task that depends on all of them.\n   usually dags are smaller than that, average ~5k tasks\n\nWhen all workers are active and working on task instances - they all get the following warning 4 times\n**[warning] Starting call to 'airflow.sdk.api.client.Client.request', this is the %d time calling it. [airflow.sdk.api.client]**\nand, the 5th time - they get this error:\n**[error] Task execute_workload[$celery_task_uuid] raise unexpected: ReadTimeout('timed out') [celery.app.trace]**\n\nWe investigated this error a little and found that the error comes from the httpx default timeout\nfrom httpx docs ('https://www.python-httpx.org/advanced/timeouts/'):\n```\nHTTPX is careful to enforce timeouts everywhere by default.\nThe default behavior is to raise a TimeoutException after 5 seconds of network inactivity.\n```\n\n### What you think should happen instead?\n\nAirflow should allow users to configure the timeout via `airflow.cfg` to accommodate users with high-load systems.\nFor example:\n```\n[api]\nHTTPX_TIMEOUT = # 5 by default\n```\nAlso - maybe add a section to the docs detailing best practices when working with very high loads to make the api server reliable.\n\n\n### How to reproduce\n\n(1) Run airflow in a kubernetes cluster with:\n~ 1k celery workers\n~ 15 api server replicas (64 worker processes. resource limits: 25Gi RAM. 8 CPU cores)\n\n(2) Have large dags so that all 1k workers do tasks in parallel (each task should take more than 5 mins)\n\n(3) Observe workers for errors (ReadTimeout)\n\n### Operating System\n\nDebian GNU/Linux 12 (bookworm)\n\n### Versions of Apache Airflow Providers\n\napache-airflow-providers-celery==3.12.2\napache-airflow-providers-common-compat==1.7.3\napache-airflow-providers-common-io==1.6.2\napache-airflow-providers-common-sql=1.27.5\napache-airflow-providers-standard==1.6.0\napache-airflow-providers-postgres==6.2.3\n\n### Deployment\n\nOfficial Apache Airflow Helm Chart\n\n### Deployment details\n\n_No response_\n\n### Anything else?\n\nProblem occurs everytime that all workers are executing a task instance (the highest load)\nlogs:\n```\n[warning] Starting call to 'airflow.sdk.api.client.Client.request', this is the 1st time calling it. [airflow.sdk.api.client]\n[warning] Starting call to 'airflow.sdk.api.client.Client.request', this is the 2nd time calling it. [airflow.sdk.api.client]\n[warning] Starting call to 'airflow.sdk.api.client.Client.request', this is the 3rd time calling it. [airflow.sdk.api.client]\n[warning] Starting call to 'airflow.sdk.api.client.Client.request', this is the 4th time calling it. [airflow.sdk.api.client]\n[error] Task execute_workload[a7469ad-3481-4fd4-b8f236b37cf1] raise unexpected: ReadTimeout('timed out') [celery.app.trace]\n```\n\n### Are you willing to submit PR?\n\n- [x] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56570\nTitle: Fix: Adjust PanelButtons spacing and alignment\nState: open\nAuthor: anshuksi282-ksolves\nLabels: area:UI\nBody:\n## Fix mis-aligned padding for Graph View options buttons\r\n\r\n**Issue** #56076\r\n\r\n**Description:**  \r\nThis PR fixes the mis-aligned padding for the Graph View options buttons in the Airflow Web UI (3.1.0rc2).  \r\n\r\n**Background:**  \r\nThe buttons on the upper-right (and slightly on the upper-left) of the Graph View were overlapping with the splitter due to inconsistent padding. \r\nAlthough this issue was previously assigned to another contributor @alisha-malik, no PR has been submitted in the past 2 weeks. I started working on it now to resolve the open UI bug.  \r\n\r\n**What this PR does:**  \r\n- Corrects the padding for Graph View option buttons to ensure consistent alignment on all sides.  \r\n- Prevents the buttons from overlapping with the splitter in both left and right positions.  \r\n\r\n**Notes:**  \r\n- Minor UI fix; does not affect core functionality.  \r\n- Before\r\n<img width=\"1084\" height=\"140\" alt=\"Screenshot from 2025-10-12 18-27-14\" src=\"https://github.com/user-attachments/assets/f0c31a00-d6c4-4e03-a20f-0256f5d54950\" />\r\n\r\n- After\r\n<img width=\"978\" height=\"140\" alt=\"Screenshot from 2025-10-12 19-03-18\" src=\"https://github.com/user-attachments/assets/150d7d47-06f9-4595-9bad-f4df71f389ca\" />\r\n<img width=\"1084\" height=\"140\" alt=\"Screenshot from 2025-10-12 19-03-00\" src=\"https://github.com/user-attachments/assets/acff7035-ebc0-49c3-bdd0-ad7b641eac40\" />\r\n\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56569\nTitle: [v3-1-test] Fix broken main after pydantic 2.12.0 - partly cleanup (#56514)\nState: closed\nAuthor: github-actions[bot]\nLabels: area:task-sdk, area:airflow-ctl\nBody:\n* Fix broken main after pydantic 2.12.0 - partly cleanup\n\n* Fix broken main after pydantic 2.12.0 - partly cleanup, align task SDK as well\n\n* Regenerate generated code\n(cherry picked from commit bfbd90181a03f16f00da4e530803c69bd83ed274)\n\nCo-authored-by: Jens Scheffler <95105677+jscheffl@users.noreply.github.com>",
  "Requirement ID: ISSUE-56568\nTitle: Bugfix/remove airflow utils deprecations in edge\nState: closed\nAuthor: jscheffl\nLabels: area:providers, area:dev-tools, provider:edge\nBody:\nRemove some deprecations in Edge from movement in Airflow core to Task SDK.",
  "Requirement ID: ISSUE-56567\nTitle: fix: prevent FK violation when cleaning dag_version table\nState: open\nAuthor: HsiuChuanHsu\nLabels: \nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n\r\n## Problem\r\nWhen running `airflow db clean` with a timestamp, the command fails with a ForeignKeyViolation error when attempting to delete old dag_version records.\r\n```bash\r\nChecking table dag_version\r\nFound 1 rows meeting deletion criteria.\r\nPerforming Delete...\r\nMoving data to table _airflow_deleted__dag_version__20251011083345\r\nTraceback (most recent call last):\r\n  File \"/usr/python/lib/python3.12/site-packages/sqlalchemy/engine/base.py\", line 1910, in _execute_context\r\n    self.dialect.do_execute(\r\n  File \"/usr/python/lib/python3.12/site-packages/sqlalchemy/engine/default.py\", line 736, in do_execute\r\n    cursor.execute(statement, parameters)\r\npsycopg2.errors.ForeignKeyViolation: update or delete on table \"dag_version\" violates foreign key constraint \"task_instance_dag_version_id_fkey\" on table \"task_instance\"\r\nDETAIL:  Key (id)=(018c5e5e-7a8b-7000-8000-000000000001) is still referenced from table \"task_instance\".\r\n```\r\n\r\n**Why this happened?** Reference to [reproduction](https://github.com/apache/airflow/issues/56192#issuecomment-3393068953). \r\n\r\nWe executed the Airflow database cleanup command:\r\n```bash\r\nairflow db clean --clean-before-timestamp \"2025-09-21T10:30\" --skip-archive --yes\r\n```\r\n\r\n**Cleanup Conditions Check (Clean before 2025-09-21):**\r\n- dag_version (2025-09-01) **meets** the cleanup condition (before 2025-09-21).\r\n- task_instance-1 (2025-09-06) **meets** the cleanup condition (before 2025-09-21).\r\n- task_instance-2 (2025-10-01) **does not meet** the cleanup condition (after 2025-09-21).\r\n\r\n**Resulting Issue:**\r\n\r\nThe attempt to delete the qualifying `dag_version` failed. This is because the newer `task_instance-2` record is still referencing it, which triggered a `RESTRICT` violation.\r\n\r\n\r\n## Fix \r\n**Refined Cleanup Logic:**\r\n- Before removing a `dag_version`, we must check for any remaining references.\r\n- If references exist, the `dag_version` will be skipped.\r\n\r\n**Chanage** \r\n- Add a `skip_if_referenced_by` parameter to _TableConfig to exclude records that are\r\nstill referenced by other tables during cleanup.\r\n- Added `_build_skip_if_referenced_filter()` function to generate exclusion conditions\r\n\r\n\r\ncloses: #56192\r\n\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56566\nTitle: Airflow installlation errors for grpcio\nState: closed\nAuthor: Srabasti\nLabels: kind:bug, kind:documentation, area:dependencies, needs-triage\nBody:\n### What do you see as an issue?\n\nOn attempting to install Airflow 3.1 on Windows 11 Pro using documentation link below:\nhttps://airflow.apache.org/docs/apache-airflow/stable/installation/installing-from-pypi.html\n\nby running command:\npip install \"apache-airflow[celery]==3.1.0\" --constraint \"https://raw.githubusercontent.com/apache/airflow/constraints-3.1.0/constraints-3.10.txt\"\n\nam getting error as below:\n          raise DistutilsPlatformError(\n          ...<3 lines>...\n          )\n      distutils.errors.DistutilsPlatformError: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n\n      [end of output]\n\n  note: This error originates from a subprocess, and is likely not a problem with pip.\n  ERROR: Failed building wheel for grpcio\n  Building wheel for sqlalchemy (pyproject.toml) ... done\n  Created wheel for sqlalchemy: filename=sqlalchemy-1.4.54-cp313-cp313-win_amd64.whl size=1557723 sha256=a8b746cccbc74ffc1f871ca061f5a0307f9573a9eb77081ee6b894cf83cad70c\n  Stored in directory: c:\\users\\sraba\\appdata\\local\\pip\\cache\\wheels\\66\\d4\\be\\4c07d30f4e85a3d3789094f906c6050425b0183674c46548d0\nSuccessfully built dill sqlalchemy\nFailed to build grpcio\nerror: failed-wheel-build-for-install\n\n\u00d7 Failed to build installable wheels for some pyproject.toml based projects\n\u2570\u2500> grpcio\n\nLog attached in \n\n[Airflow_Installation_1sttry_grpcio_error_10112025.txt](https://github.com/user-attachments/files/22869461/Airflow_Installation_1sttry_grpcio_error_10112025.txt)\n\nAs a next step, I installed vs_BuildTools and \"Desktop development with C++\", using link below:\nhttps://visualstudio.microsoft.com/visual-cpp-build-tools/\n\nPost installation, confirmed that installation is complete as per screenshot below.\n\n<img width=\"1266\" height=\"686\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/40b1d986-50fd-4d96-813e-b5befaf56058\" />\n\nAfter rerunning command below:\nby running command as below:\npip install \"apache-airflow[celery]==3.1.0\" --constraint \"https://raw.githubusercontent.com/apache/airflow/constraints-3.1.0/constraints-3.10.txt\"\nam getting error as below:\n          raise CompileError(msg)\n      distutils.compilers.C.errors.CompileError: command 'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio\\\\2022\\\\BuildTools\\\\VC\\\\Tools\\\\MSVC\\\\14.44.35207\\\\bin\\\\HostX86\\\\x64\\\\cl.exe' failed with exit code 1\n\n      [end of output]\n\n  note: This error originates from a subprocess, and is likely not a problem with pip.\n  ERROR: Failed building wheel for grpcio\nFailed to build grpcio\nerror: failed-wheel-build-for-install\n\n\u00d7 Failed to build installable wheels for some pyproject.toml based projects\n\u2570\u2500> grpcio\n\n### Solving the problem\n\nAppreciate any inputs to overcome this error, in case someone already encountered. \nOnce the fix is found, I can submit PR for documentation bug fix.\n\n### Anything else\n\nLogs already included above. \nI looked up for grpcio errors in StackOverflow using link below for any specific errors, however not found anything useful yet.\nhttps://stackoverflow.com/search?q=%5Bairflow%5Dgrpcio&s=aa4332d0-832e-4e83-aaca-dff88da1a175&newreg=b4bd086ba056410fac32007cb5016671\n\n### Are you willing to submit PR?\n\n- [x] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56565\nTitle: Optimize grid structure query with DISTINCT for dag_version_id lookup\nState: closed\nAuthor: cjonesy\nLabels: area:API\nBody:\nThe grid structure endpoint was scanning hundreds or thousands of TaskInstance rows when only a few unique dag_version_id values were needed. With my local tests using mapped tasks, this caused 30+ second load times.\r\n\r\nAdding .distinct() reduces the subquery results from potentially 1000s of rows to just 1-5 unique dag_version_ids, dramatically improving performance for DAGs with mapped tasks.\r\n\r\nThe results from my test using Postgres locally with and without `distinct()` are below:\r\n\r\n```\r\nWITH distinct():\r\n  Run 1: 37.71ms (3 unique versions)\r\n  Run 2: 34.62ms (3 unique versions)\r\n  Run 3: 35.82ms (3 unique versions)\r\n\r\nWITHOUT distinct():\r\n  Run 1: 208.54ms (20000 rows)\r\n  Run 2: 159.77ms (20000 rows)\r\n  Run 3: 189.30ms (20000 rows)\r\n```\r\n\r\ncloses #56554 \r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56564\nTitle: Update FastAPI Deprecations from HTTP error code 422 handling\nState: open\nAuthor: jscheffl\nLabels: area:API, backport-to-v3-1-test\nBody:\nFix deprecation warning:\r\n`airflow-core/src/airflow/api_fastapi/execution_api/routes/__init__.py:23 DeprecationWarning: 'HTTP_422_UNPROCESSABLE_ENTITY' is deprecated. Use 'HTTP_422_UNPROCESSABLE_CONTENT' instead.`",
  "Requirement ID: ISSUE-56563\nTitle: i have created an solution for this issue\nState: closed\nAuthor: ibrahim307le\nLabels: area:Scheduler, kind:documentation, area:UI\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56562\nTitle: airflow goes to up for retry with random id '611b2....' after it runs for 5min\nState: open\nAuthor: unrealrt3001\nLabels: kind:bug, area:core, needs-triage\nBody:\n### Apache Airflow version\n\nOther Airflow 2/3 version (please specify below)\n\n### If \"Other Airflow 2/3 version\" selected, which one?\n\n2.7.1\n\n### What happened?\n\nairflow goes to up for retry with random if '611b2....' when I clear the task to run. I am using airflow docker image, I had thought it was postgres connection overload issue, so I used pgbouncer for managing the connection, airflow ran for 3-4 days and again started giving this error of retry. \n\n# Feel free to modify this file to suit your needs.\n---\n\nx-airflow-common:\n\n  &airflow-common\n  image: airflow310v1_fastexcel:latest\n  environment:\n  &airflow-common-env\n    AIRFLOW__CORE__EXECUTOR: CeleryExecutor\n \n    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@pgbouncer/airflow\n    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@pgbouncer/airflow\n\n    AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0\n    AIRFLOW__CORE__FERNET_KEY: ''\n    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'\n    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'\n    AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session'\n    AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'\n    AIRFLOW__WEBSERVER__SHOW_TRIGGER_FORM_IF_NO_PARAMS: 'true'\n    AIRFLOW__API__MAXIMUM_PAGE_LIMIT: 10000\n    AIRFLOW__API__FALLBACK_PAGE_LIMIT: 10000\n    # AIRFLOW__CORE__DAGBAG_IMPORT_TIMEOUT: 60.0\n\n\n    # --- Celery stability & throughput ---\n    AIRFLOW__CELERY__WORKER_CONCURRENCY: 4 #\"32\"              # threads per worker container (tune below with --scale)\n    AIRFLOW__CELERY__TASK_SOFT_TIME_LIMIT: 600\n    AIRFLOW__CELERY__TASK_TIME_LIMIT: 1200\n    \n    AIRFLOW__CELERY__WORKER_AUTOSCALE: '16,4'  # max,min workers\n    AIRFLOW__CELERY__WORKER_PREFETCH_MULTIPLIER: 1 # prevents task hogging/starvation\n    AIRFLOW__CELERY__WORKER_MAX_TASKS_PER_CHILD: 20 #100\n    \n    AIRFLOW__CELERY__WORKER_DISABLE_RATE_LIMITS: 'true'\n    AIRFLOW__CELERY__BROKER_TRANSPORT_OPTIONS: >-\n      {\"visibility_timeout\": 43200, \"socket_timeout\": 60, \"retry_on_timeout\": true}\n\n    # --- Core parallelism (you have 200 cores / 2TB RAM) ---\n    AIRFLOW__CORE__TASK_RUNNER: StandardTaskRunner\n    AIRFLOW__CORE__DEFAULT_TASK_RETRIES: 1\n    \n    \n    AIRFLOW__CORE__MAX_ACTIVE_TASKS_PER_DAG: 16\n    AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG: 4\n    AIRFLOW__CORE__PARALLELISM: 32\n    AIRFLOW__CORE__DAG_CONCURRENCY: 16\n\n    # --- Scheduler robustness ---\n    AIRFLOW__SCHEDULER__CATCHUP_BY_DEFAULT: 'false'\n    AIRFLOW__SCHEDULER__MAX_TIS_PER_QUERY: 32\n    AIRFLOW__SCHEDULER__PROCESSOR_POLL_INTERVAL: 5\n\n    AIRFLOW__SCHEDULER__PARSING_PROCESSES: 2\n    \n    AIRFLOW__SCHEDULER__SCHEDULER_HEARTBEAT_SEC: 10\n    # AIRFLOW__SCHEDULER__SCHEDULE_AFTER_TASK_EXECUTION: 'true'\n    AIRFLOW__SCHEDULER__SCHEDULER_ZOMBIE_TASK_THRESHOLD: 600\n        \n    AIRFLOW__DATABASE__SQL_ALCHEMY_POOL_SIZE: 15\n    AIRFLOW__DATABASE__SQL_ALCHEMY_MAX_OVERFLOW: 20 #15\n    AIRFLOW__DATABASE__SQL_ALCHEMY_POOL_RECYCLE: 1800 #600\n    AIRFLOW__DATABASE__SQL_ALCHEMY_POOL_PRE_PING: 'true'\n    AIRFLOW__DATABASE__SQL_ALCHEMY_POOL_TIMEOUT: 30\n    AIRFLOW__DATABASE__SQL_ALCHEMY_ECHO: 'false'\n    \n    # Logging\n    AIRFLOW__LOGGING__LOGGING_LEVEL: \"INFO\"\n    AIRFLOW__CORE__ENABLE_XCOM_PICKLING: 'true' #\"false\" # Triggerer keep default,it\u2019s cheap and avoids deferral stalls\n\n    user: \"${AIRFLOW_UID:-50000}:0\"\n    shm_size: '1000g'\n    tmpfs:\n      - /dev/shm:size=1000g\n    depends_on:\n      &airflow-common-depends-on\n      redis:\n        condition: service_healthy\n      postgres:\n        condition: service_healthy\n    deploy:\n      &airflow-common-resources\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: 4\n              capabilities: [gpu]\n  \n  \n  services:\n        \n    postgres:\n      image: postgres:13\n      container_name: postgres\n      command: \n        - postgres\n        - -c\n        - max_connections=500\n        - -c\n        - shared_buffers=1GB\n        - -c\n        - effective_cache_size=2GB\n        - -c\n        - random_page_cost=1.1             # For SSD storage\n        - -c\n        - idle_in_transaction_session_timeout=300000\n        - -c\n        - statement_timeout=600000\n      environment:\n        POSTGRES_USER: airflow\n        POSTGRES_PASSWORD: airflow\n        POSTGRES_DB: airflow\n      volumes:\n        - ../databases/postgres/data:/var/lib/postgresql/data\n      healthcheck:\n        test: [\"CMD\", \"pg_isready\", \"-U\", \"airflow\"]\n        interval: 10s\n        retries: 5\n        start_period: 5s\n      restart: always\n      networks:\n        my_network:\n          ipv4_address: 172.29.1.2\n  \n    pgbouncer:\n      image: edoburu/pgbouncer:latest\n      container_name: pgbouncer\n      environment:\n        - DATABASE_URL=postgresql://airflow:airflow@postgres:5432/airflow\n        - POOL_MODE=transaction\n        - MAX_CLIENT_CONN=1000\n        - DEFAULT_POOL_SIZE=25\n        - RESERVE_POOL_SIZE=5\n        - MAX_DB_CONNECTIONS=200\n        - MIN_POOL_SIZE=10\n      ports:\n        - \"7432:6432\"\n      depends_on:\n        - postgres\n      restart: always\n      networks:\n        my_network:\n          ipv4_address: 172.29.2.14\n  \n    redis:\n      image: redis:latest\n      container_name: redis\n      command: redis-server --maxmemory 4gb --maxmemory-policy allkeys-lru\n      expose:\n        - 6379\n      healthcheck:\n        test: [\"CMD\", \"redis-cli\", \"ping\"]\n        interval: 10s\n        timeout: 30s\n        retries: 50\n        start_period: 30s\n      restart: always\n      networks:\n        my_network:\n          ipv4_address: 172.29.1.3\n      \n  \n  \n    airflow-webserver:\n      <<: *airflow-common\n      command: webserver\n      container_name: airflow-webserver\n      ports:\n        - \"8099:8080\"\n      healthcheck:\n        test: [\"CMD\", \"curl\", \"--fail\", \"http://localhost:8080/health\"]\n        interval: 30s\n        timeout: 10s\n        retries: 5\n        start_period: 30s\n      restart: always\n      depends_on:\n        <<: *airflow-common-depends-on\n        airflow-init:\n          condition: service_completed_successfully\n      deploy:\n        <<: *airflow-common-resources\n      networks:\n        my_network:\n          ipv4_address: 172.29.1.7\netc\n\nhere is my complete docker compose\n\n[docker-compose.yaml](https://github.com/user-attachments/files/22863435/docker-compose.yaml)\n\n\n### What you think should happen instead?\n\n_No response_\n\n### How to reproduce\n\ngiven in docker compose\n\n### Operating System\n\nlinux server 24.04.3\n\n### Versions of Apache Airflow Providers\n\n_No response_\n\n### Deployment\n\nDocker-Compose\n\n### Deployment details\n\n_No response_\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56561\nTitle: XCom dictionary key order not preserved\nState: open\nAuthor: vshih\nLabels: kind:bug, invalid, area:core\nBody:\n### Apache Airflow version\n\n3.1.0\n\n### If \"Other Airflow 2/3 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\nWhen passing a dictionary through XCom (at least as a return-value), key order is not preserved.  The keys appear to be sorted alphabetically when inspected from a consuming task.\n\n### What you think should happen instead?\n\nSince Python 3.7, dictionaries preserve the insertion order of keys.  JSON dumping and loading likewise preserve this also.  Since Airflow is well past that version, I would expect it to simply inherit this behavior.\n\n### How to reproduce\n\nMinimal DAG:\n\n```python\nfrom airflow.decorators import dag, task\n\n\n@dag(render_template_as_native_obj=True)\ndef d1():\n    @task\n    def create() -> dict:\n        return {\n            'C': 1,\n            'B': 2,\n            'A': 3,\n        }\n\n    @task\n    def render(d: dict):\n        print(d)\n\n    render(create())\n\n\nd1()\n```\n\nThe output:\n\n```\nINFO - {'A': 3, 'B': 2, 'C': 1} source=task.stdout\n```\n\n### Operating System\n\nMacOS Tahoe 26.0.1 (25A362)\n\n### Versions of Apache Airflow Providers\n\nI doubt any of these are relevant, but just in case:\n\n\napache-airflow==3.1.0\napache-airflow-core==3.1.0\napache-airflow-providers-amazon==9.14.0\napache-airflow-providers-celery==3.12.3\napache-airflow-providers-cncf-kubernetes==10.8.1\napache-airflow-providers-common-compat==1.7.4\napache-airflow-providers-common-io==1.6.3\napache-airflow-providers-common-messaging==2.0.0\napache-airflow-providers-common-sql==1.28.1\napache-airflow-providers-docker==4.4.3\napache-airflow-providers-elasticsearch==6.3.3\napache-airflow-providers-fab==2.4.3\napache-airflow-providers-ftp==3.13.2\napache-airflow-providers-git==0.0.8\napache-airflow-providers-google==18.0.0\napache-airflow-providers-grpc==3.8.2\napache-airflow-providers-hashicorp==4.3.2\napache-airflow-providers-http==5.3.4\napache-airflow-providers-microsoft-azure==12.7.1\napache-airflow-providers-mysql==6.3.4\napache-airflow-providers-odbc==4.10.2\napache-airflow-providers-openlineage==2.7.1\napache-airflow-providers-postgres==6.3.0\napache-airflow-providers-redis==4.3.1\napache-airflow-providers-sendgrid==4.1.3\napache-airflow-providers-sftp==5.4.0\napache-airflow-providers-slack==9.3.0\napache-airflow-providers-smtp==2.2.1\napache-airflow-providers-snowflake==6.5.4\napache-airflow-providers-ssh==4.1.4\napache-airflow-providers-standard==1.8.0\napache-airflow-task-sdk==1.1.0\nJinja2==3.1.6\n\n\n### Deployment\n\nDocker-Compose\n\n### Deployment details\n\nN/A (fresh docker install)\n\n### Anything else?\n\nHappens 100% of the time.\n\n\nSome possible clues:\n\nThe \"create\" task has this in its log:\n```\nDone. Returned value was: {'C': 1, 'B': 2, 'A': 3} source=airflow.task.operators.airflow.providers.standard.decorators.python._PythonDecoratedOperator loc=python.py:218\n```\nThe order seems to be preserved up to this point.  However, in its XCom pane, the `return_value` shows the dictionary with its keys sorted:\n\n<img width=\"974\" height=\"497\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/72fda9a9-34e7-4da3-ab6e-f1c495e9647a\" />\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56556\nTitle: Move test code to a module path that matches src\nState: closed\nAuthor: o-nikolas\nLabels: area:DAG-processing\nBody:\nThe DagBag class was moved to a module under dag_processing/dagbag.py but the test code still remained under a path matching the old module path (which still exists, with some other classes in it).\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56555\nTitle: Grid view does not show the selected DAG Run\nState: closed\nAuthor: collinmcnulty\nLabels: kind:bug, area:core, area:UI, needs-triage\nBody:\n### Apache Airflow version\n\n3.1.0\n\n### If \"Other Airflow 2/3 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\nI selected a DAG run and the Grid view selected it so that it shows `DAG name / 2025-10-09 13:00:00` but none of the 5 runs shown on the grid are the selected run, they're all later runs.\n\n<img width=\"502\" height=\"229\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/3842ca77-f10b-4d66-a1a5-9a6faafaead0\" />\n\n### What you think should happen instead?\n\nThe Grid view should always contain the selected run.\n\n### How to reproduce\n\n- Change the options to select 50 runs\n- Select an older run\n- Grid view changes back to 5 runs, none of which are the selected run\n\n### Operating System\n\ndebian bullseye\n\n### Versions of Apache Airflow Providers\n\n_No response_\n\n### Deployment\n\nAstronomer\n\n### Deployment details\n\n_No response_\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56554\nTitle: Grid View UI slow\nState: closed\nAuthor: collinmcnulty\nLabels: kind:bug, priority:high, area:core, area:UI, needs-triage\nBody:\n### Apache Airflow version\n\n3.1.0\n\n### If \"Other Airflow 2/3 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\nLoading the grid view for a DAG with <100 tasks took 30 seconds. Specifically this GET request `ui/grid/structure/dag_name?limit=5&order_by=-run_after`\n\nThis happens repeatedly across multiple DAGs and on multiple Airflow instances.\n\n### What you think should happen instead?\n\nI think this should be significantly faster. Possibly it could be progressive, loading only the data required, like the top level task groups to start with?\n\n### How to reproduce\n\nI'm not sure exactly what triggers the long load time. It's possible that mapped tasks are a factor, as we use those frequently. Maybe also the number of dag versions? This happens every time we load up the affected DAGs in the Grid View.\n\n### Operating System\n\ndebian bullseye\n\n### Versions of Apache Airflow Providers\n\n_No response_\n\n### Deployment\n\nAstronomer\n\n### Deployment details\n\n_No response_\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56553\nTitle: FIX: Added missing babel dependency in Task SDK\nState: closed\nAuthor: dabla\nLabels: full tests needed, area:task-sdk, backport-to-v3-1-test\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: [#56552](https://github.com/apache/airflow/issues/56552)\r\nrelated: [#56552](https://github.com/apache/airflow/issues/56552)\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\nAfter the [move of macros to the Task SDK](https://github.com/apache/airflow/pull/46867/files#diff-854d19db18bae58289f4ce996ca0fb34341bc0f22930620627afccbd9d6facfcL30), the babel dependency is missing.  This is probably due to the fact that in the past, the babel dependency was available through the flask-babel dependency.  This PR add the babel dependency to the Task SDK so that macro's depending on it can work again.\r\n\r\ncloses: https://github.com/apache/airflow/issues/56552\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56552\nTitle: Babel dependency is missing since macros have been moved into Task SDK\nState: closed\nAuthor: dabla\nLabels: kind:bug, area:core, needs-triage\nBody:\n### Apache Airflow version\n\n3.1.0\n\n### If \"Other Airflow 2/3 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\nSince [macro's have now been moved to the Task SDK](https://github.com/apache/airflow/pull/46867/files#diff-854d19db18bae58289f4ce996ca0fb34341bc0f22930620627afccbd9d6facfcL30), and Airflow isn't depending on Flask anymore and thus also the [flask-babel](https://pypi.org/project/flask-babel/) dependency, the babel dependency is missing since macro's like ds_format_locale depend on it.\n\n### What you think should happen instead?\n\nThe babel dependency should be added in the Task SDK.\n\n### How to reproduce\n\nUse the macros.ds_format_locale in your jinja expressions and you will get an ModuleNotFoundError: No module named 'babel' error.\n\n[logs.txt](https://github.com/user-attachments/files/22854719/logs.txt)\n\n### Operating System\n\nLinux\n\n### Versions of Apache Airflow Providers\n\n_No response_\n\n### Deployment\n\nOther 3rd-party Helm chart\n\n### Deployment details\n\n_No response_\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [x] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56547\nTitle: fix a typo in  INTHEWILD.md file\nState: closed\nAuthor: hussein-awala\nLabels: \nBody:",
  "Requirement ID: ISSUE-56546\nTitle: add `DataDog` as company using airflow\nState: closed\nAuthor: antonlin1\nLabels: \nBody:\nDataDog is using airflow. Let's make it official\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56545\nTitle: `@task.kubernetes` fails on non-root images due to `/airflow/xcom` directory creation\nState: open\nAuthor: jhgoebbert\nLabels: kind:bug, area:providers, good first issue, pending-response, provider:cncf-kubernetes\nBody:\n### Apache Airflow version\n\n3.1.0\n\n### If \"Other Airflow 2/3 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\nThe `@task.kubernetes` decorator fails when used with non-root container images such as the official `apache/airflow` image.  \n\nAfter [PR #28942](https://github.com/apache/airflow/pull/28942), the initialization phase for Kubernetes-decorated tasks creates the directory `/airflow/xcom` (see [providers/cncf/kubernetes/decorators/kubernetes.py#L98](https://github.com/apache/airflow/blob/54bd5d8cd9f6f477cc83445737614dec81c4323c/providers/cncf/kubernetes/src/airflow/providers/cncf/kubernetes/decorators/kubernetes.py#L98)).  \n\nHowever, in the default Airflow image the container runs as the **`airflow` user**, (see [HERE](https://github.com/apache/airflow/blob/54bd5d8cd9f6f477cc83445737614dec81c4323c/Dockerfile#L2058)) which does not have write permissions to `/`. As a result, the attempt to create `/airflow/xcom` fails with a permissions error, and the Kubernetes task cannot start.\n\n```\n{\"timestamp\":\"2025-10-10T13:10:06.622360Z\",\"level\":\"info\",\"event\":\"[base] + mkdir -p /airflow/xcom\",\"logger\":\"airflow.providers.cncf.kubernetes.utils.pod_manager.PodManager\",\"filename\":\"supervisor.py\",\"lineno\":1739}\n{\"timestamp\":\"2025-10-10T13:10:06.622493Z\",\"level\":\"info\",\"event\":\"[base] mkdir: cannot create directory \u2018/airflow\u2019: Permission denied\\n\"\n```\n\n### What you think should happen instead?\n\nThe Kubernetes-decorated task should run successfully, even when the image runs as a non-root user.\n\n### How to reproduce\n\n1. Use the official Apache Airflow 3 image, e.g.:\n   ```yaml\n   image: apache/airflow:3.1.0\n   ```\n\n2. Define a DAG:\n   ```python\n\tfrom airflow.decorators import dag, task\n\tfrom datetime import datetime\n\n\t@dag(start_date=datetime(2024, 1, 1), schedule=None)\n\tdef test_dag():\n\t\t@task.kubernetes(image=\"apache/airflow:3.1.0\")\n\t\tdef example_task():\n\t\t\tprint(\"Hello from Airflow task\")\n\n\t\texample_task()\n\n\ttest_dag()\n\t```\n\n3. Trigger the DAG.\n\n### Operating System\n\nKubernetes (Airflow charts)\n\n### Versions of Apache Airflow Providers\n\nLatest\n\n### Deployment\n\nOfficial Apache Airflow Helm Chart\n\n### Deployment details\n\nAirflow version: 3.1.0\nExecutor: @task.kubernetes)\nBase image: apache/airflow:3.1.0\nKubernetes version: (v1.32)\n\n### Anything else?\n\nThe /airflow/xcom directory is currently created unconditionally during initialization, assuming root permissions.\nPossible approaches to resolve this:\n- Create the directory under a writable path such as /tmp/airflow/xcom, or\n- Make the XCom path configurable for Kubernetes-decorated tasks, or\n- Check for existence and create the directory only if it\u2019s writable by the current user.\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56544\nTitle: Reduce log level for connection not found from ERROR to WARNING\nState: open\nAuthor: HsiuChuanHsu\nLabels: area:task-sdk\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n\r\n### Problem \r\nWhen KubernetesPodOperator uses the default connection ID `kubernetes_default`, the hook is designed to fallback to \r\ncluster-derived credentials. \r\nhttps://github.com/apache/airflow/blob/b7ea2a6fa5418862b93fcb11c75c02bef0f85c95/providers/cncf/kubernetes/src/airflow/providers/cncf/kubernetes/hooks/kubernetes.py#L172-L184\r\n\r\nBut the current behavior where the Kubernetes API client logs 404 responses as ERROR is causing unnecessary noise.\r\n\r\n<img width=\"1519\" height=\"361\" alt=\"Screenshot 2025-10-10 at 9 45 04\u202fAM\" src=\"https://github.com/user-attachments/assets/cc98e3fb-bfa0-4554-927f-3a346690e3fa\" />\r\n\r\n\r\n### Fix\r\nWhen default connection ID `kubernetes_default` is set, the 404 is an expected behavior, not an actual error. Also, HTTP 404 represents a resource state. ERROR severity might be too high.\r\nSo i think it would be better to reduce log level for connection not found from `ERROR` to `WARNING.` \r\n\r\nFix: #56360\r\n\r\n\r\n<img width=\"1511\" height=\"358\" alt=\"Screenshot 2025-10-10 at 8 35 35\u202fPM\" src=\"https://github.com/user-attachments/assets/0482f3cb-d498-4fdf-b8fd-66c61eeaa26f\" />\r\n\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56543\nTitle: Support passing Note to TriggerDagRunOperator\nState: open\nAuthor: Urahara\nLabels: kind:feature, good first issue, area:core\nBody:\n### Description\n\nSupport passing Note to this TriggerDagRunOperator.\n\n### Use case/motivation\n\nToday the only way to pass a note is via api, so we need to add a step just to add a note.\n\n### Related issues\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56542\nTitle: Fix openlineage dag state event emit on timed out dag\nState: closed\nAuthor: antonlin1\nLabels: area:providers, provider:openlineage\nBody:\nFor dag that is timed out, with only skipped tasks, following error is thrown\r\n```\r\nFailed to emit OpenLineage DAG failed event: \r\n Traceback (most recent call last):\r\n  File \"/usr/local/lib/python3.12/site-packages/airflow/providers/openlineage/plugins/adapter.py\", line 505, in dag_failed\r\n    **get_airflow_state_run_facet(dag_id, run_id, task_ids, dag_run_state),\r\n      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/site-packages/airflow/providers/openlineage/utils/utils.py\", line 746, in get_airflow_state_run_facet\r\n    else (ti.end_date - ti.start_date).total_seconds()\r\n          ~~~~~~~~~~~~^~~~~~~~~~~~~~~\r\nTypeError: unsupported operand type(s) for -: 'NoneType' and 'NoneType'\r\n```\r\n\r\nThis PR fixes said issue",
  "Requirement ID: ISSUE-56541\nTitle: DAG Run triggered ignoring one missing dataset event\nState: open\nAuthor: GiorgosPn\nLabels: kind:bug, area:core, needs-triage\nBody:\n### Apache Airflow version\n\nOther Airflow 2/3 version (please specify below)\n\n### If \"Other Airflow 2/3 version\" selected, which one?\n\n2.10.4\n\n### What happened?\n\nWe created a DAG, scheduled with 16 different datasets. It ran properly for a couple of days, waiting for all 16 datasets to be produced. But suddenly, it was triggerred ignoring one dataset (triggered taking into account 15/16 datasets).\n\n<img width=\"945\" height=\"630\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/f26b008a-1703-4eab-8c3c-17d3b4fe341b\" />\n<img width=\"617\" height=\"443\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/b839ef5d-ca97-45ff-b7ee-56518d492c2d\" />\n<img width=\"1833\" height=\"592\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/8827edcc-2d25-4691-bd4d-9d451570b299\" />\n\n### What you think should happen instead?\n\nThe DAG should not be triggered, since only 15 out of 16 datasets were produced at that time.\n\n### How to reproduce\n\nWe cannot reproduce it, it did not happen earlier and we haven't faced it  again some days later.\n\n### Operating System\n\nDebian 12\n\n### Versions of Apache Airflow Providers\n\nNo relevant providers used\n\n### Deployment\n\nOther Docker-based deployment\n\n### Deployment details\n\nWe use an AKS cluster in combination with customized docker image, built from the official docker image.\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56540\nTitle: feat: add async pagerduty notifier\nState: closed\nAuthor: dondaum\nLabels: area:providers, provider:pagerduty\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\nAdd an asynchronous version of `PagerdutyNotifier`. As discussed in #55237, the asynchronous Notfier uses the Pagerduty REST API and the `HttpAsyncHook` since the Pagerduty package `pagerduty` only supports sync API calls.\r\n\r\nrelated: #55237 \r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56539\nTitle: Set AIRFLOW_VERSION to the current version in the documentation quickstart\nState: closed\nAuthor: jbd\nLabels: kind:documentation\nBody:",
  "Requirement ID: ISSUE-56538\nTitle: \ud83c\udf10 2.11.0 to Anaconda\nState: closed\nAuthor: pustoshilov-d\nLabels: \nBody:\n### Description\n\nHi! I have just found out that on Anaconda there is no 2.11.0 version, however 3.1.0 exists. \nCould you push it pls?\n\n### Use case/motivation\n\nFor dags development environment\n\n### Related issues\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56537\nTitle: Support configurable multiplier for retry exponential backoff\nState: open\nAuthor: 0BVer\nLabels: kind:feature, area:core, needs-triage\nBody:\n### Description\n\nI love using Airflow to build workflow logic, but I've hit a limitation with the current retry options.\nRight now, I need to implement a customizable retry logic \u2014 specifically, I want to set a configurable multiplier for the retry exponential backoff, but the multiplier is currently hardcoded to 2 in Airflow.\n\n```python\ndef next_retry_datetime(self):\n    ...\n    min_backoff = math.ceil(delay.total_seconds() * (2 ** (self.try_number - 1)))\n    ...\n```\n\nSo I suggest adding a new parameter such as retry_delay_multiplier to allow users to configure the exponential backoff.\n\n```python\n# like this!\ndefault_args = {\n    'depends_on_past': False,\n    'retries': 2,\n    'retry_exponential_backoff': True,\n    'retry_delay': timedelta(minutes=4),\n    'retry_delay_multiplier': 5,\n}\n```\nIt will retry after 4 min, then 20 min, and then 100 min.\n\n### Use case/motivation\n\nMany data pipelines or APIs have different retry requirements: the fixed exponential backoff with a multiplier of 2 is sometimes too aggressive or too slow depending on the job's nature. If Airflow provides a configurable multiplier for exponential backoff, users can fine-tune retry intervals for different workloads. This is especially useful for critical external integrations, error-prone batch jobs, or systems with rate limits, where controlling the growth rate of delay is essential for stability, throughput, and SLA adherence.\n\nCustomizing the multiplier would make Airflow's retry system much more versatile and suitable for a wider variety of operational scenarios.\n\n### Related issues\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [x] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56536\nTitle: Fix deterministic DAG serialization\nState: open\nAuthor: anshuksi282-ksolves\nLabels: area:serialization\nBody:\n## Fix: Stable DAG Serialization Hash (Deterministic Ordering)\r\nThis PR fixes a critical issue where DAGs generated unnecessary new versions due to non-deterministic JSON ordering during serialization.\r\n\r\n### Key Changes & Verification\r\nImplemented recursive sorting logic (_sort_serialized_dag_dict) to ensure the dag_hash is 100% stable.\r\n\r\nSorting is used only for hash calculation; the original, unsorted data is still persisted (addressing reviewer concerns).\r\n\r\nAll unrelated changes were reverted to keep the PR focused and clean for backporting.\r\n\r\nThe fix is confirmed by the successful unit test output (including the previously failing asset dependency test):\r\n\r\n```bash\r\nairflow-core/tests/unit/models/test_serialized_dag.py::TestSerializedDagModel::test_get_dag_dependencies_with_all_types[raw-serialized_dags] PASSED [ 50%]\r\nairflow-core/tests/unit/models/test_serialized_dag.py::TestSerializedDagModel::test_get_dag_dependencies_with_all_types[compress-serialized_dags] PASSED [100%]\r\n```\r\n\r\n### Result\r\nSerialization is now deterministic. This reduces unnecessary DB writes and improves system reliability.\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56535\nTitle: GlueJobOperator in verbose mode do not pull logs correctly\nState: open\nAuthor: adamgorkaextbi\nLabels: kind:bug, provider:amazon, area:providers, needs-triage\nBody:\n### Apache Airflow Provider(s)\n\namazon\n\n### Versions of Apache Airflow Providers\n\napache-airflow-providers-amazon==9.10.0 up to newest at current time 9.15.0\n\n### Apache Airflow version\n\n2.11.0\n\n### Operating System\n\nDebian GNU/Linux 12\n\n### Deployment\n\nOfficial Apache Airflow Helm Chart\n\n### Deployment details\n\nofficial helm chart\n\n### What happened\n\nGlueJobOperator in dereferable mode, wait_for_completion=true, verbose=true mode is not pulling logs\nnewly introduced \nwaiter_with_logging is not working correctly\n[2025-10-09, 22:25:23 UTC] {waiter_with_logging.py:158} INFO - Status of AWS Glue job is: RUNNING\n\n\n### What you think should happen instead\n\nI should see glue logs in airflow UI i task details log section\n\n### How to reproduce\n\nrun GlueJobOperator  in dereferable mode,\nwait_for_completion=true, verbose=true\n\n### Anything else\n\nall the time\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56533\nTitle: CKV_K8S_38 Checkov Issue\nState: closed\nAuthor: ttony\nLabels: kind:feature, needs-triage\nBody:\n### Description\n\nHi Team,\n\nI would like to ask if service account for the token auto-mount be projected into other director rather than default directory?\n\n\n\n### Use case/motivation\n\n_No response_\n\n### Related issues\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56532\nTitle: Checkov CKV_K8S_35 Issues\nState: closed\nAuthor: ttony\nLabels: area:secrets, kind:feature, needs-triage\nBody:\n### Description\n\nHi Team,\n\nI would like to ask if we could not use secret as env ? is it possible to mounted via files?\n\nThanks\n\n### Use case/motivation\n\n_No response_\n\n### Related issues\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56531\nTitle: [WIP] Rust DAG Processor\nState: open\nAuthor: shahar1\nLabels: area:dev-tools, area:production-image, backport-to-v3-1-test\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56530\nTitle: [v3-1-test] Add rancher-desktop to PREFERRED_CONTEXTS in docker_command_utils (#56525)\nState: closed\nAuthor: github-actions[bot]\nLabels: area:dev-tools, backport-to-v3-1-test\nBody:\n(cherry picked from commit 2454f6efe1f53c04119a280d6177d82e3ba499a9)\n\nCo-authored-by: sage-ingle-ck <sage.ingle@creditkarma.com>",
  "Requirement ID: ISSUE-56529\nTitle: DAG Processor fails with this error airflow.exceptions.DeserializationError: An unexpected error occurred while trying to deserialize Dag 'eFundamentals_mla_availability_report'\nState: open\nAuthor: nikhilcss97\nLabels: kind:bug, area:core, needs-triage\nBody:\n### Apache Airflow version\n\n3.1.0\n\n### If \"Other Airflow 2/3 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\nAfter upgrading from Airflow 3.0.6 to Airflow 3.10 we started getting this issue in the Dag processor which keeps restarting on its own\n\nLogs:\n\n```\n2025-10-09T14:12:04.364297Z [error    ] Failed to deserialize DAG      [airflow.serialization.serialized_objects] loc=serialized_objects.py:3890\n\n    return cls._deserialize_datetime(value) if value is not None else None\n  File \"/home/airflow/.venv/lib/python3.10/site-packages/airflow/_shared/timezones/timezone.py\", line 302, in from_timestamp\n    result = coerce_datetime(dt.datetime.fromtimestamp(timestamp, tz=utc))\nTypeError: 'str' object cannot be interpreted as an integer\nThe above exception was the direct cause of the following exception:\nTraceback (most recent call last):\n  File \"/home/airflow/.venv/lib/python3.10/site-packages/airflow/jobs/dag_processor_job_runner.py\", line 61, in _execute\n    self.processor.run()\n  File \"/home/airflow/.venv/lib/python3.10/site-packages/airflow/dag_processing/manager.py\", line 272, in run\n    return self._run_parsing_loop()\n  File \"/home/airflow/.venv/lib/python3.10/site-packages/airflow/dag_processing/manager.py\", line 361, in _run_parsing_loop\n    self._collect_results()\n  File \"/home/airflow/.venv/lib/python3.10/site-packages/airflow/utils/session.py\", line 100, in wrapper\n    return func(*args, session=session, **kwargs)\n  File \"/home/airflow/.venv/lib/python3.10/site-packages/airflow/dag_processing/manager.py\", line 827, in _collect_results\n    self._file_stats[file] = process_parse_results(\n  File \"/home/airflow/.venv/lib/python3.10/site-packages/airflow/dag_processing/manager.py\", line 1155, in process_parse_results\n    update_dag_parsing_results_in_db(\n  File \"/home/airflow/.venv/lib/python3.10/site-packages/airflow/dag_processing/collection.py\", line 372, in update_dag_parsing_results_in_db\n    for attempt in run_with_db_retries(logger=log):\n  File \"/home/airflow/.venv/lib/python3.10/site-packages/tenacity/__init__.py\", line 443, in __iter__\n    do = self.iter(retry_state=retry_state)\n  File \"/home/airflow/.venv/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/home/airflow/.venv/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/usr/python/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/usr/python/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/home/airflow/.venv/lib/python3.10/site-packages/airflow/dag_processing/collection.py\", line 382, in update_dag_parsing_results_in_db\n    SerializedDAG.bulk_write_to_db(\n  File \"/home/airflow/.venv/lib/python3.10/site-packages/airflow/utils/session.py\", line 98, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/airflow/.venv/lib/python3.10/site-packages/airflow/serialization/serialized_objects.py\", line 2868, in bulk_write_to_db\n    dag_op.update_dags(orm_dags, parse_duration, session=session)\n  File \"/home/airflow/.venv/lib/python3.10/site-packages/airflow/dag_processing/collection.py\", line 534, in update_dags\n    dm.calculate_dagrun_date_fields(dag, last_automated_data_interval)  # type: ignore[arg-type]\n  File \"/home/airflow/.venv/lib/python3.10/site-packages/airflow/models/dag.py\", line 680, in calculate_dagrun_date_fields\n    next_dagrun_info = dag.next_dagrun_info(last_automated_data_interval)\n  File \"/home/airflow/.venv/lib/python3.10/site-packages/airflow/serialization/serialized_objects.py\", line 3879, in next_dagrun_info\n    return self._real_dag.next_dagrun_info(*args, **kwargs)\n  File \"/usr/python/lib/python3.10/functools.py\", line 981, in __get__\n    val = self.func(instance)\n  File \"/home/airflow/.venv/lib/python3.10/site-packages/airflow/serialization/serialized_objects.py\", line 3888, in _real_dag\n    return SerializedDAG.from_dict(self.data)\n  File \"/home/airflow/.venv/lib/python3.10/site-packages/airflow/serialization/serialized_objects.py\", line 2837, in from_dict\n    return cls.deserialize_dag(serialized_obj[\"dag\"], client_defaults)\n  File \"/home/airflow/.venv/lib/python3.10/site-packages/airflow/serialization/serialized_objects.py\", line 2528, in deserialize_dag\n    raise DeserializationError(dag_id) from err\nairflow.exceptions.DeserializationError: An unexpected error occurred while trying to deserialize Dag 'eFundamentals_mla_availability_report'\n2025-10-09T14:12:04.407428Z [info     ] Process exited                 [supervisor] exit_code=<Negsignal.SIGTERM: -15> loc=supervisor.py:709 pid=2802 signal_sent=SIGTERM\n2025-10-09T14:12:04.409223Z [info     ] Waiting up to 5 seconds for processes to exit... [airflow.utils.process_utils] loc=process_utils.py:285\nTraceback (most recent call last):\n  File \"/home/airflow/.venv/lib/python3.10/site-packages/airflow/serialization/serialized_objects.py\", line 2522, in deserialize_dag\n    return cls._deserialize_dag_internal(encoded_dag, client_defaults)\n  File \"/home/airflow/.venv/lib/python3.10/site-packages/airflow/serialization/serialized_objects.py\", line 2548, in _deserialize_dag_internal\n    deser = SerializedBaseOperator.deserialize_operator(\n  File \"/home/airflow/.venv/lib/python3.10/site-packages/airflow/serialization/serialized_objects.py\", line 1768, in deserialize_operator\n    cls.populate_operator(op, encoded_op, client_defaults)\n  File \"/home/airflow/.venv/lib/python3.10/site-packages/airflow/serialization/serialized_objects.py\", line 1610, in populate_operator\n    v = cls._deserialize_partial_kwargs(v, client_defaults)\n  File \"/home/airflow/.venv/lib/python3.10/site-packages/airflow/serialization/serialized_objects.py\", line 2133, in _deserialize_partial_kwargs\n    deserialized[k] = cls._deserialize_field_value(k, v)\n  File \"/home/airflow/.venv/lib/python3.10/site-packages/airflow/serialization/serialized_objects.py\", line 2101, in _deserialize_field_value\n    return cls._deserialize_datetime(value) if value is not None else None\n  File \"/home/airflow/.venv/lib/python3.10/site-packages/airflow/_shared/timezones/timezone.py\", line 302, in from_timestamp\n    result = coerce_datetime(dt.datetime.fromtimestamp(timestamp, tz=utc))\nTypeError: 'str' object cannot be interpreted as an integer\nThe above exception was the direct cause of the following exception:\nTraceback (most recent call last):\n  File \"/home/airflow/.venv/bin/airflow\", line 10, in <module>\n    sys.exit(main())\n  File \"/home/airflow/.venv/lib/python3.10/site-packages/airflow/__main__.py\", line 55, in main\n    args.func(args)\n  File \"/home/airflow/.venv/lib/python3.10/site-packages/airflow/cli/cli_config.py\", line 49, in command\n    return func(*args, **kwargs)\n  File \"/home/airflow/.venv/lib/python3.10/site-packages/airflow/utils/cli.py\", line 114, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/airflow/.venv/lib/python3.10/site-packages/airflow/utils/providers_configuration_loader.py\", line 54, in wrapped_function\n    return func(*args, **kwargs)\n  File \"/home/airflow/.venv/lib/python3.10/site-packages/airflow/cli/commands/dag_processor_command.py\", line 53, in dag_processor\n    run_command_with_daemon_option(\n  File \"/home/airflow/.venv/lib/python3.10/site-packages/airflow/cli/commands/daemon_utils.py\", line 86, in run_command_with_daemon_option\n    callback()\n  File \"/home/airflow/.venv/lib/python3.10/site-packages/airflow/cli/commands/dag_processor_command.py\", line 56, in <lambda>\n    callback=lambda: run_job(job=job_runner.job, execute_callable=job_runner._execute),\n  File \"/home/airflow/.venv/lib/python3.10/site-packages/airflow/utils/session.py\", line 100, in wrapper\n    return func(*args, session=session, **kwargs)\n  File \"/home/airflow/.venv/lib/python3.10/site-packages/airflow/jobs/job.py\", line 368, in run_job\n    return execute_job(job, execute_callable=execute_callable)\n  File \"/home/airflow/.venv/lib/python3.10/site-packages/airflow/jobs/job.py\", line 397, in execute_job\n    ret = execute_callable()\n  File \"/home/airflow/.venv/lib/python3.10/site-packages/airflow/jobs/dag_processor_job_runner.py\", line 61, in _execute\n    self.processor.run()\n  File \"/home/airflow/.venv/lib/python3.10/site-packages/airflow/dag_processing/manager.py\", line 272, in run\n    return self._run_parsing_loop()\n  File \"/home/airflow/.venv/lib/python3.10/site-packages/airflow/dag_processing/manager.py\", line 361, in _run_parsing_loop\n    self._collect_results()\n  File \"/home/airflow/.venv/lib/python3.10/site-packages/airflow/utils/session.py\", line 100, in wrapper\n    return func(*args, session=session, **kwargs)\n  File \"/home/airflow/.venv/lib/python3.10/site-packages/airflow/dag_processing/manager.py\", line 827, in _collect_results\n    self._file_stats[file] = process_parse_results(\n  File \"/home/airflow/.venv/lib/python3.10/site-packages/airflow/dag_processing/manager.py\", line 1155, in process_parse_results\n    update_dag_parsing_results_in_db(\n  File \"/home/airflow/.venv/lib/python3.10/site-packages/airflow/dag_processing/collection.py\", line 372, in update_dag_parsing_results_in_db\n    for attempt in run_with_db_retries(logger=log):\n  File \"/home/airflow/.venv/lib/python3.10/site-packages/tenacity/__init__.py\", line 443, in __iter__\n    do = self.iter(retry_state=retry_state)\n  File \"/home/airflow/.venv/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/home/airflow/.venv/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/usr/python/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/usr/python/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/home/airflow/.venv/lib/python3.10/site-packages/airflow/dag_processing/collection.py\", line 382, in update_dag_parsing_results_in_db\n    SerializedDAG.bulk_write_to_db(\n  File \"/home/airflow/.venv/lib/python3.10/site-packages/airflow/utils/session.py\", line 98, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/airflow/.venv/lib/python3.10/site-packages/airflow/serialization/serialized_objects.py\", line 2868, in bulk_write_to_db\n    dag_op.update_dags(orm_dags, parse_duration, session=session)\n  File \"/home/airflow/.venv/lib/python3.10/site-packages/airflow/dag_processing/collection.py\", line 534, in update_dags\n    dm.calculate_dagrun_date_fields(dag, last_automated_data_interval)  # type: ignore[arg-type]\n  File \"/home/airflow/.venv/lib/python3.10/site-packages/airflow/models/dag.py\", line 680, in calculate_dagrun_date_fields\n    next_dagrun_info = dag.next_dagrun_info(last_automated_data_interval)\n  File \"/home/airflow/.venv/lib/python3.10/site-packages/airflow/serialization/serialized_objects.py\", line 3879, in next_dagrun_info\n    return self._real_dag.next_dagrun_info(*args, **kwargs)\n  File \"/usr/python/lib/python3.10/functools.py\", line 981, in __get__\n    val = self.func(instance)\n  File \"/home/airflow/.venv/lib/python3.10/site-packages/airflow/serialization/serialized_objects.py\", line 3888, in _real_dag\n    return SerializedDAG.from_dict(self.data)\n  File \"/home/airflow/.venv/lib/python3.10/site-packages/airflow/serialization/serialized_objects.py\", line 2837, in from_dict\n    return cls.deserialize_dag(serialized_obj[\"dag\"], client_defaults)\n  File \"/home/airflow/.venv/lib/python3.10/site-packages/airflow/serialization/serialized_objects.py\", line 2528, in deserialize_dag\n    raise DeserializationError(dag_id) from err\nairflow.exceptions.DeserializationError: An unexpected error occurred while trying to deserialize Dag 'eFundamentals_mla_availability_report'\n```\n\n### What you think should happen instead?\n\n_No response_\n\n### How to reproduce\n\nWe are not exactly sure why this started happening.\n\n### Operating System\n\nUbuntu\n\n### Versions of Apache Airflow Providers\n\nWe are pretty much on the latest versions of most of the providers.\n\n### Deployment\n\nOfficial Apache Airflow Helm Chart\n\n### Deployment details\n\nWe are on K8s\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56528\nTitle: Add fail_on_file_not_exist to SFTPToGCSOperator\nState: open\nAuthor: nguy4130\nLabels: provider:google, area:providers\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n\r\n Fixes: https://github.com/apache/airflow/issues/41472\r\n\r\nAdding `fail_on_file_not_exist param` to SFTPToGCSOperator so that user can configure the parameter and operator will not fail in case of sftp file not exist.\r\n\r\nNote that this is only added to the non-wildcard source file path as that code already kind of handles when the file does not exist.\r\n\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).\r\n\r\nTested with a local DAG\r\n```python\r\nfrom airflow import DAG\r\nfrom airflow.providers.google.cloud.transfers.sftp_to_gcs import SFTPToGCSOperator\r\nfrom datetime import datetime, timedelta\r\n\r\ndefault_args = {\r\n    \"owner\": \"airflow\",\r\n    \"retries\": 1,\r\n    \"retry_delay\": timedelta(minutes=1),\r\n}\r\n\r\nwith DAG(\r\n    dag_id=\"sftp_to_gcs_example\",\r\n    default_args=default_args,\r\n    description=\"Transfer a file from SFTP to GCS\",\r\n    start_date=datetime(2023, 1, 1),\r\n    catchup=False,\r\n    tags=[\"example\", \"sftp\", \"gcs\"],\r\n) as dag:\r\n    transfer_file = SFTPToGCSOperator(\r\n        task_id=\"transfer_file\",\r\n        source_path=\"test.mv.dba\",\r\n        sftp_conn_id=\"sftp_default\",\r\n        gcp_conn_id=\"google_cloud_default\",\r\n        destination_bucket=\"airflow\",\r\n        destination_path=\"test/test.mv.db\",\r\n        fail_on_file_not_exist=False,\r\n    )\r\n```\r\n\r\nwhen `fail_on_file_not_exist=False`, the DAG exited gracefully\r\n<img width=\"1350\" height=\"258\" alt=\"Screenshot 2025-10-09 at 4 52 23\u202fPM\" src=\"https://github.com/user-attachments/assets/7ee1204d-c41c-46e2-beac-c46d6a424f10\" />\r\n\r\n\r\nwhen `fail_on_file_not_exist=True` or not set, the DAG failed with error (existing behavior)\r\n<img width=\"1315\" height=\"506\" alt=\"Screenshot 2025-10-09 at 4 53 00\u202fPM\" src=\"https://github.com/user-attachments/assets/ea4743b9-0ef4-4d44-9c63-77f0765f7a62\" />",
  "Requirement ID: ISSUE-56527\nTitle: added the path for the how-to-guide docs for copy into Snowflake\nState: closed\nAuthor: Srivats7\nLabels: area:providers, provider:snowflake\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\nAdded the path for the how-to-guide docs for copy into Snowflake for Google Cloud Storage (GCS) & Microsoft Azure Blob Storage providers\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56526\nTitle: Fix use of DeprecationWarning in celery provider to AirflowProviderDeprecationWarning\nState: closed\nAuthor: krupakar010\nLabels: area:providers, provider:celery\nBody:\nrelated: #55889\r\n   \r\n  DeprecationWarning instance is used in the celery executor provider testing, Using AirflowProviderDeprecationWarning is relevant so updated to AirflowProviderDeprecationWarning.",
  "Requirement ID: ISSUE-56525\nTitle: Add rancher-desktop to PREFERRED_CONTEXTS in docker_command_utils\nState: closed\nAuthor: sage-ingle-ck\nLabels: area:dev-tools, backport-to-v3-1-test\nBody:\nPurpose: The set-up steps for Breeze are compatible with Rancher's dockerd (moby) Container Engine when provided the appropriate context. \r\n\r\nIssue: When running `breeze start-airflow` with Rancher, script automatically sets the context to `default` which does not point to the expected `unix:///var/run/docker.sock`. This causes the user to be prompted with `Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?`.\r\n\r\nSolution: Adding `rancher-desktop` to the list of preferred contexts allows `autodetect_docker_context` to find Rancher's context and successfully start the container.",
  "Requirement ID: ISSUE-56524\nTitle: Removing `amoghrajesh` from older org\nState: closed\nAuthor: amoghrajesh\nLabels: \nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n<img width=\"500\" height=\"500\" alt=\"image\" src=\"https://github.com/user-attachments/assets/1fcca7e6-5fd0-49d8-ac93-dfd8ffe28792\" />\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56523\nTitle: Adding debug logging to Dataproc wait_for_job()\nState: closed\nAuthor: jakepage11\nLabels: provider:google, area:providers\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n\r\n---\r\n\r\n### Closes: #53620\r\n\r\nThis PR addresses the lack of debug logging in the `Dataproc` module's `wait_for_job()` method.\r\n\r\nIt adds logging statements for communicating that the process is still waiting for the job, when the \r\nmethod performs a sleep, and the state of the job after each sleep operation. This helps the users\r\nunderstand that the process to check the job run status is still running by communicating the state\r\nupon each check.\r\n\r\nLink to issue: https://github.com/apache/airflow/issues/53620#event-20194788730\r\n\r\n\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56522\nTitle: Add fail_on_file_not_exist to SFTPToGCSOperator\nState: closed\nAuthor: nguy4130\nLabels: provider:google, area:providers\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n\r\nFixes: https://github.com/apache/airflow/issues/40576\r\nSimilar to https://github.com/apache/airflow/pull/44320, this is adding `fail_on_file_not_exist` param to `SFTPToGCSOperator` so that user can configure the parameter and operator will not fail in case of sftp file not exist.\r\n\r\nTODO: Add local test result\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56521\nTitle: fixed typos in GOVERNANCE.md file\nState: closed\nAuthor: rj-pawar\nLabels: \nBody:\nFixed typos on line 23 and 26",
  "Requirement ID: ISSUE-56520\nTitle: Adding usafacts to \"INTHEWILD\" doc\nState: closed\nAuthor: jakepage11\nLabels: \nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n\r\nAdding \"USAFacts\" to INTHEWILD.md :)\r\n\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56519\nTitle: fixed the bug report template dropdown option list\nState: closed\nAuthor: Sagar1711\nLabels: area:dev-tools\nBody:\n# Issue Details\r\nThe github issue template only specifies Other Airflow 2 versions, since we have now Airflow 3 release as well, it should be updated to `Other Airflow 2/3 versions` in the dropdown option.",
  "Requirement ID: ISSUE-56518\nTitle: Add MSCI to INTHEWILD list\nState: closed\nAuthor: macdub\nLabels: \nBody:\nAdding MSCI, inc. to the list of compaines that use Airflow.",
  "Requirement ID: ISSUE-56517\nTitle: Add `Intuit` as a company using Airflow\nState: closed\nAuthor: nguy4130\nLabels: \nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56516\nTitle: Update Snowflake docs with breaking change\nState: closed\nAuthor: awiede\nLabels: area:providers, kind:documentation, provider:snowflake\nBody:\nUpdates the release notes for the snowflake provider 6.3.0 to appropriately flag a breaking change.\r\n\r\nCloses #51021 \r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56515\nTitle: adding Boeing to inthewild.md\nState: closed\nAuthor: twk123\nLabels: \nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\nJust adding Boeing Inc to the INTHEWILD.md \r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56514\nTitle: Fix broken main after pydantic 2.12.0 - partly cleanup\nState: closed\nAuthor: jscheffl\nLabels: area:airflow-ctl, backport-to-v3-1-test\nBody:\nrelated: #56482\r\n\r\nNote: This PR does NOT close the issue but partly reverts the workaround. The `prek` hook still needs a real fix.\r\n\r\nSee: https://github.com/apache/airflow/issues/56482#issuecomment-3382754639",
  "Requirement ID: ISSUE-56513\nTitle: Enhance Tableau Operator Logging with Human Readable Object DetailsTableau log in detail\nState: closed\nAuthor: sandeepmandal70\nLabels: area:providers, provider:tableau\nBody:\nLinked Issue\r\n\r\nCloses: [#49671](https://github.com/apache/airflow/issues/49671)\r\n\r\n### Description\r\n\r\nThis PR enhances the observability of Tableau workflows by improving logging within the TableauOperator.\r\n\r\nPreviously, Airflow logs only displayed Tableau object LUIDs (IDs) when a matching resource was found. This made it difficult for users and developers to identify which Tableau content (e.g., workbook, datasource) was being processed during execution.\r\n\r\nTo improve visibility, additional log lines have been added immediately after the existing resource ID log message. These new lines display user-friendly object details, including resource name, folder/project, and a direct URL to the Tableau content on Tableau Server or Tableau Cloud.\r\n\r\n\r\n\r\n**Before**\r\n`self.log.info(\"Found matching with id %s\", resource_id)`\r\n\r\n**After**\r\n\r\n```\r\nself.log.info(\"Found matching with id %s\", resource_id)\r\nself.log.info(\"Resource object : %s\", resource)\r\nself.log.info(\"Content Name : %s\", resource.name)\r\nself.log.info(\"Content Folder : %s\", resource.project_name)\r\nself.log.info(\"Content URL : %s\", resource.webpage_url)\r\n```\r\n\r\n\r\n**Benefits**\r\n\r\n- Improves log readability by including meaningful Tableau content details.\r\n- Enables operators and developers to quickly identify which Tableau assets are processed during job runs.\r\n- Provides a direct clickable URL for quick navigation and validation on Tableau Server/Cloud.\r\n- Does not change any existing behavior it only enhances logging output.\r\n\r\n\r\n**Testing Done**\r\n\r\n- Tested locally by triggering various Tableau jobs.\r\n- Verified that logs now display:\r\n\r\n> - Resource object details\r\n> - Content name and project folder\r\n> - Direct Tableau content URL\r\n\r\n- Confirmed that existing functionality and job execution remain unaffected.\r\n\r\nThis enhancement provides developers and data engineers greater transparency during Tableau workflow execution.\r\nThe Content URL in particular enables faster troubleshooting by allowing direct access to the corresponding Tableau resource without manually searching for it.\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56512\nTitle: Bump uv version from 0.9.0 to 0.9.1\nState: closed\nAuthor: bugraoz93\nLabels: area:dev-tools, area:production-image, backport-to-v3-1-test\nBody:\nrelated: https://github.com/apache/airflow/actions/runs/18386942354/job/52387778060\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56511\nTitle: [v3-1-test] Use Task Display Name in Graph if existing (#56455)\nState: closed\nAuthor: github-actions[bot]\nLabels: area:API\nBody:\n(cherry picked from commit af2701fc9eca42ebe82253f7c763f0a1303fcff4)\n\nCo-authored-by: Jens Scheffler <95105677+jscheffl@users.noreply.github.com>",
  "Requirement ID: ISSUE-56509\nTitle: Fix .airflowignore order precedence\nState: open\nAuthor: zach-overflow\nLabels: \nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n\r\nCloses: #56299\r\n\r\n## What?\r\n* Fixes a bug where the order of precedence is opposite of what is expected for gitignore-like glob patterns for `.airflowignore` rules.\r\n* Add more unit test cases for include/exclude behavior (`/**/*` patterns, individual explicit path includes / excludes).\r\n* Cleans up the unit tests:\r\n    * Ensures the test ignore/include files names match their expected behavior\r\n    * Add human-readable diff output for test failures\r\n\r\n## Testing\r\n* Ran `uv run pytest airflow-core/tests/unit/utils --skip-db-tests -n auto` without failure\r\n* Ran `prek` without failure",
  "Requirement ID: ISSUE-56508\nTitle: Airflow CLI extensions via plugins\nState: open\nAuthor: dwreeves\nLabels: area:CLI, kind:feature, needs-triage\nBody:\n### Description\n\nI'm not sure how big this is, but I imagine this is small enough to not require an AIP.\n\nExample of what this could look like:\n\n```python\nfrom airflow.plugins_manager import AirflowPlugin\nfrom airflow.cli.cli_config import (\n    ActionCommand,\n    GroupCommand,\n)\n\n\ndef function():\n    print(\"Hello, world!\")\n\n\nclass ExamplePlugin(AirflowPlugin):\n    name = \"example\"\n    cli_commands = [\n        GroupCommand(\n            name=\"example\",\n            help=\"Example command from plugin\",\n            description=(\n                \"This runs all example commands\"\n            ),\n            subcommands=[\n                ActionCommand(\n                    name=\"subcommand\",\n                    help=\"Run a subcommand\",\n                    func=function,\n                )\n            ],\n        ),\n    ]\n```\n\n### Use case/motivation\n\nThe main motivation that comes to mind is Cosmos: https://github.com/astronomer/astronomer-cosmos Cosmos is a library that runs dbt in Airflow. dbt is a command-line tool that orchestrates, manages, and runs SQL queries. It would be a big quality-of-life upgrade if Cosmos were able to execute `dbt` commands with something like: `airflow cosmos --project foo dbt run --select bar`.\n\nRight now many users struggle to execute dbt via the command line when it's configured for Airflow. I personally use a weird setup to execute one-off runs of dbt models that I expect many users would not know how to do themselves.\n\nSome advantages for Cosmos, and more generally:\n\n- Would hook easily into config configured in the user's Airflow instance.\n- Could be used to execute one-off commands. This is a popular use case for dbt; many more savvy users already have an unscheduled parameterized DAG devoted to executing manual dbt commands. There are, however, many other commands that users might want to run directly from the CLI. E.g. imagine if something like the [maintenance DAGs](https://github.com/teamclairvoyant/airflow-maintenance-dags) repo had a CLI.\n\n### Related issues\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [x] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56507\nTitle: Update XCom Table columns\nState: open\nAuthor: bbovenzi\nLabels: kind:bug, kind:feature, good first issue, area:UI\nBody:\n### Apache Airflow version\n\nmain (development)\n\n### If \"Other Airflow 2 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\nWe should display the timestamp in the Xcom table\n\nWe should also show `task_display_name` in the task column instead of `task_id`\n\n### What you think should happen instead?\n\n_No response_\n\n### How to reproduce\n\nhave xcoms, look at UI, see no timestamps\n\n### Operating System\n\n  all\n\n### Versions of Apache Airflow Providers\n\n_No response_\n\n### Deployment\n\nOfficial Apache Airflow Helm Chart\n\n### Deployment details\n\n_No response_\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56506\nTitle: Remove some irrelevant TODOs in task sdk\nState: closed\nAuthor: amoghrajesh\nLabels: area:task-sdk, backport-to-v3-1-test\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\nThese are irrelevant now. Removing them\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56505\nTitle: Bump python 3.9 to 3.9.23 and 3.12 to 3.12.12\nState: closed\nAuthor: amoghrajesh\nLabels: area:dev-tools, kind:documentation, area:production-image, backport-to-v3-1-test\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\nFixes failing main like: https://github.com/apache/airflow/actions/runs/18378223269/job/52357739969\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56503\nTitle: Replace airflow.utils.timezone.utcnow with airflow.sdk.timezone.utcnow\nState: open\nAuthor: NilsJPWerner\nLabels: provider:google, provider:amazon, area:providers, provider:cncf-kubernetes, area:Triggerer, provider:standard\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\nThis resolves the deprecation warnings in AF 3+ when airflow.utils.timezone.utcnow is used: \r\n```DeprecatedImportWarning: The `airflow.utils.timezone.utcnow` attribute is deprecated. Please use `'airflow.sdk.timezone.utcnow'```\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56501\nTitle: Logs not uploaded to S3 on failed retries\nState: open\nAuthor: TheoLauw\nLabels: kind:bug, area:providers, area:core, needs-triage\nBody:\n### Apache Airflow version\n\n3.1.0\n\n### If \"Other Airflow 2 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\nWe are experiencing a critical issue with log upload to S3 when using DbtTaskGroup from Cosmos in our Airflow 3.x environment (Cosmos 1.10.2, dbt-core 1.11.x, Astronomer runtime). The airflow deployment is running on EKS in AWS and tasks in airflow are run as KubernetesExecutor (tried with CeleryExecutors and exact same problem was observed). \n\nWhen a Cosmos DbtTaskGroup task fails and is retried, the logs for the failed attempts are not uploaded to S3 (remote logging enabled).\nOnly the logs from the last retry are available in the Airflow UI and on S3.\nThis does not happen with standard Airflow operators (e.g. BashOperator), only with Cosmos DbtTaskGroup. What is weird is that if we choose to put no retry at all, we have the logs the first time. But if we choose to put one retry on the airflow dag, we lose the first attempt but we get the second. During the execution of the tasks, we have the logs because airflow is still pulling them directly from the Task pod. But as soon as finished, if the pod have an error, it crashes before being table to upload any log to S3. So logs of the attempt are lost.\n\nThe pod running the task is stopped abruptly after failure, before logs can be uploaded.\nWe see this error in the logs:\n\nairflow.sdk.api.client.ServerResponseError: Server returned errormessage': 'Not Found', 'detail': {'detail': 'Not Found'}}...File \"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/comms.py\", line 258, in _from_frame\n\nFull stack trace of the failing pod not succeeding to upload its logs to S3 : \n\n{\"timestamp\":\"2025-10-07T09:37:34.134606Z\",\"level\":\"warning\",\"event\":\"The `airflow.models.baseoperator.BaseOperator` attribute is deprecated. Please use `'airflow.sdk.bases.operator.BaseOperator'`.\",\"category\":\"DeprecatedImportWarning\",\"filename\":\"supervisor.py\",\"lineno\":1739,\"logger\":\"py.warnings\"}\n{\"timestamp\":\"2025-10-07T09:37:34.135126Z\",\"level\":\"warning\",\"event\":\"The `airflow.hooks.base.BaseHook` attribute is deprecated. Please use `'airflow.sdk.bases.hook.BaseHook'`.\",\"category\":\"DeprecatedImportWarning\",\"filename\":\"supervisor.py\",\"lineno\":1739,\"logger\":\"py.warnings\"}\n{\"timestamp\":\"2025-10-07T09:37:37.384648Z\",\"level\":\"warning\",\"event\":\"section/key [openlineage/namespace] not found in config\",\"logger\":\"airflow.configuration\",\"filename\":\"supervisor.py\",\"lineno\":1739}\n{\"timestamp\":\"2025-10-07T09:37:37.385521Z\",\"level\":\"warning\",\"event\":\"The `airflow.utils.task_group.TaskGroup` attribute is deprecated. Please use `'airflow.sdk.TaskGroup'`.\",\"category\":\"DeprecatedImportWarning\",\"filename\":\"supervisor.py\",\"lineno\":1739,\"logger\":\"py.warnings\"}\n{\"timestamp\":\"2025-10-07T09:37:37.387229Z\",\"level\":\"warning\",\"event\":\"The `airflow.utils.task_group.TaskGroup` attribute is deprecated. Please use `'airflow.sdk.TaskGroup'`.\",\"category\":\"DeprecatedImportWarning\",\"filename\":\"supervisor.py\",\"lineno\":1739,\"logger\":\"py.warnings\"}\n{\"timestamp\":\"2025-10-07T09:37:37.388410Z\",\"level\":\"warning\",\"event\":\"The `airflow.utils.task_group.TaskGroup` attribute is deprecated. Please use `'airflow.sdk.TaskGroup'`.\",\"category\":\"DeprecatedImportWarning\",\"filename\":\"supervisor.py\",\"lineno\":1739,\"logger\":\"py.warnings\"}\n{\"timestamp\":\"2025-10-07T09:37:37.389432Z\",\"level\":\"warning\",\"event\":\"The `airflow.hooks.base.BaseHook` attribute is deprecated. Please use `'airflow.sdk.bases.hook.BaseHook'`.\",\"category\":\"DeprecatedImportWarning\",\"filename\":\"supervisor.py\",\"lineno\":1739,\"logger\":\"py.warnings\"}\n{\"timestamp\":\"2025-10-07T09:37:37.403079Z\",\"level\":\"warning\",\"event\":\"The `airflow.utils.task_group.TaskGroup` attribute is deprecated. Please use `'airflow.sdk.TaskGroup'`.\",\"category\":\"DeprecatedImportWarning\",\"filename\":\"supervisor.py\",\"lineno\":1739,\"logger\":\"py.warnings\"}\n{\"timestamp\":\"2025-10-07T09:37:37.408536Z\",\"level\":\"warning\",\"event\":\"The `airflow.utils.task_group.TaskGroup` attribute is deprecated. Please use `'airflow.sdk.TaskGroup'`.\",\"category\":\"DeprecatedImportWarning\",\"filename\":\"supervisor.py\",\"lineno\":1739,\"logger\":\"py.warnings\"}\n{\"timestamp\":\"2025-10-07T09:37:37.409877Z\",\"level\":\"warning\",\"event\":\"The `airflow.utils.task_group.TaskGroup` attribute is deprecated. Please use `'airflow.sdk.TaskGroup'`.\",\"category\":\"DeprecatedImportWarning\",\"filename\":\"supervisor.py\",\"lineno\":1739,\"logger\":\"py.warnings\"}\n{\"timestamp\":\"2025-10-07T09:37:37.803284Z\",\"level\":\"warning\",\"event\":\"The `airflow.hooks.base.BaseHook` attribute is deprecated. Please use `'airflow.sdk.bases.hook.BaseHook'`.\",\"category\":\"DeprecatedImportWarning\",\"filename\":\"supervisor.py\",\"lineno\":1739,\"logger\":\"py.warnings\"}\n{\"timestamp\":\"2025-10-07T09:37:38.058447Z\",\"level\":\"warning\",\"event\":\"The `airflow.utils.timezone.utcnow` attribute is deprecated. Please use `'airflow.sdk.timezone.utcnow'`.\",\"category\":\"DeprecatedImportWarning\",\"filename\":\"supervisor.py\",\"lineno\":1739,\"logger\":\"py.warnings\"}\n{\"timestamp\":\"2025-10-07T09:37:38.383665Z\",\"level\":\"info\",\"event\":\"DAG bundles loaded: dags-folder\",\"logger\":\"airflow.dag_processing.bundles.manager.DagBundlesManager\",\"filename\":\"supervisor.py\",\"lineno\":1739}\n{\"timestamp\":\"2025-10-07T09:37:38.383986Z\",\"level\":\"info\",\"event\":\"Filling up the DagBag from /opt/airflow/dags/test/airflow/dags/simple_dag.py\",\"logger\":\"airflow.models.dagbag.DagBag\",\"filename\":\"supervisor.py\",\"lineno\":1739}\n{\"timestamp\":\"2025-10-07T09:37:38.389834Z\",\"level\":\"info\",\"event\":\"Parsing DAG 'simple_dummy_dag' (caller=/opt/airflow/dags/test/airflow/dags/simple_dag.py, env_root=/opt/airflow/dags/test/airflow)\",\"logger\":\"airflow.utils.log.logging_mixin.LoggingMixin\",\"filename\":\"supervisor.py\",\"lineno\":1739}\n{\"timestamp\":\"2025-10-07T09:37:38.389921Z\",\"level\":\"info\",\"event\":\"Selected environment: staging\",\"logger\":\"airflow.utils.log.logging_mixin.LoggingMixin\",\"filename\":\"supervisor.py\",\"lineno\":1739}\n{\"timestamp\":\"2025-10-07T09:37:38.390126Z\",\"level\":\"info\",\"event\":\"Env file candidate: /opt/airflow/dags/test/airflow/.env.staging\",\"logger\":\"airflow.utils.log.logging_mixin.LoggingMixin\",\"filename\":\"supervisor.py\",\"lineno\":1739}\n{\"timestamp\":\"2025-10-07T09:37:38.390578Z\",\"level\":\"info\",\"event\":\"Loaded environment variables from /opt/airflow/dags/test/airflow/.env.staging\",\"logger\":\"airflow.utils.log.logging_mixin.LoggingMixin\",\"filename\":\"supervisor.py\",\"lineno\":1739}\n{\"timestamp\":\"2025-10-07T09:37:43.395721Z\",\"level\":\"info\",\"event\":\"Trying to parse the dbt project using dbt ls cache cosmos_cache__simple_dummy_dag__dbt_task_group...\",\"logger\":\"cosmos.dbt.graph\",\"filename\":\"supervisor.py\",\"lineno\":1739}\n{\"timestamp\":\"2025-10-07T09:37:43.420706Z\",\"level\":\"info\",\"event\":\"Cosmos performance: time to calculate cache identifier cosmos_cache__simple_dummy_dag__dbt_task_group for current version: 0.0005148248746991158\",\"logger\":\"cosmos.cache\",\"filename\":\"supervisor.py\",\"lineno\":1739}\n{\"timestamp\":\"2025-10-07T09:37:43.420837Z\",\"level\":\"info\",\"event\":\"Cosmos performance [simple-dummy-dag-dbt-task-group-issues-run-7p5aps05|23]: The cache size for cosmos_cache__simple_dummy_dag__dbt_task_group is 20132\",\"logger\":\"cosmos.dbt.graph\",\"filename\":\"supervisor.py\",\"lineno\":1739}\n{\"timestamp\":\"2025-10-07T09:37:43.421382Z\",\"level\":\"info\",\"event\":\"Cosmos performance: Cache hit for cosmos_cache__simple_dummy_dag__dbt_task_group - 116fbfec9b8ac0541dc22732080e7505,8503c37d87e09fa9e1f60b8912e6c0c3\",\"logger\":\"cosmos.dbt.graph\",\"filename\":\"supervisor.py\",\"lineno\":1739}\n{\"timestamp\":\"2025-10-07T09:37:43.421689Z\",\"level\":\"info\",\"event\":\"Total nodes: 27\",\"logger\":\"cosmos.dbt.graph\",\"filename\":\"supervisor.py\",\"lineno\":1739}\n{\"timestamp\":\"2025-10-07T09:37:43.421747Z\",\"level\":\"info\",\"event\":\"Total filtered nodes: 27\",\"logger\":\"cosmos.dbt.graph\",\"filename\":\"supervisor.py\",\"lineno\":1739}\n{\"timestamp\":\"2025-10-07T09:37:43.421814Z\",\"level\":\"info\",\"event\":\"Cosmos performance (simple_dummy_dag__dbt_task_group) -  [simple-dummy-dag-dbt-task-group-issues-run-7p5aps05|23]: It took 0.0262s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE\",\"logger\":\"cosmos.converter\",\"filename\":\"supervisor.py\",\"lineno\":1739}\n{\"timestamp\":\"2025-10-07T09:37:43.429799Z\",\"level\":\"info\",\"event\":\"Cosmos performance (simple_dummy_dag__dbt_task_group) - [simple-dummy-dag-dbt-task-group-issues-run-7p5aps05|23]: It took 0.00796s to build the Airflow DAG.\",\"logger\":\"cosmos.converter\",\"filename\":\"supervisor.py\",\"lineno\":1739}\n{\"timestamp\":\"2025-10-07T09:37:43.430664Z\",\"level\":\"info\",\"event\":\"Trying to parse the dbt project using dbt ls cache cosmos_cache__simple_dummy_dag__minimal_dbt_test...\",\"logger\":\"cosmos.dbt.graph\",\"filename\":\"supervisor.py\",\"lineno\":1739}\n{\"timestamp\":\"2025-10-07T09:37:43.440241Z\",\"level\":\"info\",\"event\":\"Cosmos performance: time to calculate cache identifier cosmos_cache__simple_dummy_dag__minimal_dbt_test for current version: 0.00046902894973754883\",\"logger\":\"cosmos.cache\",\"filename\":\"supervisor.py\",\"lineno\":1739}\n{\"timestamp\":\"2025-10-07T09:37:43.440335Z\",\"level\":\"info\",\"event\":\"Cosmos performance [simple-dummy-dag-dbt-task-group-issues-run-7p5aps05|23]: The cache size for cosmos_cache__simple_dummy_dag__minimal_dbt_test is 20132\",\"logger\":\"cosmos.dbt.graph\",\"filename\":\"supervisor.py\",\"lineno\":1739}\n{\"timestamp\":\"2025-10-07T09:37:43.440855Z\",\"level\":\"info\",\"event\":\"Cosmos performance: Cache hit for cosmos_cache__simple_dummy_dag__minimal_dbt_test - 116fbfec9b8ac0541dc22732080e7505,8503c37d87e09fa9e1f60b8912e6c0c3\",\"logger\":\"cosmos.dbt.graph\",\"filename\":\"supervisor.py\",\"lineno\":1739}\n{\"timestamp\":\"2025-10-07T09:37:43.440928Z\",\"level\":\"info\",\"event\":\"Total nodes: 27\",\"logger\":\"cosmos.dbt.graph\",\"filename\":\"supervisor.py\",\"lineno\":1739}\n{\"timestamp\":\"2025-10-07T09:37:43.440971Z\",\"level\":\"info\",\"event\":\"Total filtered nodes: 27\",\"logger\":\"cosmos.dbt.graph\",\"filename\":\"supervisor.py\",\"lineno\":1739}\n{\"timestamp\":\"2025-10-07T09:37:43.441026Z\",\"level\":\"info\",\"event\":\"Cosmos performance (simple_dummy_dag__minimal_dbt_test) -  [simple-dummy-dag-dbt-task-group-issues-run-7p5aps05|23]: It took 0.0104s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE\",\"logger\":\"cosmos.converter\",\"filename\":\"supervisor.py\",\"lineno\":1739}\n{\"timestamp\":\"2025-10-07T09:37:43.446371Z\",\"level\":\"info\",\"event\":\"Cosmos performance (simple_dummy_dag__minimal_dbt_test) - [simple-dummy-dag-dbt-task-group-issues-run-7p5aps05|23]: It took 0.00532s to build the Airflow DAG.\",\"logger\":\"cosmos.converter\",\"filename\":\"supervisor.py\",\"lineno\":1739}\n{\"timestamp\":\"2025-10-07T09:37:43.447464Z\",\"level\":\"warning\",\"event\":\"The `airflow.hooks.base.BaseHook` attribute is deprecated. Please use `'airflow.sdk.bases.hook.BaseHook'`.\",\"category\":\"DeprecatedImportWarning\",\"filename\":\"supervisor.py\",\"lineno\":1739,\"logger\":\"py.warnings\"}\n{\"timestamp\":\"2025-10-07T09:37:43.447542Z\",\"level\":\"warning\",\"event\":\"The `airflow.operators.bash.BashOperator` attribute is deprecated. Please use `'airflow.providers.standard.operators.bash.BashOperator'`.\",\"category\":\"DeprecatedImportWarning\",\"filename\":\"supervisor.py\",\"lineno\":1739,\"logger\":\"py.warnings\"}\n{\"timestamp\":\"2025-10-07T09:37:43.447592Z\",\"level\":\"warning\",\"event\":\"Pydantic serializer warnings:\\n  PydanticSerializationUnexpectedValue(Expected `str` - serialized value may not be as expected [input_value={'version': '116fbfec9b8a...p_id': 'dbt_task_group'}, input_type=dict])\",\"category\":\"UserWarning\",\"filename\":\"supervisor.py\",\"lineno\":1739,\"logger\":\"py.warnings\"}\n{\"timestamp\":\"2025-10-07T09:37:43.447633Z\",\"level\":\"warning\",\"event\":\"Pydantic serializer warnings:\\n  PydanticSerializationUnexpectedValue(Expected `str` - serialized value may not be as expected [input_value={'version': '116fbfec9b8a...id': 'minimal_dbt_test'}, input_type=dict])\",\"category\":\"UserWarning\",\"filename\":\"supervisor.py\",\"lineno\":1739,\"logger\":\"py.warnings\"}\n{\"timestamp\":\"2025-10-07T09:37:44.835755Z\",\"level\":\"info\",\"event\":\"dbtRunner is available. Using dbtRunner for invoking dbt.\",\"logger\":\"cosmos.operators.base\",\"filename\":\"supervisor.py\",\"lineno\":1739}\n{\"timestamp\":\"2025-10-07T09:37:44.836090Z\",\"level\":\"info\",\"event\":\"Cloning project to writable temp directory /tmp/tmptfu0b0kt from /opt/airflow/dags/test/dbt/test\",\"logger\":\"cosmos.operators.base\",\"filename\":\"supervisor.py\",\"lineno\":1739}\n{\"timestamp\":\"2025-10-07T09:37:44.838856Z\",\"level\":\"info\",\"event\":\"Partial parse is enabled and the latest partial parse file is None\",\"logger\":\"cosmos.operators.base\",\"filename\":\"supervisor.py\",\"lineno\":1739}\n{\"timestamp\":\"2025-10-07T09:37:44.839055Z\",\"level\":\"info\",\"event\":\"Using user-supplied profiles.yml at /opt/airflow/dags/test/dbt/test/profiles.yml\",\"logger\":\"cosmos.config\",\"filename\":\"supervisor.py\",\"lineno\":1739}\n{\"timestamp\":\"2025-10-07T09:37:44.840093Z\",\"level\":\"info\",\"event\":\"Trying to run dbtRunner with:\\n ['deps', '--project-dir', '/tmp/tmptfu0b0kt', '--profiles-dir', '/opt/airflow/dags/test/dbt/test', '--profile', 'test', '--target', 'staging', '--no-static-parser']\\n in /tmp/tmptfu0b0kt\",\"logger\":\"cosmos.dbt.runner\",\"filename\":\"supervisor.py\",\"lineno\":1739}\n{\"timestamp\":\"2025-10-07T09:37:44.895441Z\",\"level\":\"info\",\"event\":\"\\u001b[0m09:37:44  Running with dbt=1.11.0-b2\",\"logger\":\"task.stdout\",\"filename\":\"supervisor.py\",\"lineno\":1754}\n{\"timestamp\":\"2025-10-07T09:37:45.515707Z\",\"level\":\"info\",\"event\":\"\\u001b[0m09:37:45  Updating lock file in file path: /tmp/tmptfu0b0kt/package-lock.yml\",\"logger\":\"task.stdout\",\"filename\":\"supervisor.py\",\"lineno\":1754}\n{\"timestamp\":\"2025-10-07T09:37:45.523362Z\",\"level\":\"info\",\"event\":\"\\u001b[0m09:37:45  Installing /opt/airflow/commons/dbt/data_eng_dbt\",\"logger\":\"task.stdout\",\"filename\":\"supervisor.py\",\"lineno\":1754}\n{\"timestamp\":\"2025-10-07T09:37:45.524967Z\",\"level\":\"info\",\"event\":\"\\u001b[0m09:37:45  Installed from <local @ /opt/airflow/commons/dbt/data_eng_dbt>\",\"logger\":\"task.stdout\",\"filename\":\"supervisor.py\",\"lineno\":1754}\n{\"timestamp\":\"2025-10-07T09:37:45.525726Z\",\"level\":\"info\",\"event\":\"\\u001b[0m09:37:45  Installing entechlog/dbt_snow_mask\",\"logger\":\"task.stdout\",\"filename\":\"supervisor.py\",\"lineno\":1754}\n{\"timestamp\":\"2025-10-07T09:37:45.765544Z\",\"level\":\"info\",\"event\":\"\\u001b[0m09:37:45  Installed from version 0.2.7\",\"logger\":\"task.stdout\",\"filename\":\"supervisor.py\",\"lineno\":1754}\n{\"timestamp\":\"2025-10-07T09:37:45.765723Z\",\"level\":\"info\",\"event\":\"\\u001b[0m09:37:45  Up to date!\",\"logger\":\"task.stdout\",\"filename\":\"supervisor.py\",\"lineno\":1754}\n{\"timestamp\":\"2025-10-07T09:37:45.766406Z\",\"level\":\"info\",\"event\":\"\\u001b[0m09:37:45  Installing get-select/dbt_snowflake_query_tags\",\"logger\":\"task.stdout\",\"filename\":\"supervisor.py\",\"lineno\":1754}\n{\"timestamp\":\"2025-10-07T09:37:45.930474Z\",\"level\":\"info\",\"event\":\"\\u001b[0m09:37:45  Installed from version 2.5.1\",\"logger\":\"task.stdout\",\"filename\":\"supervisor.py\",\"lineno\":1754}\n{\"timestamp\":\"2025-10-07T09:37:45.930654Z\",\"level\":\"info\",\"event\":\"\\u001b[0m09:37:45  Updated version available: 2.5.3\",\"logger\":\"task.stdout\",\"filename\":\"supervisor.py\",\"lineno\":1754}\n{\"timestamp\":\"2025-10-07T09:37:45.931518Z\",\"level\":\"info\",\"event\":\"\\u001b[0m09:37:45  Installing dbt-labs/dbt_utils\",\"logger\":\"task.stdout\",\"filename\":\"supervisor.py\",\"lineno\":1754}\n{\"timestamp\":\"2025-10-07T09:37:46.129733Z\",\"level\":\"info\",\"event\":\"\\u001b[0m09:37:46  Installed from version 1.3.1\",\"logger\":\"task.stdout\",\"filename\":\"supervisor.py\",\"lineno\":1754}\n{\"timestamp\":\"2025-10-07T09:37:46.129896Z\",\"level\":\"info\",\"event\":\"\\u001b[0m09:37:46  Up to date!\",\"logger\":\"task.stdout\",\"filename\":\"supervisor.py\",\"lineno\":1754}\n{\"timestamp\":\"2025-10-07T09:37:46.130716Z\",\"level\":\"info\",\"event\":\"\\u001b[0m09:37:46\",\"logger\":\"task.stdout\",\"filename\":\"supervisor.py\",\"lineno\":1754}\n{\"timestamp\":\"2025-10-07T09:37:46.131005Z\",\"level\":\"info\",\"event\":\"\\u001b[0m09:37:46  Updates available for packages: ['get-select/dbt_snowflake_query_tags']\",\"logger\":\"task.stdout\",\"filename\":\"supervisor.py\",\"lineno\":1754}\n{\"timestamp\":\"2025-10-07T09:37:46.131067Z\",\"level\":\"info\",\"event\":\"Update your versions in packages.yml, then run dbt deps\",\"logger\":\"task.stdout\",\"filename\":\"supervisor.py\",\"lineno\":1754}\n{\"timestamp\":\"2025-10-07T09:37:46.495431Z\",\"level\":\"info\",\"event\":\"Trying to run dbtRunner with:\\n ['run', '--models', 'issues', '--project-dir', '/tmp/tmptfu0b0kt', '--profiles-dir', '/opt/airflow/dags/test/dbt/test', '--profile', 'test', '--target', 'staging', '--no-static-parser']\\n in /tmp/tmptfu0b0kt\",\"logger\":\"cosmos.dbt.runner\",\"filename\":\"supervisor.py\",\"lineno\":1739}\n{\"timestamp\":\"2025-10-07T09:37:46.500154Z\",\"level\":\"info\",\"event\":\"\\u001b[0m09:37:46  Running with dbt=1.11.0-b2\",\"logger\":\"task.stdout\",\"filename\":\"supervisor.py\",\"lineno\":1754}\n{\"timestamp\":\"2025-10-07T09:37:46.500980Z\",\"level\":\"info\",\"event\":\"\\u001b[0m09:37:46  [\\u001b[33mWARNING\\u001b[0m]: Deprecated functionality\",\"logger\":\"task.stdout\",\"filename\":\"supervisor.py\",\"lineno\":1754}\n{\"timestamp\":\"2025-10-07T09:37:46.501046Z\",\"level\":\"info\",\"event\":\"Usage of `--models`, `--model`, and `-m` is deprecated in favor of `--select` or\",\"logger\":\"task.stdout\",\"filename\":\"supervisor.py\",\"lineno\":1754}\n{\"timestamp\":\"2025-10-07T09:37:46.501118Z\",\"level\":\"info\",\"event\":\"`-s`.\",\"logger\":\"task.stdout\",\"filename\":\"supervisor.py\",\"lineno\":1754}\n{\"timestamp\":\"2025-10-07T09:37:47.898632Z\",\"level\":\"info\",\"event\":\"\\u001b[0m09:37:47  Registered adapter: snowflake=1.10.2\",\"logger\":\"task.stdout\",\"filename\":\"supervisor.py\",\"lineno\":1754}\n{\"timestamp\":\"2025-10-07T09:37:48.270847Z\",\"level\":\"info\",\"event\":\"\\u001b[0m09:37:48  Unable to do partial parsing because saved manifest not found. Starting full parse.\",\"logger\":\"task.stdout\",\"filename\":\"supervisor.py\",\"lineno\":1754}\n{\"timestamp\":\"2025-10-07T09:37:49.739177Z\",\"level\":\"info\",\"event\":\"\\u001b[0m09:37:49  Found 5 models, 22 data tests, 625 macros\",\"logger\":\"task.stdout\",\"filename\":\"supervisor.py\",\"lineno\":1754}\n{\"timestamp\":\"2025-10-07T09:37:49.752263Z\",\"level\":\"info\",\"event\":\"\\u001b[0m09:37:49\",\"logger\":\"task.stdout\",\"filename\":\"supervisor.py\",\"lineno\":1754}\n{\"timestamp\":\"2025-10-07T09:37:49.752374Z\",\"level\":\"info\",\"event\":\"\\u001b[0m09:37:49  Concurrency: 16 threads (target='staging')\",\"logger\":\"task.stdout\",\"filename\":\"supervisor.py\",\"lineno\":1754}\n{\"timestamp\":\"2025-10-07T09:37:49.752445Z\",\"level\":\"info\",\"event\":\"\\u001b[0m09:37:49\",\"logger\":\"task.stdout\",\"filename\":\"supervisor.py\",\"lineno\":1754}\n{\"timestamp\":\"2025-10-07T09:37:49.768383Z\",\"level\":\"info\",\"event\":\"\\u001b[0m09:37:49\",\"logger\":\"task.stdout\",\"filename\":\"supervisor.py\",\"lineno\":1754}\n{\"timestamp\":\"2025-10-07T09:37:49.768650Z\",\"level\":\"info\",\"event\":\"\\u001b[0m09:37:49  Finished running  in 0 hours 0 minutes and 0.03 seconds (0.03s).\",\"logger\":\"task.stdout\",\"filename\":\"supervisor.py\",\"lineno\":1754}\n{\"timestamp\":\"2025-10-07T09:37:49.769138Z\",\"level\":\"info\",\"event\":\"\\u001b[0m09:37:49  Encountered an error:\",\"logger\":\"task.stdout\",\"filename\":\"supervisor.py\",\"lineno\":1754}\n{\"timestamp\":\"2025-10-07T09:37:49.769208Z\",\"level\":\"info\",\"event\":\"Runtime Error\",\"logger\":\"task.stdout\",\"filename\":\"supervisor.py\",\"lineno\":1754}\n{\"timestamp\":\"2025-10-07T09:37:49.769273Z\",\"level\":\"info\",\"event\":\"  Database error while listing schemas in database \\\"TEST_STAGING\\\"\",\"logger\":\"task.stdout\",\"filename\":\"supervisor.py\",\"lineno\":1754}\n{\"timestamp\":\"2025-10-07T09:37:49.769338Z\",\"level\":\"info\",\"event\":\"  Database Error\",\"logger\":\"task.stdout\",\"filename\":\"supervisor.py\",\"lineno\":1754}\n{\"timestamp\":\"2025-10-07T09:37:49.769402Z\",\"level\":\"info\",\"event\":\"    ('Could not deserialize key data. The data may be in an incorrect format, it may be encrypted with an unsupported algorithm, or it may be an unsupported key type (e.g. EC curves with explicit parameters).', [<OpenSSLError(code=503841036, lib=60, reason=524556, reason_text=unsupported)>])\",\"logger\":\"task.stdout\",\"filename\":\"supervisor.py\",\"lineno\":1754}\n{\"timestamp\":\"2025-10-07T09:37:49.769646Z\",\"level\":\"info\",\"event\":\"\\u001b[0m09:37:49  [\\u001b[33mWARNING\\u001b[0m][DeprecationsSummary]: Deprecated functionality\",\"logger\":\"task.stdout\",\"filename\":\"supervisor.py\",\"lineno\":1754}\n{\"timestamp\":\"2025-10-07T09:37:49.769711Z\",\"level\":\"info\",\"event\":\"Summary of encountered deprecations:\",\"logger\":\"task.stdout\",\"filename\":\"supervisor.py\",\"lineno\":1754}\n{\"timestamp\":\"2025-10-07T09:37:49.769775Z\",\"level\":\"info\",\"event\":\"- ModelParamUsageDeprecation: 1 occurrence\",\"logger\":\"task.stdout\",\"filename\":\"supervisor.py\",\"lineno\":1754}\n{\"timestamp\":\"2025-10-07T09:37:49.769857Z\",\"level\":\"info\",\"event\":\"To see all deprecation instances instead of just the first occurrence of each,\",\"logger\":\"task.stdout\",\"filename\":\"supervisor.py\",\"lineno\":1754}\n{\"timestamp\":\"2025-10-07T09:37:49.769919Z\",\"level\":\"info\",\"event\":\"run command again with the `--show-all-deprecations` flag. You may also need to\",\"logger\":\"task.stdout\",\"filename\":\"supervisor.py\",\"lineno\":1754}\n{\"timestamp\":\"2025-10-07T09:37:49.769998Z\",\"level\":\"info\",\"event\":\"run with `--no-partial-parse` as some deprecations are only encountered during\",\"logger\":\"task.stdout\",\"filename\":\"supervisor.py\",\"lineno\":1754}\n{\"timestamp\":\"2025-10-07T09:37:49.770073Z\",\"level\":\"info\",\"event\":\"parsing.\",\"logger\":\"task.stdout\",\"filename\":\"supervisor.py\",\"lineno\":1754}\n{\"timestamp\":\"2025-10-07T09:37:50.159921Z\",\"level\":\"error\",\"event\":\"Task failed with exception\",\"logger\":\"task\",\"filename\":\"supervisor.py\",\"lineno\":1739,\"error_detail\":[{\"exc_type\":\"CosmosDbtRunError\",\"exc_value\":\"dbt invocation did not complete with unhandled error: Runtime Error\\n  Database error while listing schemas in database \\\"TEST_STAGING\\\"\\n  Database Error\\n    ('Could not deserialize key data. The data may be in an incorrect format, it may be encrypted with an unsupported algorithm, or it may be an unsupported key type (e.g. EC curves with explicit parameters).', [<OpenSSLError(code=503841036, lib=60, reason=524556, reason_text=unsupported)>])\",\"exc_notes\":[],\"syntax_error\":null,\"is_cause\":false,\"frames\":[{\"filename\":\"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py\",\"lineno\":920,\"name\":\"run\"},{\"filename\":\"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py\",\"lineno\":1307,\"name\":\"_execute_task\"},{\"filename\":\"/home/airflow/.local/lib/python3.12/site-packages/cosmos/operators/base.py\",\"lineno\":310,\"name\":\"execute\"},{\"filename\":\"/home/airflow/.local/lib/python3.12/site-packages/cosmos/operators/local.py\",\"lineno\":853,\"name\":\"build_and_run_cmd\"},{\"filename\":\"/home/airflow/.local/lib/python3.12/site-packages/cosmos/operators/local.py\",\"lineno\":650,\"name\":\"run_command\"},{\"filename\":\"/home/airflow/.local/lib/python3.12/site-packages/cosmos/operators/local.py\",\"lineno\":258,\"name\":\"handle_exception_dbt_runner\"},{\"filename\":\"/home/airflow/.local/lib/python3.12/site-packages/cosmos/dbt/runner.py\",\"lineno\":109,\"name\":\"handle_exception_if_needed\"}],\"is_group\":false,\"exceptions\":[]}]}\n{\"timestamp\":\"2025-10-07T09:37:50.203777Z\",\"level\":\"error\",\"event\":\"API server error\",\"status_code\":404,\"detail\":{\"detail\":\"Not Found\"},\"message\":\"Not Found\",\"logger\":\"supervisor\",\"filename\":\"supervisor.py\",\"lineno\":616}\n{\"timestamp\":\"2025-10-07T09:37:50.204113Z\",\"level\":\"error\",\"event\":\"Top level error\",\"logger\":\"task\",\"filename\":\"supervisor.py\",\"lineno\":1739,\"error_detail\":[{\"exc_type\":\"AirflowRuntimeError\",\"exc_value\":\"API_SERVER_ERROR: {'status_code': 404, 'message': 'Not Found', 'detail': {'detail': 'Not Found'}}\",\"exc_notes\":[],\"syntax_error\":null,\"is_cause\":false,\"frames\":[{\"filename\":\"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py\",\"lineno\":1452,\"name\":\"main\"},{\"filename\":\"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py\",\"lineno\":1397,\"name\":\"finalize\"},{\"filename\":\"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/comms.py\",\"lineno\":207,\"name\":\"send\"},{\"filename\":\"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/comms.py\",\"lineno\":271,\"name\":\"_get_response\"},{\"filename\":\"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/comms.py\",\"lineno\":258,\"name\":\"_from_frame\"}],\"is_group\":false,\"exceptions\":[]}]}\n{\"timestamp\":\"2025-10-07T09:37:50.244332Z\",\"level\":\"error\",\"event\":\"/usr/python/lib/python3.12/multiprocessing/resource_tracker.py:279: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown\",\"logger\":\"task.stderr\",\"filename\":\"supervisor.py\",\"lineno\":1754}\n{\"timestamp\":\"2025-10-07T09:37:50.244508Z\",\"level\":\"error\",\"event\":\"  warnings.warn('resource_tracker: There appear to be %d '\",\"logger\":\"task.stderr\",\"filename\":\"supervisor.py\",\"lineno\":1754}\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/execute_workload.py\", line 125, in <module>\n    main()\n  File \"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/execute_workload.py\", line 121, in main\n    execute_workload(workload)\n  File \"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/execute_workload.py\", line 66, in execute_workload\n    supervise(\n  File \"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/supervisor.py\", line 1887, in supervise\n    exit_code = process.wait()\n                ^^^^^^^^^^^^^^\n  File \"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/supervisor.py\", line 988, in wait\n    self.update_task_state_if_needed()\n  File \"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/supervisor.py\", line 1002, in update_task_state_if_needed\n    self.client.task_instances.finish(\n  File \"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/api/client.py\", line 221, in finish\n    self.client.patch(f\"task-instances/{id}/state\", content=body.model_dump_json())\n  File \"/home/airflow/.local/lib/python3.12/site-packages/httpx/_client.py\", line 1218, in patch\n    return self.request(\n           ^^^^^^^^^^^^^\n  File \"/home/airflow/.local/lib/python3.12/site-packages/tenacity/__init__.py\", line 338, in wrapped_f\n    return copy(f, *args, **kw)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/airflow/.local/lib/python3.12/site-packages/tenacity/__init__.py\", line 477, in __call__\n    do = self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/airflow/.local/lib/python3.12/site-packages/tenacity/__init__.py\", line 378, in iter\n    result = action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/home/airflow/.local/lib/python3.12/site-packages/tenacity/__init__.py\", line 400, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/python/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/python/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/home/airflow/.local/lib/python3.12/site-packages/tenacity/__init__.py\", line 480, in __call__\n    result = fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/api/client.py\", line 861, in request\n    return super().request(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/airflow/.local/lib/python3.12/site-packages/httpx/_client.py\", line 825, in request\n    return self.send(request, auth=auth, follow_redirects=follow_redirects)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/airflow/.local/lib/python3.12/site-packages/httpx/_client.py\", line 914, in send\n    response = self._send_handling_auth(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/airflow/.local/lib/python3.12/site-packages/httpx/_client.py\", line 942, in _send_handling_auth\n    response = self._send_handling_redirects(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/airflow/.local/lib/python3.12/site-packages/httpx/_client.py\", line 999, in _send_handling_redirects\n    raise exc\n  File \"/home/airflow/.local/lib/python3.12/site-packages/httpx/_client.py\", line 982, in _send_handling_redirects\n    hook(response)\n  File \"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/api/client.py\", line 181, in raise_on_4xx_5xx_with_note\n    return get_json_error(response) or response.raise_for_status()\n           ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/api/client.py\", line 171, in get_json_error\n    raise err\nairflow.sdk.api.client.ServerResponseError: Server returned error\n\n### What you think should happen instead?\n\nEven if the task fail, for each retries, I should always get the logs available in the UI. \n\n### How to reproduce\n\n\n 1.   Create a DbtTaskGroup with a failing dbt model and retries > 0\n 2.   Set the tasks as KubernetesExecutors\n 3.   Choose S3 for remote logging with airflow\n 4.   Trigger the DAG\n 5.   Observe that only the last retry's logs are available in S3/UI\n\n\n### Operating System\n\nDebian GNU/Linux 12 (bookworm)\n\n### Versions of Apache Airflow Providers\n\n_No response_\n\n### Deployment\n\nOfficial Apache Airflow Helm Chart\n\n### Deployment details\n\n_No response_\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56500\nTitle: AttributeError: 'pydantic_core._pydantic_core.ValidationInfo' object has no attribute 'data_interval_start'\nState: closed\nAuthor: ToneVDB\nLabels: kind:bug, area:API, area:core, needs-triage\nBody:\n### Apache Airflow version\n\n3.1.0\n\n### If \"Other Airflow 2 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\nWhen triggering a DAG from the UI or through the API i get the following error and stacktrace:\n``` bash\n  File \"/home/airflow/.local/lib/python3.12/site-packages/fastapi/dependencies/utils.py\", line 674, in solve_dependencies\n\n    ) = await request_body_to_args(  # body_params checked above\n\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/home/airflow/.local/lib/python3.12/site-packages/fastapi/dependencies/utils.py\", line 938, in request_body_to_args\n\n    v_, errors_ = _validate_value_with_model_field(\n\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/home/airflow/.local/lib/python3.12/site-packages/fastapi/dependencies/utils.py\", line 714, in _validate_value_with_model_field\n\n    v_, errors_ = field.validate(value, values, loc=loc)\n\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/home/airflow/.local/lib/python3.12/site-packages/fastapi/_compat.py\", line 142, in validate\n\n    self._type_adapter.validate_python(value, from_attributes=True),\n\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/home/airflow/.local/lib/python3.12/site-packages/pydantic/type_adapter.py\", line 441, in validate_python\n\n    return self.validator.validate_python(\n\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/home/airflow/.local/lib/python3.12/site-packages/airflow/api_fastapi/core_api/datamodels/dag_run.py\", line 103, in check_data_intervals\n\n    if (values.data_interval_start is None) != (values.data_interval_end is None):\n\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nAttributeError: 'pydantic_core._pydantic_core.ValidationInfo' object has no attribute 'data_interval_start'\n```\n\n### What you think should happen instead?\n\nI expect the dag to be triggered, adding or removing the field from the dag content also does not help\n\n### How to reproduce\n\napache-airflow = [\n    {version= \"3.1.0\", extras=[\"pandas\"]}\n] \nflask = \"<3.0\"\nwerkzeug = \"<2.3\"\n\n\n### Operating System\n\nFROM apache/airflow:3.1.0\n\n### Versions of Apache Airflow Providers\n\n_No response_\n\n### Deployment\n\nDocker-Compose\n\n### Deployment details\n\n_No response_\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56498\nTitle: Missing postgres docker images\nState: closed\nAuthor: jackraymund\nLabels: kind:bug, area:helm-chart, needs-triage\nBody:\n### Official Helm Chart version\n\n1.15.0\n\n### Apache Airflow version\n\n2.9.3\n\n### Kubernetes Version\n\n1.32.4\n\n### Helm Chart configuration\n\n_No response_\n\n### Docker Image customizations\n\n_No response_\n\n### What happened\n\nAfter restart I cannot make airflow works, because of missing postgres image. \n\n### What you think should happen instead\n\nImage should be on repo.\n\n\n### How to reproduce\n\ndocker pull docker.io/bitnami/postgresql:16.1.0-debian-11-r15                                                                                                                                                                                        130\nError response from daemon: manifest for bitnami/postgresql:16.1.0-debian-11-r15 not found: manifest unknown: manifest unknown\n\n### Anything else\n\nHow I can change this image? There is all images available to change in values.yaml except postgres. And what version of postgres should i use?\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56497\nTitle: Dag processor loses connection to db very often\nState: open\nAuthor: rcampos87\nLabels: kind:bug, area:core, needs-triage\nBody:\n### Apache Airflow version\n\n3.1.0\n\n### If \"Other Airflow 2 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\nIn over 6days, the dag processor has restarted 235 times due to \n```\n\u2502\u2502 dag-processor     _mysql.connection.query(self, query)            \n\u2502\u2502 dag-processor sqlalchemy.exc.OperationalError: (MySQLdb.OperationalError) (2013, 'Lost connection to server during query')           \n ```\n\n### What you think should happen instead?\n\nDag processor should handle loss of connection appropriately\n\n### How to reproduce\n\nDeploy airflow 3.1.0 in k8s with mysql as db\n\n### Operating System\n\nDebian bookworm\n\n### Versions of Apache Airflow Providers\n\n_No response_\n\n### Deployment\n\nOfficial Apache Airflow Helm Chart\n\n### Deployment details\n\n```\ndagProcessor:\n  enabled: true\n  replicas: 1\n\n  resources:\n    limits:\n      memory: 1500Mi\n\n  logGroomerSidecar:\n    retentionDays: 3\n```\n\n### Anything else?\nSame happens in 3.0.6 too\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56496\nTitle: Improve formatting for Codespaces setup instructions in 03a_contributors_quick_start_beginners.rst file\nState: closed\nAuthor: KoviAnusha\nLabels: area:dev-tools, backport-to-v3-1-test\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n1. Update the \"Option B \u2013 One-Click GitHub Codespaces\" section to enhance readability and clarity. \r\n2. Use proper markdown code block formatting for the Docker command, consistent spacing, and clearer step numbering. \r\n\r\nThis improves comprehension for users setting up Codespaces and installing Docker Buildx/Compose for Breeze.\r\n\r\n**Tests:**\r\n\r\n<img width=\"472\" height=\"371\" alt=\"image\" src=\"https://github.com/user-attachments/assets/94449a15-66d5-4435-9453-35de7fb3f54a\" /> <img width=\"501\" height=\"290\" alt=\"image\" src=\"https://github.com/user-attachments/assets/9e4b3299-f7f9-4416-bb18-16c467af980a\" />\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56495\nTitle: Enable PT011 rule to prvoider tests\nState: closed\nAuthor: xchwan\nLabels: area:providers, provider:common-sql, provider:common-io, provider:common-compat, provider:common-messaging\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\nissue: Enable Even More PyDocStyle Checks #40567\r\n@ferruzzi\r\nThis PR is for enable PT011 rule:\r\nPT011: all instances of pytest.raises should include a match term to make sure they are catching the right error.\r\nhttps://docs.astral.sh/ruff/rules/pytest-raises-too-broad/\r\nThere are 102 files changes is need.\r\nSo, I separate to many PR, which contains about 5 file changes for easy review.\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56494\nTitle: airflow.providers.apache.kafka ConsumeFromTopicOperator cannot be used for pipelining because it returns None.\nState: open\nAuthor: JosephCatrambone-Gridware\nLabels: kind:bug, area:providers, kind:feature, provider:apache-kafka, AIP-82\nBody:\n### Description\n\nAs of Airflow 3.0.6, the ConsumeFromTopicOperator takes the name+path to a function and applies it to all messages in a batch, but the results of this function call are discarded and the method returns None.  Rather than dropping the result of the `apply_function`, the results of the apply_function could be accumulated and returned so that other tasks could use the results.\n\n### Use case/motivation\n\nIt would be nice to be able to pipeline operators and functions like we can with other Python tasks and other Operators.  An example:\n\n```python\ndef my_service_prefilter(message):\n    # This will get messages from my-topic-1 and my-topic-2.  We will extract and return some dicts.\n    if message and message.value():\n        decoded = json.loads(message.value().decode(\"utf-8\")\n        if decoded['some_value']:\n            return decoded\n    return None\n\n@task.python\ndef handle_batch(decoded_items: list[dict]):\n    for item in decoded_items:\n        if item:\n            do_something_with_item(item)\n\nmessage_batch = ConsumeFromTopicOperator(\n    task_id=\"consume_pending_kafka_messages\",\n    topics=[\"my-topic-1\", \"my-topic-2\", ],\n    apply_function=\"my_service.prefilter_function\",\n)\n\nmessage_batch >> handle_batch\n```\n\nWritten differently:\n\nKafka message stream -> ConsumeFromTopicOperator -> [message returned by the apply_function * batch size] -> whatever downstream task.\n\nI initially wrote this as a bug report after spending a while with a colleague who had treated the ConsumeFromTopicOperator like other operators, but realized that returning None is the intended behavior.  \n\nI recognize that the Airflow documentation explicitly states that Airflow is _not_ made to be used in the streaming data sense, but in this case if we need to transform some data and distribute it to other tasks we need to build our own operators instead of just reusing the existing Operators.  The Operator documentation seems to suggest that Operators be composable.\n\n### Related issues\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56493\nTitle: Add ability to disable apiserver from the helm chart using apiServer.\u2026\nState: open\nAuthor: bluek1te\nLabels: area:helm-chart\nBody:\n\u2026enabled\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\nFrom https://github.com/apache/airflow/issues/56492, add the ability to disable the api server so that we can create a deployment that only creates workers (for remote execution with airflow 3.0+)",
  "Requirement ID: ISSUE-56492\nTitle: Add ability to set enabled: false for the apiServer in helm chart\nState: open\nAuthor: bluek1te\nLabels: kind:feature, area:helm-chart, needs-triage\nBody:\n### Description\n\nAdd ability to set enabled: false to the apiserver configuration in the helm chart\n\n### Use case/motivation\n\nIt would be great if we could use the helm chart to only deploy an instance of a worker for remote execution on an external cluster. We would expect the api server to be deployed with a separate configuration of the helm chart. It is currently possible to disable the webserver (for Airflow < 3.0.0) so I think it would be great if we could do it for the api server as well.\n\n### Related issues\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [x] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56491\nTitle: [v3-1-test] Bump uv version to 0.9.0 and ruff to 0.14.0 (#56467)\nState: closed\nAuthor: ashb\nLabels: area:dev-tools, kind:documentation, area:production-image, backport-to-v3-1-test\nBody:\nCherry picked from commit 0ec6653d304a2c3a965bb1efcf64201308774a61)\r\n\r\nCo-authored-by: Amogh Desai <amoghrajesh1999@gmail.com>\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56490\nTitle: Add @sage-ingle-ck to Credit Karma contributors in INTHEWILD.md\nState: closed\nAuthor: sage-ingle-ck\nLabels: \nBody:\nAirflow Summit 2025 Contributor's Workshop submission.",
  "Requirement ID: ISSUE-56489\nTitle: [v3-1-test] Fix broken main after pydantic 2.12.0 (#56483)\nState: closed\nAuthor: github-actions[bot]\nLabels: area:airflow-ctl\nBody:\n* Fix broken main after pydantic 2.12.0\n\n* Fix broken main after pydantic 2.12.0\n(cherry picked from commit 463d51c885c010d3650d36be481d3135aed07b15)\n\nCo-authored-by: Jens Scheffler <95105677+jscheffl@users.noreply.github.com>",
  "Requirement ID: ISSUE-56488\nTitle: [v3-1-test] 56058: Validating latestRun before accessing its properties (#56303)\nState: closed\nAuthor: github-actions[bot]\nLabels: area:UI\nBody:\n* 56058: Checking for latestRun before invoking isStatePending\n\n* Fixed formatting\n\n---------\n(cherry picked from commit 6821ad11106f3790a44a265f138d7d2155b29093)\n\nCo-authored-by: Vivek Nanda <viveknanda1984@yahoo.com>\nCo-authored-by: Vivek Nanda <vnanda@cloudera.com>",
  "Requirement ID: ISSUE-56486\nTitle: Use UI snapshot testing to generate docs screenshots\nState: open\nAuthor: bbovenzi\nLabels: kind:documentation, kind:meta, area:UI\nBody:\n### Body\n\nIt is always a pain to manually update screenshots whenever we make a UI change.\nWe also need to expand our test coverage.\n\n\nLet's combine snapshot tests to generate images for each of our docs screenshots. We will need to list out all of the screenshots we generate, figure out if we're rendering a full page or a single component, and mock all API requests that view uses. The mock data will need to reflect real example use cases.\n\nVitest snapshots: https://vitest.dev/guide/snapshot\n\n### Committer\n\n- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",
  "Requirement ID: ISSUE-56485\nTitle: [v3-1-test] Allow sub-pages in React UI plugins (#56413)\nState: closed\nAuthor: github-actions[bot]\nLabels: area:UI\nBody:\n(cherry picked from commit a75359921267b64fe3bb03bd499c02781e327ec6)\n\nCo-authored-by: Jens Scheffler <95105677+jscheffl@users.noreply.github.com>",
  "Requirement ID: ISSUE-56484\nTitle: [v3-1-test] feat: make clipboard hover (#56382)\nState: closed\nAuthor: github-actions[bot]\nLabels: area:UI\nBody:\n(cherry picked from commit 0dca308dc58a8025ab299fbc06dfa368e393e62d)\n\nCo-authored-by: LI,JHE-CHEN <103923510+RoyLee1224@users.noreply.github.com>",
  "Requirement ID: ISSUE-56483\nTitle: Fix broken main after pydantic 2.12.0\nState: closed\nAuthor: jscheffl\nLabels: area:airflow-ctl, backport-to-v3-1-test\nBody:\nPydantic 2.12.0 broke main/canary runs as well as makes many CI Pre merge checks fail\r\n\r\nHotfix to exclude pydantic 2.12.0, a better solution needs to be made as recorded in https://github.com/apache/airflow/issues/56482",
  "Requirement ID: ISSUE-56482\nTitle: Fix pydantic dependencies in CI codegen for airflowctl\nState: open\nAuthor: jscheffl\nLabels: kind:bug, area:CI, area:airflow-ctl\nBody:\n### Apache Airflow version\n\nmain (development)\n\n### If \"Other Airflow 2 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\nSee https://github.com/apache/airflow/actions/runs/18346568868/job/52258527242\n\nMain broke after Pydantic 2.12.0 was released inside codegen\n\n### What you think should happen instead?\n\nShould not fail\nAlso should not source project dependencies in general to make codegen\n\n### How to reproduce\n\n-\n\n### Operating System\n\nLinux\n\n### Versions of Apache Airflow Providers\n\n_No response_\n\n### Deployment\n\nOfficial Apache Airflow Helm Chart\n\n### Deployment details\n\n_No response_\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56481\nTitle: Simple sending of return value from Go tasks to XCom\nState: closed\nAuthor: ashb\nLabels: area:go-sdk\nBody:\nThis right now is very simple and will need to evolve over time -- it doens't\r\nsupport XCom backends for instance.\r\n\r\nWe also don't handle any of the \"serialization\" format for more advanced\r\ntypes, such as datetime/time.Time etc. That will come later\r\n\r\n\r\nIn order to call this HTPT API correctly we needed to update the client generator.",
  "Requirement ID: ISSUE-56480\nTitle: [v3-1-test] Emit log stream stopped warning as ndjson (#56474)\nState: closed\nAuthor: github-actions[bot]\nLabels: area:logging\nBody:\nChanges the format of the log stream stopped warning emitted by the TaskLogReader to ndjson when it encounters no end of log marker in a stream. Mixing ndjson and non-ndjson means the UI will not show any logs at all.\n(cherry picked from commit 87826505f25278928e1a42e12dd2a04d7eb584b3)\n\nCo-authored-by: Ian Buss <ianbuss@users.noreply.github.com>",
  "Requirement ID: ISSUE-56479\nTitle: Airflowctl datamodel generator not updating existed models\nState: open\nAuthor: bugraoz93\nLabels: kind:bug, area:API, area:core\nBody:\n### Apache Airflow version\n\nmain (development)\n\n### What happened?\n\nIn this PR #56022, `DagRunResponse` updated here.\nhttps://github.com/apache/airflow/pull/56022/files#diff-e004e118f325bcefabca412ca9054ed4218ee6ddfdadde18df10e2839b053663R103\nUnfortunately, it only created the new model `AssetSummary` but not updated the `DagRunResponse`.\nhttps://github.com/apache/airflow/pull/56022/files#diff-2bf2d4418fc23f286b2019543db484f51250f5f3eef5cb4857b645e60656ca47R52\n\n\n### What you think should happen instead?\n\nIt should update existed datamodels.\n\n### How to reproduce\n\nUpdating a datamodel in Core/Public API and running generate airflowctl datamodels hook.\n\n### Operating System\n\nLinux\n\n\n### Are you willing to submit PR?\n\n- [x] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56478\nTitle: Render DAG tags alphabetically\nState: open\nAuthor: stuart23\nLabels: area:UI\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n\r\nThe tags on the DAGs card view are rendered in the order they are received from the API server which can be inconsistent and makes it difficult to compare DAGs with the same tag-set. This sorts the labels first before rendering them.\r\n\r\nA test has been added as well to confirm the functionality.",
  "Requirement ID: ISSUE-56477\nTitle: Log server not finding host.\nState: closed\nAuthor: geoffreylarnold\nLabels: kind:bug, area:Scheduler, area:core, needs-triage\nBody:\n### Apache Airflow version\n\n3.1.0\n\n### If \"Other Airflow 2 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\nThe python scheduler task does not seem to be finding my systems local address when starting up the log in `airflow scheduler`.\nI have tested two recommended settings for `hostname_callable` in the config file (`airflow.utils.net.get_host_ip_address` and `socket.getfqdn`) and both result in the same behavior. And confirmed the change in the setting with `airflow config get-value core hostname_callable`\n\nBecause of this no tasks can run in the airflow instance.\n\nError:\n```[2025-10-08T13:50:35.242333Z] {core.py:50} INFO - Starting log server on http://[::]:8793\n[2025-10-08T13:50:35.248641Z] {scheduler_job_runner.py:1018} INFO - Starting the scheduler\n[2025-10-08T13:50:35.251945Z] {executor_loader.py:281} INFO - Loaded executor: :LocalExecutor:\n[2025-10-08T13:50:35.256073Z] {scheduler_job_runner.py:2240} INFO - Adopting or resetting orphaned tasks for active dag runs\nWARNING:  ASGI app factory detected. Using it, but please consider setting the --factory flag explicitly.\nINFO:     Started server process [674660]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://:8793 (Press CTRL+C to quit)\n[2025-10-08T13:51:24.493282Z] {scheduler_job_runner.py:1720} INFO - DAG Waze_Data_Feed is at (or above) max_active_runs (1 of 1), not creating any more runs\n[2025-10-08T13:51:24.572908Z] {scheduler_job_runner.py:419} INFO - 1 tasks up for execution:\n        <TaskInstance: Waze_Data_Feed.data_pull scheduled__2025-10-08T13:50:00+00:00 [scheduled]>\n[2025-10-08T13:51:24.573145Z] {scheduler_job_runner.py:491} INFO - DAG Waze_Data_Feed has 0/16 running and queued tasks\n[2025-10-08T13:51:24.573670Z] {scheduler_job_runner.py:630} INFO - Setting the following tasks to queued state:\n        <TaskInstance: Waze_Data_Feed.data_pull scheduled__2025-10-08T13:50:00+00:00 [scheduled]>\n[2025-10-08T13:51:24.576911Z] {scheduler_job_runner.py:715} INFO - Trying to enqueue tasks: [<TaskInstance: Waze_Data_Feed.data_pull scheduled__2025-10-08T13:50:00+00:00 [scheduled]>] for executor: LocalExecutor(parallelism=32)\n[2025-10-08T13:51:24.601266Z] {local_executor.py:65} INFO - Worker starting up pid=675283\n[2025-10-08T13:51:24.612964Z] {local_executor.py:65} INFO - Worker starting up pid=675284\n[2025-10-08T13:51:24.679010Z] {supervisor.py:1870} INFO - Secrets backends loaded for worker count=1 backend_classes=['EnvironmentVariablesBackend']\n/home/mgradmin/airflow/airflow_env/lib/python3.12/site-packages/airflow/sdk/execution_time/supervisor.py:476 DeprecationWarning: This process (pid=675283) is multi-threaded, use of fork() may lead to deadlocks in the child.\n[2025-10-08T13:51:24.706402Z] {before.py:42} WARNING - Starting call to 'airflow.sdk.api.client.Client.request', this is the 1st time calling it.\n[2025-10-08T13:51:25.708321Z] {before.py:42} WARNING - Starting call to 'airflow.sdk.api.client.Client.request', this is the 2nd time calling it.\n[2025-10-08T13:51:27.631341Z] {before.py:42} WARNING - Starting call to 'airflow.sdk.api.client.Client.request', this is the 3rd time calling it.\n[2025-10-08T13:51:30.964791Z] {before.py:42} WARNING - Starting call to 'airflow.sdk.api.client.Client.request', this is the 4th time calling it.\n[2025-10-08T13:51:38.037899Z] {supervisor.py:709} INFO - Process exited pid=675285 exit_code=<Negsignal.SIGKILL: -9> signal_sent=SIGKILL\n[2025-10-08T13:51:38.038389Z] {local_executor.py:100} ERROR - uhoh\nTraceback (most recent call last):\n  File \"/home/mgradmin/airflow/airflow_env/lib/python3.12/site-packages/httpx/_transports/default.py\", line 101, in map_httpcore_exceptions\n    yield\n  File \"/home/mgradmin/airflow/airflow_env/lib/python3.12/site-packages/httpx/_transports/default.py\", line 250, in handle_request\n    resp = self._pool.handle_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mgradmin/airflow/airflow_env/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py\", line 256, in handle_request\n    raise exc from None\n  File \"/home/mgradmin/airflow/airflow_env/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py\", line 236, in handle_request\n    response = connection.handle_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mgradmin/airflow/airflow_env/lib/python3.12/site-packages/httpcore/_sync/connection.py\", line 101, in handle_request\n    raise exc\n  File \"/home/mgradmin/airflow/airflow_env/lib/python3.12/site-packages/httpcore/_sync/connection.py\", line 78, in handle_request\n    stream = self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mgradmin/airflow/airflow_env/lib/python3.12/site-packages/httpcore/_sync/connection.py\", line 124, in _connect\n    stream = self._network_backend.connect_tcp(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mgradmin/airflow/airflow_env/lib/python3.12/site-packages/httpcore/_backends/sync.py\", line 207, in connect_tcp\n    with map_exceptions(exc_map):\n  File \"/usr/lib/python3.12/contextlib.py\", line 158, in __exit__\n    self.gen.throw(value)\n  File \"/home/mgradmin/airflow/airflow_env/lib/python3.12/site-packages/httpcore/_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ConnectError: [Errno 111] Connection refused\n```\n\n### What you think should happen instead?\n\nThe log server should display the proper hostname or ip address instead of `[::]` as it is currently doing, and tasks should be able to write to the log.\n\n### How to reproduce\n\nFrom what I've read from other issues pertaining to this issue in the past it appears to be specific to the enviornment in which this instance is deployed.\n\n### Operating System\n\nUbuntu 24.04.3 LTS\n\n### Versions of Apache Airflow Providers\n\n```apache-airflow-providers-common-compat==1.7.4\napache-airflow-providers-common-io==1.6.3\napache-airflow-providers-common-sql==1.28.1\napache-airflow-providers-docker==4.4.3\napache-airflow-providers-fab==3.0.0\napache-airflow-providers-ftp==3.13.2\napache-airflow-providers-http==5.3.4\napache-airflow-providers-imap==3.9.2\napache-airflow-providers-jdbc==5.2.3\napache-airflow-providers-microsoft-mssql==4.3.2\napache-airflow-providers-odbc==4.10.2\napache-airflow-providers-oracle==4.2.0\napache-airflow-providers-postgres==6.3.0\napache-airflow-providers-slack==9.3.0\napache-airflow-providers-smtp==2.2.1\napache-airflow-providers-ssh==4.1.4\napache-airflow-providers-standard==1.8.0\napache-airflow-providers-tableau==5.2.0\n```\n\n### Deployment\n\nOther\n\n### Deployment details\n\npython venv pip installation\n\n### Anything else?\n\nThis occurs no matter the DAG or Task that is being started by the scheduler\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56476\nTitle: Fix deterministic DAG serialization\nState: closed\nAuthor: anshuksi282-ksolves\nLabels: area:serialization\nBody:\n# Fix deterministic DAG serialization in Airflow 3.1.0\r\n\r\n**Issue:** #56471\r\n\r\n## Context and Problem\r\nIn Airflow 3.1.0, serialized DAGs were being stored in the `serialized_dag` table on nearly every parsing cycle, even when the underlying DAG file had no functional changes.  \r\nThe root cause was non-deterministic serialization: dictionary keys and list elements in the JSON column were not consistently ordered between parses.  \r\nThis caused unnecessary new DAG versions and made version tracking unstable.\r\n\r\n## Impact\r\nEvery DAG parse, even for unchanged DAGs, generated a new version in the database.  \r\nThis increased DB writes, made DAG version history confusing, and could affect webserver and scheduler performance.\r\n\r\n## Solution\r\n- Added a `_sort_serialized_dag_dict` method in `SerializedDagModel` that recursively sorts dictionaries and lists in the serialized DAG JSON.  \r\n- Updated `hash` and `serialize_dag` methods to use this deterministic ordering before computing DAG hashes.  \r\n- This ensures logically identical DAGs produce the same serialized JSON and hash, preventing unnecessary new versions.  \r\n- Tested by creating a sample DAG (`test_serialized_dag`) and verifying that multiple parses without DAG changes do not create new database entries.  \r\n- Verified output in Python DB shell using `SerializedDagModel.get_latest_serialized_dags`, showing consistent `dag_hash` and `data`.\r\n\r\n## Result\r\nSerialized DAGs are now deterministic. Only actual DAG changes trigger new versions, reducing database writes and improving system reliability.\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56474\nTitle: Emit log stream stopped warning as ndjson\nState: closed\nAuthor: ianbuss\nLabels: area:logging, backport-to-v3-1-test\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\n\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n\r\nChanges the format of the log stream stopped warning emitted by the TaskLogReader to ndjson when it encounters no end of log marker in a stream. Mixing ndjson and non-ndjson means the UI will not show any logs at all.\r\n\r\ncloses: #56473\r\n\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56473\nTitle: End of log marker text warning is not in json format which breaks UI\nState: closed\nAuthor: ianbuss\nLabels: kind:bug, area:core, area:UI\nBody:\n### Apache Airflow version\n\n3.1.0\n\n### If \"Other Airflow 2 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\nIf the configured logging provider does not, or has not yet, emitted the end of log marker, we emit a message in the logs response. This is currently:\n```\n(Log stream stopped - End of log marker not found; logs may be incomplete.)\n```\n\nIf the UI encounters this, it refuses to show any logs at all since it is not in ndjson format. There are no error logs in the browser console.\n\n### What you think should happen instead?\n\nThe warning text should be emitted in ndjson format to match the content type of the response.\n\n### How to reproduce\n\nUse a logging provider which does not write an end of log marker, for example if sidecar logging to elasticsearch is configured in an environment.\n\n### Operating System\n\nLinux\n\n### Versions of Apache Airflow Providers\n\n_No response_\n\n### Deployment\n\nOfficial Apache Airflow Helm Chart\n\n### Deployment details\n\n_No response_\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [x] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56472\nTitle: Added Vrahad Analaytics to the list of companies using Apache\u00a0Airflow\nState: closed\nAuthor: varun-bhardwaj-sde\nLabels: \nBody:\nAdded Vrahad Analytics to INTHEWILD.md\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n<!--** Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).-->",
  "Requirement ID: ISSUE-56471\nTitle: Non-deterministic DAG serialization for Dinamically generated dags leads to excessive versions\nState: open\nAuthor: wolvery\nLabels: kind:bug, area:MetaDB, area:core, needs-triage, affected_version:3.0\nBody:\n### Apache Airflow version\n\n3.1.0\n\n### If \"Other Airflow 2 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\nWe are experiencing an issue where the dag processor creates a new version of a DAG in the serialized_dag table on nearly every parsing cycle, even when the underlying DAG file is functionally unchanged. \n\nThe root cause appears to be a non-deterministic serialization process. The order of dictionary keys and list elements in the resulting JSON column of serialized_dag table is inconsistent between parses. This leads to different versions for logically identical DAGs, only the order of keys inside of the JSON are changing.\n\n\nExample:\n```\n--- version_1\n+++ version_2\n@@ -1,13 +1,13 @@\n-\"dag_id\": \"test\",\n-\"max_consecutive_failed_dag_runs\": 7,\n-\"timetable\": { ... },\n-\"relative_fileloc\": \"revision_dags/test.py\",\n-\"task_group\": { ... },\n-\"fileloc\": \"/opt/airflow/dags/revision_dags/test.py\",\n-\"timezone\": \"UTC\",\n-\"default_args\": { ... },\n-\"description\": \"DAG for [domain='test', data_product='polaroid_input_features', pipeline='main']\",\n-\"max_active_runs\": 1,\n-\"tags\": [ ... ],\n-\"start_date\": 1640995200.0\n+\"max_consecutive_failed_dag_runs\": 7,\n+\"task_group\": { ... },\n+\"timezone\": \"UTC\",\n+\"max_active_runs\": 1,\n+\"fileloc\": \"/opt/airflow/dags/revision_dags/test.py\",\n+\"timetable\": { ... },\n+\"start_date\": 1640995200.0,\n+\"description\": \"DAG for [domain='test', data_product='polaroid_input_features', pipeline='main']\",\n+\"default_args\": { ... },\n+\"tags\": [ ... ],\n+\"relative_fileloc\": \"revision_dags/test.py\",\n+\"dag_id\": \"test\"\n```\n\n### What you think should happen instead?\n\nIt should sort the keys internally to perform the comparison and avoid the creation of a new version.\n\n### How to reproduce\n\nCreating a simple dynamic dag and importing in the global seems to lead to the problem.\n\n### Operating System\n\nairflow:3.1.0 python 3.10 image\n\n### Versions of Apache Airflow Providers\n\n_No response_\n\n### Deployment\n\nOfficial Apache Airflow Helm Chart\n\n### Deployment details\n\nOfficial Apache Airflow Helm Chart\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [x] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56470\nTitle: Missing airflow / docker compose Documentation for 3.X and sqlalchemy 2.X\nState: open\nAuthor: kilian-le-calvez\nLabels: kind:bug, kind:documentation, area:auth, needs-triage\nBody:\n### Doc missing for : docker-compose / sqlalchemy2.X / airflow3.X / simple auth manager\n\nHello,\n\nI just struggled a long time because i was blindly testing configuration to upgrade my airflow project to 3.X using sqlalchemy 2.X.\n\nSpecs are:\nDocker file with image\nFROM apache/airflow:3.1.0-python3.12\n\nfollowing the official 3.1 docker-compose file\n\nhaving sqlalchemy in my requirements:\nsqlalchemy==2.0.36\n\na test dag for the worker to work and sign jwt\n\nWhat i struggled with:\nThe Fab Auth Manager (old) is still active but not working for sqlalchemy 2.X\nThe Simple Auth Manager is easy to use but provide only authentification for the UI login.\nSince now there is JWT authentification, the worker could not verify the jwt when trying to launch a DAG.\n\n### Solving the problem\n\nYou have to go through all the following configuration page:\nhttps://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html\n\nTo find the:\nEnvironment Variables:\nAIRFLOW__API_AUTH__JWT_SECRET\n\nThat will enable the worker jwt sign effectively.\n\nBut this is not referenced in the official docker-compose file since it use fab auth manager (which is incompatible with sqlalchemy 2.X)\n\nCould you please add a doc page that the reference a working setup using simple auth manager ? Because some people might also need to use sqlalchemy 2.X and took me very much time to find the good working information.\n\n### Anything else\n\nOnly relevant for people that want to use sqlalchemy 2.X and so upgrading airflow to 3.X without using fab auth manager (old using sqlalchemy <2.0)\n\nThis is one of the first issue i submit, that's just requesting a doc for a specific use case so i did not linked all my config and showing code. Thank you very much for your time\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56467\nTitle: Bump uv version to 0.9.0 and ruff to 0.14.0\nState: closed\nAuthor: amoghrajesh\nLabels: area:dev-tools, kind:documentation, area:production-image, backport-to-v3-1-test\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\nFixing CI failures like this one: https://github.com/apache/airflow/actions/runs/18331431783/job/52207104054\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56465\nTitle: Callback getting stuck when calling get_task_states\nState: open\nAuthor: lxndrslr\nLabels: kind:bug, area:core, needs-triage\nBody:\n### Apache Airflow version\n\n3.1.0\n\n### If \"Other Airflow 2 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\nWhen trying to call ti.get_task_states from a DAG callback it gets stuck and does not return an error.\n\n[test_task_instances_dag.py.log](https://github.com/user-attachments/files/22752822/test_task_instances_dag.py.log)\n[attempt=1.log](https://github.com/user-attachments/files/22752827/attempt.1.log)\n[test_task_instances_dag.py](https://github.com/user-attachments/files/22752823/test_task_instances_dag.py)\n\n### What you think should happen instead?\n\nI think it should throw an error if calling the API is not allowed, give an timeout error after some time or return the correct result.\n\n### How to reproduce\n\nUse the following DAG and check the dag_processor logs. It starts executing but does not throw an error or return the done log. The task logs show how the output should be.\n[test_task_instances_dag.py](https://github.com/user-attachments/files/22752850/test_task_instances_dag.py)\n\n\n### Operating System\n\nDebian GNU/Linux 12 (bookworm\n\n### Versions of Apache Airflow Providers\n\napache-airflow-providers-amazon==9.14.0\napache-airflow-providers-celery==3.12.3\napache-airflow-providers-cncf-kubernetes==10.8.1\napache-airflow-providers-common-compat==1.7.4\napache-airflow-providers-common-io==1.6.3\napache-airflow-providers-common-messaging==2.0.0\napache-airflow-providers-common-sql==1.28.1\napache-airflow-providers-docker==4.4.3\napache-airflow-providers-elasticsearch==6.3.3\napache-airflow-providers-fab==2.4.3\napache-airflow-providers-ftp==3.13.2\napache-airflow-providers-git==0.0.8\napache-airflow-providers-google==18.0.0\napache-airflow-providers-grpc==3.8.2\napache-airflow-providers-hashicorp==4.3.2\napache-airflow-providers-http==5.3.4\napache-airflow-providers-microsoft-azure==12.7.1\napache-airflow-providers-mysql==6.3.4\napache-airflow-providers-odbc==4.10.2\napache-airflow-providers-openlineage==2.7.1\napache-airflow-providers-postgres==6.3.0\napache-airflow-providers-redis==4.3.1\napache-airflow-providers-sendgrid==4.1.3\napache-airflow-providers-sftp==5.4.0\napache-airflow-providers-slack==9.3.0\napache-airflow-providers-smtp==2.2.1\napache-airflow-providers-snowflake==6.5.4\napache-airflow-providers-ssh==4.1.4\napache-airflow-providers-standard==1.8.0\n\n### Deployment\n\nDocker-Compose\n\n### Deployment details\n\nMost recent docker compose file with 3.1\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56464\nTitle: Bump uv version to 0.8.24 and prek to 0.2.4\nState: closed\nAuthor: amoghrajesh\nLabels: area:dev-tools, area:production-image, backport-to-v3-1-test\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\nCI fails on main do to new version of `uv` and `prek`: https://github.com/apache/airflow/actions/runs/18317915864/job/52163286950\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56463\nTitle: Add ANSI support to log viewer\nState: open\nAuthor: RoyLee1224\nLabels: area:UI\nBody:\n## Related issue:\r\n\r\ncloses: #56421\r\n\r\n## Why\r\n> The Airflow UI should re-introduce the functionality to display styled/colored log messages in the Task Log view.\r\n\r\n## What\r\nUse `anser` to parse log strings into styled spans with CSS classes, which are then mapped to the Chakra UI theme for light/dark mode support.\r\n\r\n## Screenshots\r\n<img width=\"1229\" height=\"762\" alt=\"light\" src=\"https://github.com/user-attachments/assets/bc639316-6904-4c2e-baff-87764c57e6c2\" />\r\n<img width=\"1229\" height=\"762\" alt=\"dark\" src=\"https://github.com/user-attachments/assets/43ee2cbf-bc5f-402f-a630-8c96e8b6f5ab\" />\r\n\r\n\r\n\r\n### We can do this now!\r\n<img width=\"1691\" height=\"717\" alt=\"CleanShot 2025-10-09 at 21 41 48\" src=\"https://github.com/user-attachments/assets/3e4aafcf-79de-4b5d-81e2-72a02ebb1c51\" />\r\n\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56460\nTitle: Add POSIX exit code to REST API output for GET:/dags/{dag_id}/dagRuns/{dag_run_id}/taskInstances/{task_id}\nState: closed\nAuthor: mthurmaier-hb\nLabels: kind:feature, area:API, needs-triage\nBody:\n### Description\n\nThe aforementioned REST endpoint returns a state but not an exit code.  state of \"failed\" doesn't indicate why a task failed.  A POSIX status code, e.g. 0-255, could.\n\nIt is not clear to me whether it belongs in the return for this endpoint or whether we may need to extend the API by having:\nGET:/dags/{dag_id}/dagRuns/{dag_run_id}/taskInstances/{task_id}/statuses/{task_try_number} like we have for logs to get the status code for each try, or both.  I.e., if Airflow gives up trying because the max tries is exceeded, then the status code for the .../{task_id} can be the status code of the last try?\n\n### Use case/motivation\n\nWhen a task fails, I need a way to determine WHY it failed, programmatically.  It is very inconvenient to get the logs and then try to parse them to find the error.  A POSIX exit code is intended for just this purpose, to give a number that can be used to indicate how or why something failed, with 0 meaning the task / command / whatever succeeded.\n\n### Related issues\n\nnot that I could find.\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56459\nTitle: Bug: Unable to use expand on a TaskGroup containing a @task.kubernetes_cmd task\nState: open\nAuthor: SoinSoin\nLabels: kind:bug, area:core, area:dynamic-task-mapping, area:TaskGroup, needs-triage, affected_version:3.0, affected_version:3.1\nBody:\n### Apache Airflow version\n\n3.0.6\n\n### If \"Other Airflow 2 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\n**Summary**\n---\nSince Airflow 3.0 (tested on 3.0.6), it is impossible to use `expand` on a `@task_group` containing a task decorated with `@task.kubernetes_cmd`.  \nExpansion works correctly for a standalone `@task.kubernetes_cmd` task, or for a `@task_group` containing a `@task.kubernetes` task, but fails in the specific case of a task group + `kubernetes_cmd`.  \nThe mapped data is not correctly interpreted or propagated.\n\n---\n\nThis code raises an error or does not correctly map the values to the internal TaskGroup task (`echo_cmd`).  \nNote: If you replace `@task.kubernetes_cmd` with `@task.kubernetes`, it works as expected.\n\n---\n\n**Expected behavior**\n- Expansion (`expand`) on a TaskGroup containing a `@task.kubernetes_cmd` task should work just like with other TaskFlow-compatible tasks.\n\n**Observed behavior**\n- The mapping/expand does not pass the expected values to the internal task within the TaskGroup.\n- The value injected into the internal task is a `MappedArgument` object (e.g., `<airflow.models.xcom_arg.MappedArgument object at ...>`) instead of the actual value (e.g. `\"foo\"`, `\"bar\"`).\n- There is no similar issue with `@task.kubernetes` in the same scenario.\n\n---\n\n**Technical details**\n- When mapping via `expand` on a `@task_group` containing a `@task.kubernetes_cmd` task, the value received by the internal task is an unresolved `MappedArgument` object.\n\n---\n\n**Environment**\n- Airflow: 3.0.6\n- Deployment: via the official Helm chart ([apache/airflow](https://github.com/apache/airflow/tree/main/chart))\n- Image used: `apache/airflow:3.0.6`\n- Python version: not explicitly set, using the official image default\n\n---\n\n**Notes / leads**\n- It seems that expand/mapping on TaskGroups does not work with this specific decorator, while it does with others.\n- No existing issue found as of October 2025.\n\n---\n\n**Thanks for your help and for all your great work on Airflow!**\n\n### What you think should happen instead?\n\n_No response_\n\n### How to reproduce\n\n```python\nfrom airflow.decorators import dag, task, task_group\nfrom datetime import datetime\n\n@dag(start_date=datetime(2023,1,1), schedule=None, catchup=False)\ndef demo():\n    @task\n    def make_values():\n        return [\"foo\", \"bar\", \"baz\"]\n\n    @task.kubernetes_cmd(image=\"alpine\")\n    def echo_cmd(name):\n        return [\"echo\", name]\n\n    @task_group\n    def group_task(name):\n        echo_cmd(name)\n\n    # This works:\n    # echo_cmd.expand(name=make_values())\n\n    # This does NOT work:\n    group_task.expand(name=make_values())\n\ndag = demo()\n```\n\n### Operating System\n\napache/airflow:3.0.6\n\n### Versions of Apache Airflow Providers\n\n_No response_\n\n### Deployment\n\nOfficial Apache Airflow Helm Chart\n\n### Deployment details\n\n_No response_\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56458\nTitle: Fixed Error when parsing master URL #56453\nState: open\nAuthor: allabright\nLabels: area:providers, provider:apache-spark\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #56453\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\nFixed error with parsing of spark submit URL.\r\nLinked to https://github.com/apache/airflow/issues/56453\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56457\nTitle: Make Edge Worker using async loop\nState: open\nAuthor: jscheffl\nLabels: area:providers, provider:edge\nBody:\nSo far the Edge worker running remote is using a simgle threaded loop to fetch, process, monitor tasks and upload results logs as well as heartbeat. This limits the parallelism achieveable on a Edge Worker - how many tasks can be handled in parallel.\r\n\r\nWith this PR a major refactor is made to use Python AsyncIO for handling of tasks in order to improve concurrency with many tasks.\r\n\r\nNote: Before merging this needs to have proper review and testing as a lot of logic and libraries change with the risk of degraded quality/stability as of implementation glitched. Therefore this PR is WIP.\r\n\r\nFYI @dheerajturaga",
  "Requirement ID: ISSUE-56456\nTitle: Enable dynamic generation of assets for Edge UI Plugin\nState: open\nAuthor: jscheffl\nLabels: provider:amazon, area:providers, area:dev-tools, provider:airbyte, provider:alibaba, provider:apprise, provider:arangodb, provider:asana, provider:atlassian-jira, provider:apache-kafka, provider:apache-hive, provider:apache-druid, provider:apache-spark, provider:apache-beam, provider:apache-livy, provider:apache-drill, provider:apache-cassandra, provider:apache-flink, provider:apache-hdfs, provider:apache-kylin, provider:apache-pig, provider:apache-pinot, provider:apache-impala, provider:apache-iceberg, backport-to-v3-1-test, provider:apache-tinkerpop\nBody:\nSo far the generated UI assets for Edge Worker React Plugin were manually compiled and checked-in the repo. This is a bit slowing down parallel development as the generated assets always generate merge conflicts and checking in compiled/generated code in general is not good.\r\n\r\nThis PR changes the way for Edge to compile React UI assets when `breeze start-airflow` as well as when Wheel/Sdist is packaged.\r\n\r\nThis is WIP/Draft to have an early review. Especially looking forward for review by @potiuk as within this PR we need to enable building providers with hatchling selectively which has an impact on reproducible builds and CI ecosystem.\r\n\r\nTODO before completion:\r\n- [ ] Check that generated package from provider generation is complete and \"similar\" to previous release / deployment test\r\n\r\nA follow-up of this PR would be apply the same to FAB provider and Fab UI.",
  "Requirement ID: ISSUE-56455\nTitle: Use Task Display Name in Graph if existing\nState: closed\nAuthor: jscheffl\nLabels: area:API, type:bug-fix, area:UI, backport-to-v3-1-test\nBody:\ncloses: #56094\r\n\r\nAfter https://github.com/apache/airflow/pull/56393 I noticed and was notified about the same problem exists on Graph view. This PR fixes and aligns to make the same on Grid view.",
  "Requirement ID: ISSUE-56454\nTitle: Resolution of #56453. Prepend \"spark://\" to the URI when sending the spark-submit command.\nState: closed\nAuthor: allabright\nLabels: area:providers, provider:apache-spark\nBody:\nLinked to issue: https://github.com/apache/airflow/issues/56453\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #56453\r\nrelated: #56453\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\nQuick fix to prepend \"spark://\" to the URI when submitting a job.\r\n\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56453\nTitle: Error when parsing master URL\nState: open\nAuthor: allabright\nLabels: kind:bug, area:providers, needs-triage, provider:apache-spark\nBody:\n### Apache Airflow Provider(s)\n\napache-spark\n\n### Versions of Apache Airflow Providers\n\napache-airflow-providers-apache-spark==5.3.2\n\n### Apache Airflow version\n\n3.1.0\n\n### Operating System\n\nMac Tahoe Version 26.0.1 (25A362)\n\n### Deployment\n\nDocker-Compose\n\n### Deployment details\n\nRunning locally using docker-compose from this page: https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html\n\n### What happened\n\nIssue has already been raised here: https://github.com/apache/airflow/issues/46169\n\nThe \"spark://\" part of the URI is being parsed out, leading to an error in spark-submit. Changing the host manually in the GUI from \"spark-master\" to \"spark://spark-master\" causes it to work. However, specifying it as an environment variable or via the command line results in the \"spark://\" part being stripped.\n\n### What you think should happen instead\n\nThe \"spark://\" part of the URI should be kept as part of the spark-submit command.\n\n### How to reproduce\n\nRun Airflow using the docker compose file specified above, and add the following command to airflow-init:\n\n/entrypoint airflow connections get spark_default >/dev/null 2>&1 || /entrypoint airflow connections add 'spark_default' --conn-uri 'spark://spark-master:7077'\n\nThen try to run a spark job using this connection. The expected command is:\n\nspark-submit --master spark://spark-master:7077\n\nThe actual command is\n\nspark-submit --master spark-master:7077\n\nLeading to the following error:\n\n[2025-10-07 13:49:44] INFO - : org.apache.spark.SparkException: Could not parse Master URL: 'spark-master:7077' source=airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook loc=spark_submit.py:644\n\nTask failed with exception source=task loc=task_runner.py:972\nAirflowException: Cannot execute: spark-submit --master spark-master:7077 --name arrow-spark --verbose --deploy-mode client /opt/airflow/src/spark_test.py. Error code is: 1.\n\n\n### Anything else\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [x] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56452\nTitle: Fix text selection visibility in logs and code\nState: open\nAuthor: anshuksi282-ksolves\nLabels: area:providers, area:UI, provider:fab\nBody:\nFix text selection contrast in Airflow UI logs and code views\r\n\r\nIssue: #56449\r\n\r\nContext and Problem\r\nFollowing the v3.1.0 theming overhaul, Airflow adopted Chakra UI semantic tokens and a Tailwind-inspired color palette. However, explicit ::selection pseudo-element styling was omitted. As a result, browsers defaulted to low-contrast selection highlights that blend into the log and code background, making user text selections nearly invisible.\r\n\r\nImpact\r\nUsers cannot reliably select and copy text from logs or code blocks, hindering troubleshooting, debugging, and overall accessibility.\r\n\r\nSolution\r\n\r\nAdded global ::selection, ::-moz-selection, and ::-webkit-selection rules in the Chakra theme configuration (src/theme.ts) under globalCss, enforcing a clear blue highlight (blue.500 in light mode, blue.300 in dark mode).\r\n\r\nCreated a dedicated CSS file (src/assets/selection.css) with selection rules for pre, code, .log-viewer, and elements with data-testid*=\"log\", ensuring consistent blue highlighting in log and code areas.\r\n\r\nImported selection.css in the application entry point (src/main.tsx) so that styles load early.\r\n\r\nThese changes restore high-contrast text selection, comply with WCAG AA contrast requirements, and improve user experience when copying or inspecting logs and code.\r\n\r\n[Screencast from 2025-10-07 17-14-37.webm](https://github.com/user-attachments/assets/ff0a8254-5c4a-43ce-82b5-9622e10db6b2)\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56451\nTitle: Dag Processor fails with \"ValueError: Not a valid timetable\"\nState: open\nAuthor: ciancolo\nLabels: kind:bug, area:core, needs-triage\nBody:\n### Apache Airflow version\n\n3.1.0\n\n### If \"Other Airflow 2 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\nAfter upgrading Airflow from version 3.0.6 to 3.1.0, we started encountering an issue with the Dag Processor service.\n\nDuring the DAG serialization phase, the Dag Processor raises an exception and then restarts itself.\n\nBelow is an excerpt from the logs:\n\n```\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]: [2025-10-07T10:09:21.642749Z] {dag_processor_job_runner.py:63} ERROR - Exception when executing DagProcessorJob\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]: Traceback (most recent call last):\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:   File \"/opt/tenaris/venvs/airflow/lib/python3.10/site-packages/airflow/jobs/dag_processor_job_runner.py\", line 61, in _execute\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:     self.processor.run()\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:   File \"/opt/tenaris/venvs/airflow/lib/python3.10/site-packages/airflow/dag_processing/manager.py\", line 272, in run\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:     return self._run_parsing_loop()\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:   File \"/opt/tenaris/venvs/airflow/lib/python3.10/site-packages/airflow/dag_processing/manager.py\", line 361, in _run_parsing_loop\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:     self._collect_results()\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:   File \"/opt/tenaris/venvs/airflow/lib/python3.10/site-packages/airflow/utils/session.py\", line 100, in wrapper\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:     return func(*args, session=session, **kwargs)\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:   File \"/opt/tenaris/venvs/airflow/lib/python3.10/site-packages/airflow/dag_processing/manager.py\", line 827, in _collect_results\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:     self._file_stats[file] = process_parse_results(\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:   File \"/opt/tenaris/venvs/airflow/lib/python3.10/site-packages/airflow/dag_processing/manager.py\", line 1155, in process_parse_results\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:     update_dag_parsing_results_in_db(\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:   File \"/opt/tenaris/venvs/airflow/lib/python3.10/site-packages/airflow/dag_processing/collection.py\", line 372, in update_dag_parsing_results_in_db\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:     for attempt in run_with_db_retries(logger=log):\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:   File \"/opt/tenaris/venvs/airflow/lib/python3.10/site-packages/tenacity/__init__.py\", line 445, in __iter__\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:     do = self.iter(retry_state=retry_state)\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:   File \"/opt/tenaris/venvs/airflow/lib/python3.10/site-packages/tenacity/__init__.py\", line 378, in iter\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:     result = action(retry_state)\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:   File \"/opt/tenaris/venvs/airflow/lib/python3.10/site-packages/tenacity/__init__.py\", line 400, in <lambda>\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:     self._add_action_func(lambda rs: rs.outcome.result())\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:   File \"/usr/local/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:     return self.__get_result()\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:   File \"/usr/local/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:     raise self._exception\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:   File \"/opt/tenaris/venvs/airflow/lib/python3.10/site-packages/airflow/dag_processing/collection.py\", line 382, in update_dag_parsing_results_in_db\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:     SerializedDAG.bulk_write_to_db(\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:   File \"/opt/tenaris/venvs/airflow/lib/python3.10/site-packages/airflow/utils/session.py\", line 98, in wrapper\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:     return func(*args, **kwargs)\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:   File \"/opt/tenaris/venvs/airflow/lib/python3.10/site-packages/airflow/serialization/serialized_objects.py\", line 2868, in bulk_write_to_db\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:     dag_op.update_dags(orm_dags, parse_duration, session=session)\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:   File \"/opt/tenaris/venvs/airflow/lib/python3.10/site-packages/airflow/dag_processing/collection.py\", line 530, in update_dags\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:     last_automated_data_interval = get_run_data_interval(dag.timetable, last_automated_run)\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:   File \"/opt/tenaris/venvs/airflow/lib/python3.10/site-packages/airflow/models/dag.py\", line 143, in get_run_data_interval\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:     return infer_automated_data_interval(timetable, run.logical_date)\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:   File \"/opt/tenaris/venvs/airflow/lib/python3.10/site-packages/airflow/models/dag.py\", line 121, in infer_automated_data_interval\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:     raise ValueError(f\"Not a valid timetable: {timetable!r}\")\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]: ValueError: Not a valid timetable: <airflow.timetables.trigger.CronTriggerTimetable object at 0x7f2b67e9f940>\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]: [2025-10-07T10:09:21.653271Z] {supervisor.py:709} INFO - Process exited pid=2721376 exit_code=<Negsignal.SIGTERM: -15> signal_sent=SIGTERM\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]: [2025-10-07T10:09:21.661775Z] {process_utils.py:285} INFO - Waiting up to 5 seconds for processes to exit...\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]: Traceback (most recent call last):\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:   File \"/opt/tenaris/venvs/airflow/bin/airflow\", line 8, in <module>\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:     sys.exit(main())\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:   File \"/opt/tenaris/venvs/airflow/lib/python3.10/site-packages/airflow/__main__.py\", line 55, in main\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:     args.func(args)\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:   File \"/opt/tenaris/venvs/airflow/lib/python3.10/site-packages/airflow/cli/cli_config.py\", line 49, in command\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:     return func(*args, **kwargs)\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:   File \"/opt/tenaris/venvs/airflow/lib/python3.10/site-packages/airflow/utils/cli.py\", line 114, in wrapper\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:     return f(*args, **kwargs)\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:   File \"/opt/tenaris/venvs/airflow/lib/python3.10/site-packages/airflow/utils/providers_configuration_loader.py\", line 54, in wrapped_function\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:     return func(*args, **kwargs)\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:   File \"/opt/tenaris/venvs/airflow/lib/python3.10/site-packages/airflow/cli/commands/dag_processor_command.py\", line 53, in dag_processor\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:     run_command_with_daemon_option(\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:   File \"/opt/tenaris/venvs/airflow/lib/python3.10/site-packages/airflow/cli/commands/daemon_utils.py\", line 86, in run_command_with_daemon_option\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:     callback()\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:   File \"/opt/tenaris/venvs/airflow/lib/python3.10/site-packages/airflow/cli/commands/dag_processor_command.py\", line 56, in <lambda>\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:     callback=lambda: run_job(job=job_runner.job, execute_callable=job_runner._execute),\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:   File \"/opt/tenaris/venvs/airflow/lib/python3.10/site-packages/airflow/utils/session.py\", line 100, in wrapper\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:     return func(*args, session=session, **kwargs)\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:   File \"/opt/tenaris/venvs/airflow/lib/python3.10/site-packages/airflow/jobs/job.py\", line 368, in run_job\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:     return execute_job(job, execute_callable=execute_callable)\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:   File \"/opt/tenaris/venvs/airflow/lib/python3.10/site-packages/airflow/jobs/job.py\", line 397, in execute_job\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:     ret = execute_callable()\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:   File \"/opt/tenaris/venvs/airflow/lib/python3.10/site-packages/airflow/jobs/dag_processor_job_runner.py\", line 61, in _execute\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:     self.processor.run()\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:   File \"/opt/tenaris/venvs/airflow/lib/python3.10/site-packages/airflow/dag_processing/manager.py\", line 272, in run\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:     return self._run_parsing_loop()\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:   File \"/opt/tenaris/venvs/airflow/lib/python3.10/site-packages/airflow/dag_processing/manager.py\", line 361, in _run_parsing_loop\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:     self._collect_results()\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:   File \"/opt/tenaris/venvs/airflow/lib/python3.10/site-packages/airflow/utils/session.py\", line 100, in wrapper\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:     return func(*args, session=session, **kwargs)\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:   File \"/opt/tenaris/venvs/airflow/lib/python3.10/site-packages/airflow/dag_processing/manager.py\", line 827, in _collect_results\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:     self._file_stats[file] = process_parse_results(\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:   File \"/opt/tenaris/venvs/airflow/lib/python3.10/site-packages/airflow/dag_processing/manager.py\", line 1155, in process_parse_results\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:     update_dag_parsing_results_in_db(\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:   File \"/opt/tenaris/venvs/airflow/lib/python3.10/site-packages/airflow/dag_processing/collection.py\", line 372, in update_dag_parsing_results_in_db\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:     for attempt in run_with_db_retries(logger=log):\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:   File \"/opt/tenaris/venvs/airflow/lib/python3.10/site-packages/tenacity/__init__.py\", line 445, in __iter__\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:     do = self.iter(retry_state=retry_state)\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:   File \"/opt/tenaris/venvs/airflow/lib/python3.10/site-packages/tenacity/__init__.py\", line 378, in iter\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:     result = action(retry_state)\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:   File \"/opt/tenaris/venvs/airflow/lib/python3.10/site-packages/tenacity/__init__.py\", line 400, in <lambda>\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:     self._add_action_func(lambda rs: rs.outcome.result())\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:   File \"/usr/local/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:     return self.__get_result()\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:   File \"/usr/local/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:     raise self._exception\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:   File \"/opt/tenaris/venvs/airflow/lib/python3.10/site-packages/airflow/dag_processing/collection.py\", line 382, in update_dag_parsing_results_in_db\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:     SerializedDAG.bulk_write_to_db(\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:   File \"/opt/tenaris/venvs/airflow/lib/python3.10/site-packages/airflow/utils/session.py\", line 98, in wrapper\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:     return func(*args, **kwargs)\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:   File \"/opt/tenaris/venvs/airflow/lib/python3.10/site-packages/airflow/serialization/serialized_objects.py\", line 2868, in bulk_write_to_db\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:     dag_op.update_dags(orm_dags, parse_duration, session=session)\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:   File \"/opt/tenaris/venvs/airflow/lib/python3.10/site-packages/airflow/dag_processing/collection.py\", line 530, in update_dags\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:     last_automated_data_interval = get_run_data_interval(dag.timetable, last_automated_run)\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:   File \"/opt/tenaris/venvs/airflow/lib/python3.10/site-packages/airflow/models/dag.py\", line 143, in get_run_data_interval\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:     return infer_automated_data_interval(timetable, run.logical_date)\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:   File \"/opt/tenaris/venvs/airflow/lib/python3.10/site-packages/airflow/models/dag.py\", line 121, in infer_automated_data_interval\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]:     raise ValueError(f\"Not a valid timetable: {timetable!r}\")\nOct 07 12:09:21 daleulbd11.dalmine.techint.net airflow[2721133]: ValueError: Not a valid timetable: <airflow.timetables.trigger.CronTriggerTimetable object at 0x7f2b67e9f940>\nOct 07 12:09:22 daleulbd11.dalmine.techint.net systemd[1]: airflow-dag-processor.service: Main process exited, code=exited, status=1/FAILURE\nOct 07 12:09:22 daleulbd11.dalmine.techint.net systemd[1]: airflow-dag-processor.service: Failed with result 'exit-code'.\nOct 07 12:09:22 daleulbd11.dalmine.techint.net systemd[1]: airflow-dag-processor.service: Consumed 32.023s CPU time.\nOct 07 12:09:27 daleulbd11.dalmine.techint.net systemd[1]: airflow-dag-processor.service: Scheduled restart job, restart counter is at 15533.\nOct 07 12:09:27 daleulbd11.dalmine.techint.net systemd[1]: Stopped Airflow Dag processor daemon.\nOct 07 12:09:27 daleulbd11.dalmine.techint.net systemd[1]: airflow-dag-processor.service: Consumed 32.023s CPU time.\n```\n\nDespite this error, the Airflow instance itself works correctly \u2014 all DAGs appear in the UI and execute as expected.\nNo changes were made to any DAG configurations.\n\n```\ndefault_args = {\n    'owner': 'airflow',\n    'email': ['myemail'],\n    'email_on_failure': True,\n    'email_on_retry': False,\n    'retries': 2,\n    'retry_delay': timedelta(minutes=10),\n    'execution_timeout':timedelta(minutes=50),\n    'weight_rule': 'upstream'\n}\n\ndag = DAG(\n    dag_id=\"dag_name\",\n    schedule=\"0 2 * * *\",\n    start_date=pendulum.datetime(2022, 8, 28, tz=\"Europe/Rome\"),\n    catchup=False,\n    max_active_tasks=7,\n    default_args=default_args)\n```\n\n### What you think should happen instead?\n\n_No response_\n\n### How to reproduce\n\nI\u2019m not sure what triggered this behavior. I can provide my Airflow configuration if it helps reproduce the issue.\n\n### Operating System\n\nUbuntu 22.04.5 LTS\n\n### Versions of Apache Airflow Providers\n\n```\napache-airflow                           3.1.0\napache-airflow-core                      3.1.0\napache-airflow-providers-apache-hdfs     4.10.3\napache-airflow-providers-apache-spark    5.3.2\napache-airflow-providers-apache-sqoop    3.2.0\napache-airflow-providers-common-compat   1.7.4\napache-airflow-providers-common-io       1.6.3\napache-airflow-providers-common-sql      1.28.1\napache-airflow-providers-elasticsearch   6.3.3\napache-airflow-providers-fab             2.4.3\napache-airflow-providers-http            5.3.4\napache-airflow-providers-jdbc            5.2.3\napache-airflow-providers-postgres        6.3.0\napache-airflow-providers-salesforce      5.11.3\napache-airflow-providers-smtp            2.2.1\napache-airflow-providers-ssh             4.1.4\napache-airflow-providers-standard        1.8.0\napache-airflow-providers-tableau         5.2.0\napache-airflow-task-sdk                  1.1.0\n```\n\n### Deployment\n\nVirtualenv installation\n\n### Deployment details\n\n_No response_\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56450\nTitle: Cannot make \"Grid\" section narrower in airflow 3.1.0\nState: closed\nAuthor: Felix-neko\nLabels: kind:bug, area:UI\nBody:\n### Apache Airflow version\n\n3.1.0\n\n### If \"Other Airflow 2 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\nIn `apache-airflow==3.0.6` I could make the \"logs\" section wider and \"grid\" section narrower to see more logs.\nThat was really useful (see screencast)\nhttps://github.com/user-attachments/assets/e32f9cbb-5632-4e1b-8902-53416e46ea44\n\nBut in `apache-airflow==3.1.0` I cannot make the 'grid' section narrower (despite that I have only a few runs). See screencast:\nhttps://github.com/user-attachments/assets/ebb12489-62ad-4147-89cb-689a21a4673c\n\nP.S. Also, when task instance is still running, the separator is moving with jerks and freezes.\n\n### What you think should happen instead?\n\n_No response_\n\n### How to reproduce\n\nJust install airflow 3.1.0, go to logs of any task, open grid view and try to make the grid view narrower.\n\n### Operating System\n\nUbuntu 24.04\n\n### Versions of Apache Airflow Providers\n\n_No response_\n\n### Deployment\n\nvirtualenv\n\n### Deployment details\n\n_No response_\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56449\nTitle: Text selection in logs is almost invisible (extremely low contrast in airflow 3.1.0)\nState: open\nAuthor: Felix-neko\nLabels: kind:bug, area:core, area:UI, needs-triage\nBody:\n### Apache Airflow version\n\n3.1.0\n\n### If \"Other Airflow 2 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\nHi folks! I'm having another problem when has tried to use Airflow 3.1.0\n\nWhen I selected logs with mouse in previous versions of airflow (airflow 3.0.6) -- the selection was at least visible:\n\n<img width=\"1790\" height=\"1134\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/ccd7e37d-9ce1-4f28-b6dd-636932ce5040\" />\n\nBut in version 3.1.0 the text selection highlight is so low contrast that it's almost invisible!\n\n<img width=\"1790\" height=\"1134\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/c5146f17-ddaa-4283-b7d5-9e4a2bd2b1c0\" />\n\nCan you please make selection highlight more constrast?\n\n### What you think should happen instead?\n\nThe selection should be at least visible\n\n### How to reproduce\n\nJust install airflow 3.1.0, see any log and try to select some text\n\n### Operating System\n\nUbuntu 24.04\n\n### Versions of Apache Airflow Providers\n\n_No response_\n\n### Deployment\n\nVirtualenv installation\n\n### Deployment details\n\n_No response_\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56448\nTitle: I'm trying to select logs with mouse -- only the logs that are currently in viewport are selected\nState: open\nAuthor: Felix-neko\nLabels: kind:bug, area:core, area:UI, needs-triage\nBody:\n### Apache Airflow version\n\n3.1.0\n\n### If \"Other Airflow 2 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\nHi folks! I'm trying to select some logs in my airflow 3.1.0 (a bit more that could be placed in one screen). I'm pressing left mouse button and start selecting text. I'm moving mouse cursor down to the bottom of the viewport, the text is scrolled -- everything looks okay.\n\nBut.\n\nThe logs that are not currently on the screen are **not** selected. Can you please fix it?\n\n### What you think should happen instead?\n\n_No response_\n\n### How to reproduce\n\nI'm using Ubuntu 24.04 + Google Chrome + Python 3.11\n\nHere's some screencast: https://www.sendspace.com/file/yddkb0\n\n### Operating System\n\nUbuntu 24.04\n\n### Versions of Apache Airflow Providers\n\n_No response_\n\n### Deployment\n\nVirtualenv installation\n\n### Deployment details\n\n_No response_\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56447\nTitle: Enable multiple Authentication method in Airflow 3\nState: closed\nAuthor: trongchata\nLabels: kind:feature, area:API, area:auth, needs-triage\nBody:\n### Description\n\n## context \nNow we have deployed airflow 3 and integrated with OAuth2 SSO.   But we still have a requirement for triggering the DAG via RESTful from python code.  \n\n## requirement \nIs there any possible to enable two types of Authentication method for Airflow 3?  \nOr \nIs there any possible we deploy two api-server, one with OAuth2 and another one with LDAP username/password ?  \nBoth of these two airflow api-server service need to connect to same airflow db.  \n\nThank you\n\n### Use case/motivation\n\n_No response_\n\n### Related issues\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [x] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56446\nTitle: Improve the performance of the DAG Processor\nState: open\nAuthor: fshehadeh\nLabels: kind:feature, area:core, needs-triage\nBody:\n### Description\n\nWe run Airflow 3 in an AWS ECS cluster. We noticed that the DAG processor is always taking all the CPU in the container, and is continuously reloading our DAGs. In our setup, we have about 50 python files, which contain the definitions for about 43 DAGs, and the processing time for the DAGs is about one second per file on average, and the longest processing time is 3 seconds.\n\nLooking closer at the logs, we noticed that the DAGs are continuously being parsed and loaded to the DB. We wanted Airflow to quickly detect and reflect changes to our DAGs, and because of 60 seconds for min_file_process_interval. However, giving that we don't change the files that often, we wanted an optimization that would skip the process of DAGs when we can tell for sure that they have not changed. We noticed the introduction of DAG bundles, and the versioning of the bundles which is leveraged by the GIT DAG bundles, but not the local ones. We decided to take this approach further:\n1. Calculate a checksum for the folder of the local DAG bundle, and use that as a version (similar to the GIT commit ID).\n2. Track the bundle version for each file as it is parsed. When it is time to populate the file queue, we can compare the current bundle version with that of the file from the last time it was parsed. If the version is the same, then that means that the DAG python file (and any locally imported common python code) has not changed, and therefore we can skip processing it.\n3. Add a maximum age, after which we will force the processing of the DAG files (in case there are important side effects of loading them).\n\nAfter doing these changes, the CPU dropped down significantly. While item 1 above is specific to local DAG bundles, I think items 2 and 3 can be beneficial even for GIT bundles.\n\n### Use case/motivation\n\n_No response_\n\n### Related issues\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56445\nTitle: AIP-82: implement Google Pub/Sub message queue provider\nState: open\nAuthor: dejii\nLabels: provider:google, area:providers, area:dev-tools, kind:documentation, backport-to-v3-1-test\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\nPending review from the Google team (see https://github.com/apache/airflow/pull/54494#issuecomment-3370717117)\r\n\r\nrelated: https://github.com/apache/airflow/pull/54494\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56444\nTitle: LocalExecutor is not honoring the configuration for \"parallelism\"\nState: closed\nAuthor: fshehadeh\nLabels: kind:bug, area:core, needs-triage, area:Executors-core\nBody:\n### Apache Airflow version\n\n3.1.0\n\n### If \"Other Airflow 2 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\nWe run Airflow with a LocalExecutor that in turn runs tasks in an AWS ECS cluster. We have a situation where we want a large number of tasks to run at the same time (some tasks consume data from other tasks). We have been using this setup without issues with Airflow 2.x, using a parallelism configuration that matches the number of desired tasks. However, when we upgraded to Airflow 3.x, we noticed that some of the tasks would run right away, while others would get queued, and won't execute until the first group of tasks has completed, suggesting that the local executor was unable to spawn enough children tasks to match the parallelism configuration.\n\nI created a DAG that created a few long running tasks and started a few run in parallel, expecting that the number of spawned children would match the parallelism configuration, but that didn't happen. The total number of children was always smaller than the max.\n\nLooking at the code, I noticed this comment https://github.com/apache/airflow/blob/4ecebc2973587ebaa2cb12482de82e93d15c092f/airflow-core/src/airflow/executors/local_executor.py#L186\n>         # If we're using spawn in multiprocessing (default on macOS now) to start tasks, this can get called a\n>         # via `sync()` a few times before the spawned process actually starts picking up messages. Try not to\n>         # create too much\n>\n>        need_more_workers = len(self.workers) < num_outstanding\n>        if need_more_workers and (self.parallelism == 0 or len(self.workers) < self.parallelism):\n>             # This only creates one worker, which is fine as we call this directly after putting a message on\n>             # activity_queue in execute_async\n>             self._spawn_worker()\n\nThe code can effectively skip spawning a new child, when it sees that we have enough workers to cover the outstanding tasks. Unfortunately, these workers might actually be already running other tasks, which means that we have to wait for them to complete before they start working on the new tasks. This defeats the purpose and definition of parallelism. I don't think the check is necessary, even if spawning of children might occur in an async manner. The important thing is to compare the number of requested workers (no matter if they have started or not) and the parallelism config.\n\nIn my setup, I ended up patching the above file, removing the check for \"need_more_workers\", and with this change I can now see that the number of children can match the parallelism configuration.\n\nAs a side note: our tasks can take a long time to complete (more than 10 minutes), and when the the queued tasks eventually start, they would call the API to mark the task as started, but this call would fail with 403, because the JWT token would have expired by then. Changing the API JWT expiry might have fixed thee 403 error, but the root problem is that not enough tasks were allowed to run in parallel.\n\nThis is probably a low priority issue, since I guess it is not common to use a local executor, and demand high parallelism at the same time.\n\n### What you think should happen instead?\n\nThe maximum number of children tasks spawned by Airflow should match the parallelism config.\n\n### How to reproduce\n\nI created the following DAG to ramp up the number of tasks, and verify how many can run in parallel. I used it while configuring Airflow to use the LocalExecutor, and the default of 32 for parallelism\n\n```\nfrom airflow.models import DAG\nfrom airflow.models import Variable\nfrom airflow.providers.standard.operators.python import PythonOperator\nimport time\n\nimport logging\nLOGGER = logging.getLogger(__name__)\n\n\n# This DAG is meant to help verify the configuration of maximum\n# tasks in the LocalExecutor\n\n\ndef simulate_work():\n    count = 20\n    LOGGER.info(f\"Running the loop for {count} times\")\n    for i in range(count):\n        LOGGER.info(f\"Attempt: {i+1}\")\n        time.sleep(60)\n    LOGGER.info(f\"Done\")\n\ndag = DAG(\n        dag_id='Test_Parallelism_Bug',\n        max_active_runs=10,\n        max_active_tasks=100,\n        catchup=False\n    )\n\nfor i in range(5):\n    task =  PythonOperator(task_id=f\"task-{i}\",\n        python_callable=simulate_work, dag=dag, retries=0)\n```\n\n### Operating System\n\nUsing Airflow docker image\n\n### Versions of Apache Airflow Providers\n\n_No response_\n\n### Deployment\n\nOther Docker-based deployment\n\n### Deployment details\n\n_No response_\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56443\nTitle: #53140 Better error handling for running ti heartbeats after task is cleared\nState: open\nAuthor: kriti-sc\nLabels: area:API, area:task-sdk\nBody:\nReturn `410 - GONE` instead of `404 - NOT_FOUND` when the running task instance heartbeats after task is cleared. \r\n\r\nWhen a running task is cleared, the previous task instance is moved from the Task Instance table to the Task Instance History table. As a result, if the task instance hearbeats, the api server returns a 404. A more appropriate error is 410. \r\n\r\ncloses: [#53140](https://github.com/apache/airflow/issues/53140)\r\n\r\n#### Airflow task logs:\r\n<img width=\"1160\" height=\"438\" alt=\"image\" src=\"https://github.com/user-attachments/assets/3c0ca02a-704d-4e01-a6eb-79cd58281471\" />\r\n\r\n\r\n#### Scheduler and API server logs:\r\n```\r\napi-server | [2025-10-06T21:28:42.135650Z] {task_instances.py:608} ERROR - Task Instance not found in Task Instance table, might have moved to the Task Instance History table ti_id=0199bb6c-f0be-7686-a26e-30f70d893ae1\r\napi-server | INFO:     127.0.0.1:53520 - \"PUT /execution/task-instances/0199bb6c-f0be-7686-a26e-30f70d893ae1/heartbeat HTTP/1.1\" 410 Gone\r\nscheduler  | [2025-10-06T21:28:42.137967Z] {supervisor.py:1103} ERROR - Server indicated the task shouldn't be running anymore detail={'detail': {'reason': 'not_found', 'message': 'Task Instance not found, might have moved to the Task Instance History table'}} status_code=410 ti_id=UUID('0199bb6c-f0be-7686-a26e-30f70d893ae1')\r\nscheduler  | [2025-10-06T21:28:42.144732Z] {supervisor.py:713} INFO - Process exited pid=96561 exit_code=0 signal_sent=SIGTERM\r\nscheduler  | [2025-10-06T21:28:42.144989Z] {supervisor.py:1899} INFO - Task finished exit_code=0 duration=15.431881000055 final_state=SERVER_TERMINATED\r\n```\r\n\r\n\r\n\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56442\nTitle: [v3-1-test] Add uvicorn to spelling ignore list (#56441)\nState: closed\nAuthor: github-actions[bot]\nLabels: \nBody:\n(cherry picked from commit d448d08566cf0f91d7e7e7aba6ad5fd9dd554ea2)\n\nCo-authored-by: Kaxil Naik <kaxilnaik@gmail.com>",
  "Requirement ID: ISSUE-56441\nTitle: Add uvicorn to spelling ignore list\nState: closed\nAuthor: kaxil\nLabels: backport-to-v3-1-test\nBody:\nIt's failing on v3-1-test so likely will/is already failing on main too:\r\n\r\nhttps://github.com/apache/airflow/actions/runs/18283739956/job/52054518666\r\n\r\n```\r\n\r\nFile path: /opt/airflow/airflow-core/docs/RELEASE_NOTES.rst\r\nIncorrect Spelling: 'uvicorn'\r\nLine with Error: ' to align with uvicorn\u2019s logging configuration. The new option accepts a path to a logging configuration file compatible with '\r\n------------------------------ Error   3 ------------------------------\r\nRELEASE_NOTES.rst:270: (uvicorn)  process less necessary. Additionally, with uvicorn\u2019s spawn behavior instead of fork, there is\r\n\r\nFile path: /opt/airflow/airflow-core/docs/RELEASE_NOTES.rst\r\nIncorrect Spelling: 'uvicorn'\r\nLine with Error: 'process less necessary. Additionally, with uvicorn\u2019s spawn behavior instead of fork, there is'\r\n====================================================================================================\r\n\r\n```\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56435\nTitle: Add poke_mode_only to version_compat to fix the incorrect deprecation warning\nState: closed\nAuthor: zachliu\nLabels: provider:amazon, area:providers\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n`poke_mode_only` is supposed to be imported from a new location in Airflow 3.1+\r\n\r\nhttps://github.com/apache/airflow/blob/54bd5d8cd9f6f477cc83445737614dec81c4323c/airflow-core/src/airflow/sensors/__init__.py#L29-L34\r\n\r\nWe see this deprecation warning\r\n\r\n```\r\nDeprecatedImportWarning: The `airflow.sensors.base.poke_mode_only` attribute is deprecated. Please use `'airflow.sdk.bases.sensor.poke_mode_only'`.\r\n```\r\n\r\nbecause the latest `providers-amazon` is still importing from the old location\r\n\r\nhttps://github.com/apache/airflow/blob/696fd73accc1f8ed867befe2abe0be64f4eacc0b/providers/amazon/src/airflow/providers/amazon/aws/sensors/s3.py#L39\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56434\nTitle: Use name passed to `@asset` decorator when fetching the asset.\nState: closed\nAuthor: tirkarthi\nLabels: area:task-sdk, backport-to-v3-1-test\nBody:\nWhen name is used in `@asset` decorator then fetching asset should use the value instead of always using the function name which leads to asset not being found.\r\n\r\ncloses #56401",
  "Requirement ID: ISSUE-56433\nTitle: feature: Allow batch filtering for dags using Keycloak provider\nState: open\nAuthor: stegololz\nLabels: area:providers, area:dev-tools, area:API, kind:documentation, backport-to-v3-1-test, provider:keycloak\nBody:\nDue to the growing number of dags we have, the default filtering flow to list the dags was leading to performance issue as hinted in the documentation of function filter_authorized_connections.\r\n\r\n- This pull request adds a provider specific implementation to Keycloak, allowing to authenticate using batch functions.\r\n- Updated the documentation to setup a Keycloak environment to match new breeze subcommand\r\n\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56432\nTitle: KubernetesHook: verify_ssl flag does not work\nState: open\nAuthor: ralichkov\nLabels: kind:bug, area:providers, area:core, provider:cncf-kubernetes, needs-triage\nBody:\n### Apache Airflow version\n\nmain (development)\n\n### If \"Other Airflow 2 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\nConnecting to a local Kubernetes cluster using `KubernetesHook` without SSL verification does not work despite the appropriate `disable_verify_ssl` being set to `true`.\n\n### What you think should happen instead?\n\nThis reason this happens is because in `KubernetesHook`'s `get_conn()`, `_disable_verify_ssl()` calls `Configuration.set_default()`, which only [mutates an internal copy of the Kubernetes Configuration class](https://github.com/kubernetes-client/python/blob/v33.1.0/kubernetes/client/configuration.py#L192-L201). \n\nWe don't don't set these new defaults in the hook's `self.client_configuration`, which we should \n do by calling [get_default_copy()](https://github.com/kubernetes-client/python/blob/8f5578ee6845d33b3fb54867e2ea88349df6d07e/kubernetes/client/configuration.py#L203-L215) like `kube_client.py` does.\n\n### How to reproduce\n\nBreeze setup:\n```sh\n# files/airflow-breeze-config/environment_variables.env\nAIRFLOW_CONN_KUBERNETES_DEFAULT='{\"conn_type\": \"kubernetes\", \"extra\": {\"kube_config_path\": \"/files/dummy_kube_config.yml\", \"namespace\": \"default\", \"disable_verify_ssl\": true}}'\n```\nWhere `/files/dummy_kube_config.yml` is:\n<summary>\n\n```yaml\n# /files/dummy_kube_config.yml\napiVersion: v1\nkind: Config\nclusters:\n- name: dummy\n  cluster:\n    server: http://127.0.0.1:8080\ncontexts:\n- name: dummy\n  context:\n    cluster: dummy\n    user: dummy\ncurrent-context: dummy\nusers:\n- name: dummy\n  user: {}\n```\n\n</summary>\n\n in a `breeze shell` python interpreter:\n```python\nfrom airflow.providers.cncf.kubernetes.hooks.kubernetes import KubernetesHook\nhook = KubernetesHook()\nclient = hook.get_conn()\nprint(client.configuration.verify_ssl)\n# True\n```\n\n### Operating System\n\nMacOS Sequoia Version 15.7.1 \n\n### Versions of Apache Airflow Providers\n\nEditable install with no version control (apache-airflow-providers-cncf-kubernetes==10.8.2)\n\n### Deployment\n\nDocker-Compose (verified using Breeze)\n\n### Deployment details\n\n_No response_\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [x] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56431\nTitle: [v3-1-test] Fix scheduler crash with email notifications (#56429)\nState: closed\nAuthor: github-actions[bot]\nLabels: area:Scheduler, area:DAG-processing\nBody:\nThe ``EmailNotificationRequest`` class name (25 characters) exceeded the\ndatabase constraint for ``DbCallbackRequest.callback_type column`` (20\ncharacters), causing scheduler crashes when email notifications were\ntriggered for task failures or retries.\n\nThis fix renames the class to ``EmailRequest`` (12 characters) to fit within\nthe constraint. A backwards compatibility alias ensures existing DB\nentries with `'EmailNotificationRequest'` can still be deserialized via\ngetattr lookup.\n\nThe 20-character limit is arbitrary and does not affect performance.\nIn a follow-up PR for 3.2, we should increase this to 50+ characters\nto accommodate descriptive class names without requiring abbreviations.\n\nFixes #56426\n(cherry picked from commit a18fc01dbda319d6670cfab9071b2760a7fc9fe3)\n\nCo-authored-by: Kaxil Naik <kaxilnaik@gmail.com>",
  "Requirement ID: ISSUE-56430\nTitle: DAG with BigQueryToGCSOperator or GCSToBigQueryOperator not generating job_id when triggered by asset event in Airflow 3\nState: open\nAuthor: EliveltonRepolho\nLabels: kind:bug, provider:google, area:providers\nBody:\n### Apache Airflow Provider(s)\n\ngoogle\n\n### Versions of Apache Airflow Providers\n\napache-airflow-providers-google==18.0.0\n\n### Apache Airflow version\n\n3.1.0+astro.1\n\n### Operating System\n\nLinux\n\n### Deployment\n\nAstronomer\n\n### Deployment details\n\n_No response_\n\n### What happened\n\nWhen a DAG is triggered by Asset event, due to some changes in Airflow 3 the logical_date is not passed through the context anymore with asset triggered dagruns.\n\nOperators `BigQueryToGCSOperator` and are broken `GCSToBigQueryOperator` due this issue:\n```\nKeyError: 'logical_date'\nFile \"/usr/local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py\", line 920 in run\n\nFile \"/usr/local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py\", line 1302 in _execute_task\n\nFile \"/usr/local/lib/python3.12/site-packages/airflow/sdk/bases/operator.py\", line 416 in wrapper\n\nFile \"/usr/local/airflow/plugins/operators/gcs_to_bigquery_operator.py\", line 27 in execute\n\nFile \"/usr/local/lib/python3.12/site-packages/airflow/sdk/bases/operator.py\", line 416 in wrapper\n\nFile \"/usr/local/lib/python3.12/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py\", line 340 in execute\n```\n\n### What you think should happen instead\n\n`BigQueryToGCSOperator` and `GCSToBigQueryOperator` should be able to generate a job_id regardless of whether it is triggered in a scheduled / manually triggered DAG or through an asset event.\n\nI don't know much about Airflow models, but possibly changing `logical_date` by `dag_run_id` would do the work ?\n\n### How to reproduce\n\nCreate DAG triggered by Asset that has `BigQueryToGCSOperator` and `GCSToBigQueryOperator`\n\n### Anything else\n\nthis issue was fixed for `BigQueryInsertJobOperator` in https://github.com/apache/airflow/pull/55092, but we have the other 2 with same issue. \n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56429\nTitle: Fix scheduler crash with email notifications\nState: closed\nAuthor: kaxil\nLabels: area:Scheduler, area:DAG-processing, backport-to-v3-1-test\nBody:\nThe ``EmailNotificationRequest`` class name (25 characters) exceeded the database constraint for ``DbCallbackRequest.callback_type column`` (20 characters), causing scheduler crashes when email notifications were triggered for task failures or retries.\r\n\r\nThis fix renames the class to ``EmailRequest`` (12 characters) to fit within the constraint. A backwards compatibility alias ensures existing DB entries with `'EmailNotificationRequest'` can still be deserialized via getattr lookup.\r\n\r\nThe 20-character limit is arbitrary and does not affect performance. In a follow-up PR for 3.2, we should increase this to 50+ characters to accommodate descriptive class names without requiring abbreviations.\r\n\r\nFixes #56426\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56427\nTitle: DAG Params Enum not works as expected\nState: open\nAuthor: anavrotski\nLabels: kind:bug, area:core, area:UI, affected_version:3.1\nBody:\n### Apache Airflow version\n\n3.1.0\n\n### If \"Other Airflow 2 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\nWhen param of type enum is used in DAG (see picture), it is not displayed as a drop-down list after pressing the trigger dag button.\n\n<img width=\"894\" height=\"192\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/12a0dd87-b678-483a-bb84-75809cbeee17\" />\n\nSo I should delete square brackets, and then start typing something, and only after that drop-down list appears (see video)\n\nhttps://github.com/user-attachments/assets/b848adf5-1503-4e8c-a7d8-6c0ea6477e37\n\n### What you think should happen instead?\n\nAfter pressing the trigger dag button, params of type enum should be present as a drop-down list without additional actions.\n\n### How to reproduce\n\nJust create DAG with param of type enum.\n\n### Operating System\n\nlinux\n\n### Versions of Apache Airflow Providers\n\n_No response_\n\n### Deployment\n\nOfficial Apache Airflow Helm Chart\n\n### Deployment details\n\nAirflow 3.1.0, chart 1.18.0, deployed to Kubernetes (EKS).\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56426\nTitle: DbCallbackRequest callback_type schema constraint\nState: closed\nAuthor: gunjisairevanth\nLabels: kind:bug, area:MetaDB, priority:high, area:core\nBody:\n### Apache Airflow version\n\n3.1.0\n\n### If \"Other Airflow 2 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\nThe current schema constraint for `DbCallbackRequest.callback_type` doesn\u2019t support recording `EmailNotificationRequest` in the `callback_request` table because the string length limit is 20 characters.\n\nThis schema constraint is blocking the `_run_scheduler_loop` which is faling to emit the scheduler heartbeat.\n\ncode : https://github.com/apache/airflow/blob/3.1.0/airflow-core/src/airflow/models/db_callback_request.py#L42C28-L42C34\n\n### What you think should happen instead?\n\nAs default, it uses the `EmailNotificationRequest` for email notification. we can should increase the string length that support the native class of airflow. \n\n### How to reproduce\n\n**Prerequisites:**\nSMTP configured (or any email backend)\n\ncode \n\n```\nfrom datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\n\ndef failing_task():\n    \"\"\"This task will always fail to trigger email notification.\"\"\"\n    raise Exception(\"This task intentionally fails to reproduce the email callback issue\")\n\ndefault_args = {\n    'owner': 'test',\n    'email': ['test@example.com'],  # Configure your email here\n    'email_on_failure': True,       # This triggers the bug\n    'email_on_retry': False,\n    'retries': 0,                   # No retries to fail faster\n}\n\ndag = DAG(\n    'reproduce_email_callback_bug',\n    default_args=default_args,\n    description='Simple DAG to reproduce EmailNotificationRequest database constraint issue',\n    schedule_interval=None,         # Manual trigger only\n    start_date=datetime(2024, 1, 1),\n    catchup=False,\n)\n\n# Task that will fail and trigger email notification\nfail_task = PythonOperator(\n    task_id='intentional_failure',\n    python_callable=failing_task,\n    dag=dag,\n)\n```\n\nrun the above dag in airflow . \n\nTask fails \u2192 Scheduler tries to send email notification\nScheduler crashes with error:\n   sqlalchemy.exc.DataError: (psycopg2.errors.StringDataRightTruncation) \n   value too long for type character varying(20)\n \n\n\n### Operating System\n\nPRETTY_NAME=\"Debian GNU/Linux 12 (bookworm)\" NAME=\"Debian GNU/Linux\" VERSION_ID=\"12\" VERSION=\"12 (bookworm)\" VERSION_CODENAME=bookworm ID=debian HOME_URL=\"https://www.debian.org/\" SUPPORT_URL=\"https://www.debian.org/support\" BUG_REPORT_URL=\"https://bugs.debian.org/\"\n\n### Versions of Apache Airflow Providers\n\n_No response_\n\n### Deployment\n\nOfficial Apache Airflow Helm Chart\n\n### Deployment details\n\n_No response_\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56425\nTitle: fix: Astroid 4 breaks docs building\nState: open\nAuthor: rich7420\nLabels: area:providers, kind:documentation, provider:airbyte\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\nrelated: #56420 \r\n\r\nThis PR switch to apidoc+autodoc only for provider docs when Astroid \u2265 4 and everything else still uses AutoAPI.  On Sphinx startup, we run sphinx-apidoc to generate _api, sidestepping Astroid 4 vs AutoAPI ignore quirks.  This PR changed the ignore list (exclude get_provider_info.py, version_compat.py, etc.) that aren\u2019t useful for API and can break parsing. Also This updated Airbyte docs: Python API now links to _api/modules; System tests is a note instead of a hard toctree, avoiding dead links.  W#ithout using version pinning or downgrades, once AutoAPI supports Astroid 4, we can just remove the conditional and revert easily.\r\n\r\n### test\r\n`uv run --group docs build-docs --docs-only`\r\n\r\n<img width=\"666\" height=\"205\" alt=\"image\" src=\"https://github.com/user-attachments/assets/4c35b81c-cb1a-4d74-b8a9-92ec094ab922\" />\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56424\nTitle: Scheduler failing because of callback_request callback_type StringDataRightTruncation\nState: closed\nAuthor: qamasailer\nLabels: kind:bug, area:Scheduler, area:MetaDB, area:core, needs-triage\nBody:\n### Apache Airflow version\n\n3.1.0\n\n### If \"Other Airflow 2 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\nAfter the upgrade to Airflow 3.1.0 the failing of one of our DAGs and its email notification for that led to the scheduler crashing with the following error message.\n```\n    sys.exit(main())\n             ^^^^^^\n  File \"/home/airflow/.local/lib/python3.12/site-packages/airflow/__main__.py\", line 55, in main\n    args.func(args)\n  File \"/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py\", line 49, in command\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py\", line 114, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/providers_configuration_loader.py\", line 54, in wrapped_function\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/scheduler_command.py\", line 52, in scheduler\n    run_command_with_daemon_option(\n  File \"/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/daemon_utils.py\", line 86, in run_command_with_daemon_option\n    callback()\n  File \"/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/scheduler_command.py\", line 55, in <lambda>\n    callback=lambda: _run_scheduler_job(args),\n                     ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/scheduler_command.py\", line 43, in _run_scheduler_job\n    run_job(job=job_runner.job, execute_callable=job_runner._execute)\n  File \"/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py\", line 100, in wrapper\n    return func(*args, session=session, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/airflow/.local/lib/python3.12/site-packages/airflow/jobs/job.py\", line 368, in run_job\n    return execute_job(job, execute_callable=execute_callable)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/airflow/.local/lib/python3.12/site-packages/airflow/jobs/job.py\", line 397, in execute_job\n    ret = execute_callable()\n          ^^^^^^^^^^^^^^^^^^\n  File \"/home/airflow/.local/lib/python3.12/site-packages/airflow/jobs/scheduler_job_runner.py\", line 1042, in _execute\n    self._run_scheduler_loop()\n  File \"/home/airflow/.local/lib/python3.12/site-packages/airflow/jobs/scheduler_job_runner.py\", line 1345, in _run_scheduler_loop\n    num_finished_events += self._process_executor_events(\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/airflow/.local/lib/python3.12/site-packages/airflow/jobs/scheduler_job_runner.py\", line 745, in _process_executor_events\n    return SchedulerJobRunner.process_executor_events(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/airflow/.local/lib/python3.12/site-packages/airflow/jobs/scheduler_job_runner.py\", line 977, in process_executor_events\n    executor.send_callback(email_request)\n  File \"/home/airflow/.local/lib/python3.12/site-packages/airflow/executors/base_executor.py\", line 586, in send_callback\n    self.callback_sink.send(request)\n  File \"/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py\", line 99, in wrapper\n    with create_session() as session:\n         ^^^^^^^^^^^^^^^^\n  File \"/usr/python/lib/python3.12/contextlib.py\", line 144, in __exit__\n    next(self.gen)\n  File \"/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py\", line 43, in create_session\n    session.commit()\n  File \"/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py\", line 1454, in commit\n    self._transaction.commit(_to_root=self.future)\n  File \"/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py\", line 832, in commit\n    self._prepare_impl()\n  File \"/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py\", line 811, in _prepare_impl\n    self.session.flush()\n  File \"/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py\", line 3449, in flush\n    self._flush(objects)\n  File \"/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py\", line 3588, in _flush\n    with util.safe_reraise():\n         ^^^^^^^^^^^^^^^^^^^\n  File \"/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py\", line 70, in __exit__\n    compat.raise_(\n  File \"/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py\", line 211, in raise_\n    raise exception\n  File \"/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py\", line 3549, in _flush\n    flush_context.execute()\n  File \"/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py\", line 456, in execute\n    rec.execute(self)\n  File \"/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py\", line 630, in execute\n    util.preloaded.orm_persistence.save_obj(\n  File \"/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py\", line 245, in save_obj\n    _emit_insert_statements(\n  File \"/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py\", line 1238, in _emit_insert_statements\n    result = connection._execute_20(\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py\", line 1710, in _execute_20\n    return meth(self, args_10style, kwargs_10style, execution_options)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/sql/elements.py\", line 334, in _execute_on_connection\n    return connection._execute_clauseelement(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py\", line 1577, in _execute_clauseelement\n    ret = self._execute_context(\n          ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py\", line 1953, in _execute_context\n    self._handle_dbapi_exception(\n  File \"/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py\", line 2134, in _handle_dbapi_exception\n    util.raise_(\n File \"/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py\", line 211, in raise_\n    raise exception\n  File \"/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py\", line 1910, in _execute_context\n    self.dialect.do_execute(\n  File \"/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py\", line 736, in do_execute\n    cursor.execute(statement, parameters)\nsqlalchemy.exc.DataError: (psycopg2.errors.StringDataRightTruncation) value too long for type character varying(20)\n[SQL: INSERT INTO callback_request (created_at, priority_weight, callback_data, callback_type) VALUES (%(created_at)s, %(priority_weight)s, %(callback_data)s, %(callback_type)s) RETURNING callback_request.id]\n[parameters: {'created_at': datetime.datetime(2025, 10, 6, 7, 43, 34, 612854, tzinfo=Timezone('UTC')), 'priority_weight': 1, 'callback_data': '\"{\\\\\"filepath\\\\\":\\\\\"commons/esg_load.py\\\\\",\\\\\"bundle_name\\\\\":\\\\\"dags-folder\\\\\",\\\\\"bundle_version\\\\\":null,\\\\\"msg\\\\\":\\\\\"Executor CeleryExecutor(paralle ... (1361 characters truncated) ... :null,\\\\\"next_method\\\\\":null,\\\\\"next_kwargs\\\\\":null,\\\\\"xcom_keys_to_clear\\\\\":[],\\\\\"should_retry\\\\\":false},\\\\\"type\\\\\":\\\\\"EmailNotificationRequest\\\\\"}\"', 'callback_type': 'EmailNotificationRequest'}]\n(Background on this error at: https://sqlalche.me/e/14/9h9h)\n```\n\n\n\n### What you think should happen instead?\n\nWe made a quick-fix by changing the callback_type field to length 30, however I think that this is not a stable and good solution and should be fixed in airflow by either changing the field length in the models or by changing the value for this callback_type to something below 20 characters.\n\n### How to reproduce\n\nI tried reproducing, but I cannot get it to fail in the running system again.\nI can understand why its happening by going through the code:\n\nhttps://github.com/apache/airflow/blob/cc1a0007eb20c6c29f420e8f8867f2eaf56de787/airflow-core/src/airflow/jobs/scheduler_job_runner.py#L977\nhttps://github.com/apache/airflow/blob/main/airflow-core/src/airflow/executors/base_executor.py#L586\nhttps://github.com/apache/airflow/blob/main/airflow-core/src/airflow/callbacks/database_callback_sink.py#L38\nhttps://github.com/apache/airflow/blob/main/airflow-core/src/airflow/models/db_callback_request.py#L48\nuses the class name which is too long for the field.\n\n### Operating System\n\nDebian GNU/Linux 12 (bookworm)\n\n### Versions of Apache Airflow Providers\n\n_No response_\n\n### Deployment\n\nDocker-Compose\n\n### Deployment details\n\n_No response_\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [x] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56423\nTitle: Revert \"AIP-82: implement Google Pub/Sub message queue provider\"\nState: closed\nAuthor: jason810496\nLabels: provider:google, area:providers, area:dev-tools, kind:documentation, backport-to-v3-1-test\nBody:\nReverts apache/airflow#54494 as https://github.com/apache/airflow/pull/54494#issuecomment-3370765711 point out.",
  "Requirement ID: ISSUE-56422\nTitle: Fix NoneType error when updating serialized DAG\nState: open\nAuthor: anshuksi282-ksolves\nLabels: area:serialization\nBody:\nIssue: #56306\r\nFIX: Prevent AttributeError when updating SerializedDagModel for dynamic DAGs\r\n\r\nThis PR fixes a stability issue in `SerializedDagModel.write_dag` by adding a null check before updating an existing record for dynamic DAGs.\r\n\r\n### Context and Problem\r\nIn the section of `write_dag` dedicated to updating dynamic DAGs (i.e., when `dag_version` exists but has no task instances), the code retrieves the latest `SerializedDagModel` instance using `cls.get(dag.dag_id, session=session)`.\r\n\r\nIf `cls.get(dag.dag_id, session=session)` returns `None` (for example, due to a race condition where the record was just deleted or not found in the current session), the subsequent code attempts to modify attributes:\r\n\r\n`latest_ser_dag._data = new_serialized_dag._data`\r\n\r\nThis would raise an `AttributeError` because `latest_ser_dag` is `None`.\r\n\r\n### Solution\r\n- Added a null check to ensure that `latest_ser_dag` exists before updating its attributes.\r\n- This prevents `AttributeError` and improves the stability of DAG serialization for dynamic DAGs.\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements. See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.\r\n The ASF licenses this file under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License. You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied. See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n-->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests.\r\n-->\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\n\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\n\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\n\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\n\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56421\nTitle: Task Logs in UI No Longer Support Color/Styling (Regression from Airflow 2)\nState: open\nAuthor: aga98\nLabels: kind:bug, area:UI\nBody:\n### Apache Airflow version\n\n3.1.0\n\n### If \"Other Airflow 2 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\nIn Airflow 3.0.0+, task logs displayed in the Airflow UI no longer render colored or styled messages.\n\nIn Airflow 2.x, if a task log message contained ANSI escape codes (e.g., from using the Python logging or logs generated by dbt), the Airflow UI (specifically the Task Log view) would render these colors, significantly improving the readability and triage speed for log analysis.\n\nWith the switch to structured logging (JSON format) in Airflow 3+, the logs are now rendered from the structured data, and any color/style information present in the original log message is logged as raw.\n\nCheck this example in Airflow 3.1.0:\n\n<img width=\"1419\" height=\"239\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/101cf003-d912-4680-9982-440192c73125\" />\n\nThe color codes `[34m` and `[0m` are displayed and ignored for the formatting of the log.\n\nThis also affects formatted logs coming from dbt executions. For example:\n\n<img width=\"1089\" height=\"111\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/d9a3aa43-d5e5-4352-a335-b60ec3108f4b\" />\n\n\n### What you think should happen instead?\n\nThe Airflow UI should re-introduce the functionality to display styled/colored log messages in the Task Log view.\n\nSince the logs are now structured, the ideal solution would be to re-enable ANSI Code Interpretation: If the log message (event field in the JSON log) contains ANSI escape codes, the UI should be able to interpret and apply these styles to the rendered message, as it did in Airflow 2.\n\nCheck the same example in Airflow 2.9.3\n\n<img width=\"1327\" height=\"244\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/dea3f854-6fe8-4c45-84cf-998d7ad1f5d8\" />\n\nAnd for dbt:\n\n<img width=\"1061\" height=\"92\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/67dfb374-6638-4e0c-973c-0de8a8b26871\" />\n\n### How to reproduce\n\nThe following DAG can be added to Airflow to validate how the color codes are not rendered correctly:\n\n```python\nimport logging\nfrom datetime import datetime\n\nfrom airflow.sdk import dag, task\n# for Airflow 2\n# from airflow.decorators import dag, task\n\nlog = logging.getLogger(__name__)\n\n@dag(\n    dag_id=\"color_log_reproduction_dag\",\n    start_date=datetime(2025, 1, 1),\n    schedule=None,\n    catchup=False,\n)\ndef check_log_colors():\n    \"\"\"DAG to test rendering of ANSI-colored logs in the Airflow 3 UI.\"\"\"\n\n    @task\n    def log_colored_messages():\n        # ANSI Codes: \\033[91m = Red, \\033[93m = Yellow, \\033[0m = Reset\n        log.error(\"\\033[91mThis ERROR message should be RED in the UI.\\033[0m\")\n        log.warning(\"\\033[93mThis WARNING message should be YELLOW in the UI.\\033[0m\")\n        log.info( \"This line contains \\033[91mRED\\033[0m text mid-message.\")\n\n    log_colored_messages()\n\ncheck_log_colors()\n```\n\nOutput for Airflow 3:\n\n<img width=\"1271\" height=\"138\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/f8c895f9-95ce-4f16-a88a-448558dc0d75\" />\n\nOutput for Airflow 2:\n\n<img width=\"724\" height=\"98\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/8a62283e-7b50-44ae-9738-0a6145804cf2\" />\n\n### Operating System\n\nDebian GNU/Linux 12 (bookworm)\n\n### Versions of Apache Airflow Providers\n\n_No response_\n\n### Deployment\n\nOfficial Apache Airflow Helm Chart\n\n### Deployment details\n\n_No response_\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56420\nTitle: Astroid 4 breaks docs building\nState: open\nAuthor: potiuk\nLabels: area:dev-env, good first issue, kind:documentation\nBody:\nAstroid 4 does not play well with autoapi-ignore settins.",
  "Requirement ID: ISSUE-56419\nTitle: Limit astroid to < 4 to fix broken docs builds\nState: closed\nAuthor: amoghrajesh\nLabels: \nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\nIt seems like astroid 4.0.0 introduced breaking changes that are incompatible with the current version of sphinx-autoapi. The sphinx-autoapi extension uses astroid for static analysis of Python code to extract docstrings, and the API changes in astroid 4.x break this functionality.\r\n\r\nSince I am not an expert in sphinx, and because I do not want to break current docs build / formatting, this is the best solution from me for now atleast (cc @sunank200 could you check for the docs part?)\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56418\nTitle: fix: scheduler, triggerer, worker airflow components have inconsistent log volume\nState: open\nAuthor: rich7420\nLabels: \nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\nrelated: #56015 \r\n\r\nHow I Fixed It in this PRr\r\n\r\nI added the missing logs volume mount to the `wait-for-airflow-migrations` initContainers in these files:\r\n\r\n- chart/templates/scheduler/scheduler-deployment.yaml\r\n- chart/templates/triggerer/triggerer-deployment.yaml\r\n- chart/templates/workers/worker-deployment.yaml\r\n\r\n### Test\r\n\r\n### Test 1: Reproduce the Original Problem\r\nGrab this values file to mimic the issue:\r\n\r\n```yaml\r\n# test-original-issue.yaml\r\nscheduler:\r\n  extraVolumeMounts:\r\n  - mountPath: /usr/local/airflow/logs\r\n    name: airflow-logs\r\n  - mountPath: /tmp\r\n    name: tmp\r\n  extraVolumes:\r\n  - emptyDir: {}\r\n    name: airflow-logs\r\n  - emptyDir: {}\r\n    name: tmp\r\n\r\ntriggerer:\r\n  extraVolumeMounts:\r\n  - mountPath: /usr/local/airflow/logs\r\n    name: airflow-logs\r\n  - mountPath: /tmp\r\n    name: tmp\r\n  extraVolumes:\r\n  - emptyDir: {}\r\n    name: airflow-logs\r\n  - emptyDir: {}\r\n    name: tmp\r\n\r\nworkers:\r\n  extraVolumeMounts:\r\n  - mountPath: /usr/local/airflow/logs\r\n    name: airflow-logs\r\n  - mountPath: /tmp\r\n    name: tmp\r\n  extraVolumes:\r\n  - emptyDir: {}\r\n    name: airflow-logs\r\n  - emptyDir: {}\r\n    name: tmp\r\n\r\nsecurityContexts:\r\n  containers:\r\n    readOnlyRootFilesystem: true\r\n```\r\n\r\nRun this to check:\r\n\r\n```bash\r\nhelm template test-release chart/ -f test-original-issue.yaml | grep -A 25 \"wait-for-airflow-migrations\"\r\n```\r\n\r\nWhat you should see is all initContainers now have both the standard logs volume and any extra mounts\u2014no more mismatches.\r\n\r\n### Test 2: With Persistence Enabled\r\nTry this values file for persistent logs:\r\n\r\n```yaml\r\n# test-persistence.yaml\r\nlogs:\r\n  persistence:\r\n    enabled: true\r\n    subPath: \"airflow-logs\"\r\n```\r\n\r\nCheck it out:\r\n\r\n```bash\r\nhelm template test-release chart/ -f test-persistence.yaml | grep -A 25 \"wait-for-airflow-migrations\"\r\n```\r\n\r\nWhat you should see is InitContainers include the logs volume with the right `subPath` config.\r\n\r\n### Test 3: Just the Basics\r\nNo extra bells and whistles\u2014test the default setup:\r\n\r\n```yaml\r\n# test-minimal.yaml\r\n# Empty file, basically defaults only\r\n```\r\n\r\nRun:\r\n\r\n```bash\r\nhelm template test-release chart/ -f test-minimal.yaml | grep -A 25 \"wait-for-airflow-migrations\"\r\n```\r\n\r\nThen you should see Every initContainer has the standard logs volume mount, loud and clear.\r\n\r\n## Any Breaking Changes?\r\nNope, none at all. \r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56416\nTitle: Fix/snow trigger hook decouple\nState: closed\nAuthor: yash1thsa\nLabels: area:dev-tools, backport-to-v3-1-test\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE - #55747 - https://github.com/apache/airflow/issues/55747\r\n\r\nRoot cause : Synchronous calls triggered underneath async functions. Main issue is in Triggerer code that calls the hook. Triggerer is async by default. But when it hits the hook, the error happens during the initialization itself.\r\n\r\nAlternatives tried:\r\n\r\n1. Create a asynchronous path in trigerrer - get_request_url_header_params, get_connection, get_header etc. But this still did not stop the async to sync issue.\r\n2. Decouple hook from trigger. Handle the hook completely in operator class itself. \r\n\r\nOption#2 is cleaner and worked fine. Code has been tested in airflow UI and corresponding test cases are added for both happy path and exceptions\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---",
  "Requirement ID: ISSUE-56415\nTitle: Close German translation gaps for full UI translation 2025-10-05\nState: closed\nAuthor: jscheffl\nLabels: area:UI, area:translations, translation:de\nBody:\nSome housekeeping of German translation. Small movement since 3.1.0.\r\n\r\n@TJaniF / @m1racoli Looking forward for review!",
  "Requirement ID: ISSUE-56414\nTitle: [v3-1-test] fix: show appropriate time units in grid view (#56403)\nState: closed\nAuthor: github-actions[bot]\nLabels: area:UI, area:translations, translation:default, translation:de, translation:pl, translation:zh-TW, translation:nl, translation:he, translation:ko, translation:ar, translation:fr, translation:es, translation:tr, allow translation change, translation:ca, translation:hi, translation:hu, translation:zh-CN, translation:it, translation:pt\nBody:\n* fix: show appropriate time units in grid view\n\n* i18n: remove second translation keys\n(cherry picked from commit f10c90575d703e1c14728680d6e915c7bfa1bc65)\n\nCo-authored-by: LI,JHE-CHEN <103923510+RoyLee1224@users.noreply.github.com>",
  "Requirement ID: ISSUE-56413\nTitle: Allow sub-pages in React UI plugins\nState: closed\nAuthor: jscheffl\nLabels: type:bug-fix, area:UI, backport-to-v3-1-test\nBody:\nAs a follow-up on #55642 I noticed that one minor nit change was not made to main in PR https://github.com/apache/airflow/pull/56205 - the route definition allowing React UI Plugin pages to have sub-pages.\r\n\r\nI assume this is non-breaking (at least in my naive tests) therefore it should also be applied in my view to the next minor as bugfix. But before merge would like to have a review by @pierrejeambrun not that I mis-understood.",
  "Requirement ID: ISSUE-56412\nTitle: [DO NOT MERGE] Attempt to fix Link to Dag in Plugin + BackCompat\nState: open\nAuthor: jscheffl\nLabels: area:providers, area:UI, provider:edge\nBody:\nFollow-up attempt of #55642 in making this Backcompat to 3.1.0. Unable to make it working.",
  "Requirement ID: ISSUE-56411\nTitle: Add logs to UI route\nState: closed\nAuthor: dor-bernstein\nLabels: area:API, area:UI\nBody:",
  "Requirement ID: ISSUE-56410\nTitle: [v3-1-test] Use Task Display Name in Grid if existing (#56393)\nState: closed\nAuthor: jscheffl\nLabels: area:API, area:UI\nBody:\n* Use Task Display Name in Grid if existing\r\n\r\n* Consider short task ID w/o prefix if no display name\r\n\r\n* Add some tests with labels on tasks\r\n\r\n* Apply ruff (cherry picked from commit d43b468c1852fb94f29748126242400cf22bb411)",
  "Requirement ID: ISSUE-56409\nTitle: [v3-1-test] Make Dag Run ID visible in Dag Header Card (#56392)\nState: closed\nAuthor: github-actions[bot]\nLabels: area:UI\nBody:\n(cherry picked from commit e651ba9d148bda8104e31dcf750e9439b4ff0e0f)\n\nCo-authored-by: Jens Scheffler <95105677+jscheffl@users.noreply.github.com>",
  "Requirement ID: ISSUE-56408\nTitle: [providers-fab/v1-5] Better version check for Werkzeug\nState: open\nAuthor: potiuk\nLabels: area:providers\nBody:\nWerkzeug 3.0.0 deprecated `__version__` and recommends to use importlib version to check for version.\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56407\nTitle: [providers-fab/v1-5] Prepare FAB 1.5.4 provider documentation for release\nState: open\nAuthor: potiuk\nLabels: area:providers\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56406\nTitle: feature:Trigger form missing \"Select Recent Configurations\" from airflow 2\nState: open\nAuthor: rich7420\nLabels: area:API, area:UI, area:task-sdk\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\ncloses: #56254 \r\n\r\nAdding the dropdown allows users to quickly select and reuse previous manual\r\nDAG run configurations, improving workflow efficiency and reducing\r\nmanual input errors.\r\n\r\n- This PR has a dropdown component with auto-fill functionality\r\n- Also with tests for backend and frontend\r\n\r\nresults:\r\n![telegram-cloud-photo-size-5-6273931047833635809-y](https://github.com/user-attachments/assets/bd83c14c-0631-4031-a48b-775911706789)\r\n![telegram-cloud-photo-size-5-6273931047833635810-y](https://github.com/user-attachments/assets/d56fbae3-3dea-46b7-adfc-b1fbe34e2654)\r\n![telegram-cloud-photo-size-5-6273931047833635811-y](https://github.com/user-attachments/assets/4e4fd1ab-7edc-480d-8930-006673544c2e)\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56405\nTitle: docs: correct command to exit breeze from start-airflow to stop_airflow\nState: closed\nAuthor: Lohith625\nLabels: area:dev-tools, backport-to-v3-1-test\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n### Overview\r\nThis PR fixes an incorrect command mentioned in the contributor quickstart documentation.\r\n\r\n### Problem\r\nThe documentation currently says to exit Breeze using the command start-airflow, which is incorrect and results in a command not found error.\r\n\r\n### Fix\r\nReplaced start-airflow with the correct command stop_airflow in 03a_contributors_quick_start_beginners.rst.\r\n\r\n### Related Issue\r\nCloses #56397\r\n\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56404\nTitle: fix: Add auto-refresh functionality to Required Actions page\nState: open\nAuthor: rich7420\nLabels: area:UI, backport-to-v3-1-test\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\nrelated: #56081 \r\n\r\n- This PR adds useAutoRefresh hook to HITLTaskInstances component\r\n- Pass refetchInterval to useTaskInstanceServiceGetHitlDetails query\r\n- Enable automatic refresh every 3 seconds for HITL task instances\r\n\r\nWe don't need to press refresh-key manually.\r\n\r\nthanks for review.\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56403\nTitle: fix: show appropriate time units in grid view\nState: closed\nAuthor: RoyLee1224\nLabels: type:bug-fix, area:UI, backport-to-v3-1-test\nBody:\n## Related\r\npart of #56250\r\n\r\n## Why \r\n> The grid dag timing seems to always be in terms of seconds\r\n\r\n## Screenshots\r\n<img width=\"436\" height=\"322\" alt=\"duration_before\" src=\"https://github.com/user-attachments/assets/58e0263e-932f-4137-b4b1-73158d7738b2\" />\r\n\r\n<img width=\"436\" height=\"322\" alt=\"duration_after\" src=\"https://github.com/user-attachments/assets/359e2e0a-653f-467f-ae64-dc0e0dbcca3f\" />\r\n\r\n## Notes\r\n\r\nWe would need to remove all of the translation keys for \"seconds\"\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56402\nTitle: Fix KubernetesPodOperator termination_grace_period parameter not being applied to pod spec\nState: closed\nAuthor: HsiuChuanHsu\nLabels: area:providers, provider:cncf-kubernetes\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n**Problem**\r\nThe issue was that the `termination_grace_period` parameter in the KubernetesPodOperator was misapplied.\r\n\r\nWhile it was correctly utilized in the `on_kill()` method to specify the grace period when Airflow explicitly deleted a pod, it was not being used to set the pod's `spec.terminationGracePeriodSeconds` field during creation.\r\n\r\n**Changes: Updated `build_pod_request_obj()` method**\r\n   - Added `termination_grace_period_seconds=self.termination_grace_period` to `k8s.V1PodSpec` construction\r\n   - Now the parameter correctly sets the pod's terminationGracePeriodSeconds field\r\n   - Updated docstring to clarify that the parameter sets both:\r\n     - The pod's `terminationGracePeriodSeconds` field\r\n     - The grace period when deleting the pod via `on_kill()`\r\n\r\ncloses: #56349 \r\n\r\n\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56401\nTitle: ASSET_NOT_FOUND when passing name argument to @asset\nState: closed\nAuthor: plutaniano\nLabels: kind:bug, area:core\nBody:\n### Apache Airflow version\n\n3.1.0\n\n### If \"Other Airflow 2 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\nThe following DAG produces an error when triggered.\n\n```python\nfrom airflow.sdk import asset\n\n\n@asset(\n    schedule=None,\n    name=\"custom_name\",\n)\ndef function_name(self): ...\n```\n\n```\n[2025-10-04 22:40:51] INFO - DAG bundles loaded: dags-folder source=airflow.dag_processing.bundles.manager.DagBundlesManager loc=manager.py:179\n[2025-10-04 22:40:51] INFO - Filling up the DagBag from /opt/airflow/dags/a.py source=airflow.models.dagbag.DagBag loc=dagbag.py:593\n[2025-10-04 22:40:51] ERROR - Task failed with exception source=task loc=task_runner.py:972\nAirflowRuntimeError: ASSET_NOT_FOUND: {'name': 'function_name'}\nArquivo \"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py\", linha 920 em run\nArquivo \"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py\", linha 1307 em _execute_task\nArquivo \"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/bases/operator.py\", linha 416 em wrapper\nArquivo \"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/standard/operators/python.py\", linha 199 em execute\nArquivo \"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/definitions/asset/decorators.py\", linha 97 em determine_kwargs\nArquivo \"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/definitions/asset/decorators.py\", linha 89 em _iter_kwargs\nArquivo \"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/definitions/asset/decorators.py\", linha 81 em _fetch_asset\n```\n\n\n### What you think should happen instead?\n\nThe error shouldn't happen and the DAG should run normally.\n\n### How to reproduce\n\nIn an AIrflow 3.1.0 instance, add the following DAG and trigger it. The error will appear in the logs.\n\n```python\nfrom airflow.sdk import asset\n\n\n@asset(\n    schedule=None,\n    name=\"custom_name\",\n)\ndef function_name(self): ...\n```\n\n### Operating System\n\nDebian 12\n\n### Versions of Apache Airflow Providers\n\napache-airflow-core==3.1.0\n\n### Deployment\n\nOfficial Apache Airflow Helm Chart\n\n### Deployment details\n\n_No response_\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [x] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56400\nTitle: let PubsubPullTrigger exceptions propagate to triggerer framework\nState: open\nAuthor: dejii\nLabels: provider:google, area:providers\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n`PubsubPullTrigger` caught exceptions and yielded error events, creating unwanted DAG runs for asset watchers when exceptions are thrown in the trigger.\r\n\r\nThis fix is to let exceptions propagate to the triggerer framework, which handles them appropriately https://github.com/apache/airflow/issues/54804#issuecomment-3368644951\r\n\r\n- Removed error handling from `PubsubPullTrigger.run()`\r\n- Added tests for exception scenarios\r\n\r\nrelated: https://github.com/apache/airflow/issues/54804\r\n\r\nThis fix also enables the Google Pub/Sub messaging queue feature https://github.com/apache/airflow/pull/54494\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56399\nTitle: Fix SparkKubernetesOperator deferrable mode launcher attribute error\nState: open\nAuthor: OmarFarooq908\nLabels: area:providers, provider:cncf-kubernetes\nBody:\n- Fix AttributeError: 'SparkKubernetesOperator' object has no attribute 'launcher'\r\n- Add defensive checks in process_pod_deletion() and on_kill() methods\r\n- Handle cases where launcher attribute is not available in deferrable mode\r\n- Prevents crashes when process_pod_deletion() is called from trigger_reentry()\r\n- Maintains backward compatibility when launcher is available\r\n- Shows appropriate warning when launcher is not available\r\n\r\nFixes #56291\r\n\r\nThe issue occurs because in deferrable mode:\r\n1. execute() sets self.launcher during _setup_spark_configuration()\r\n2. execute() defers to a trigger via execute_async()\r\n3. When trigger completes, trigger_reentry() calls _clean() -> cleanup()\r\n4. cleanup() calls process_pod_deletion() but self.launcher is not available\r\n\r\nThis fix adds defensive checks to handle both scenarios:\r\n- When launcher is available (normal execution)\r\n- When launcher is not available (deferrable mode trigger_reentry)\r\n\r\nTested with full Airflow environment to ensure the fix works correctly.\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56398\nTitle: Werkzeug Version Check\nState: closed\nAuthor: utieyin\nLabels: area:providers, provider:fab\nBody:\nAdded feature detection in werkzeug fab providers to allow detecting hashing algorithms available to different versions of werkzeug\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\nAdded feature detection in werkzeug fab providers to allow detecting hashing algorithms available to different versions of werkzeug.\r\nThe current code checks the version of werkzeug and uses that to determine what hash to use. This breaks for werkzeug greater than 2.3.8 as seen here https://github.com/pallets/werkzeug/blob/504a8c4fbda9b8b2fd09e817544ffd228f23458e/CHANGES.rst#L182.\r\nThe proposed change uses feature detection to see if scrypt is available or not\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56397\nTitle: Incorrect name of the command to exit breeze\nState: closed\nAuthor: farrukh-t\nLabels: kind:bug, kind:documentation, needs-triage\nBody:\n### What do you see as an issue?\n\nThere is a line in [contribution quickstart](https://github.com/apache/airflow/blob/main/contributing-docs/03a_contributors_quick_start_beginners.rst) docs that says that in order to exit breeze one needs to type `start-airflow`:\\\nhttps://github.com/apache/airflow/blob/3a93f9b075b55fc3f657288e140cbdf46d92c56d/contributing-docs/03a_contributors_quick_start_beginners.rst?plain=1#L87\n\nHowever, I think the command to exit breeze is `stop_airflow`, not `start-airflow`.\n\nrunning `start-airflow` inside one of the tmux panes fails with the following error:\n```sh\nbash: start-airflow: command not found\n```\n\nThe intro message in the tmux pane also suggests to use `stop_airflow` to exit, not `start-airflow`:\n\n<img width=\"1393\" height=\"579\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/0669392f-c6d2-4fdb-a603-b577d6b970ca\" />\n\n### Solving the problem\n\nIn contribution quickstart on line https://github.com/apache/airflow/blob/3a93f9b075b55fc3f657288e140cbdf46d92c56d/contributing-docs/03a_contributors_quick_start_beginners.rst?plain=1#L87 `start-airflow` command should be replaced with `stop_airflow`\n\n### Anything else\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [x] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56396\nTitle: fix: Add max_retry_delay to MappedOperator model\nState: open\nAuthor: dabla\nLabels: \nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: [#56184](https://github.com/apache/airflow/issues/56184)\r\nrelated: [#56185](https://github.com/apache/airflow/pull/56185)\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\nFixes issue [#56184](https://github.com/apache/airflow/issues/56184)\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56395\nTitle: fix: apply ruff formatting\nState: open\nAuthor: isharghura\nLabels: area:providers, area:logging, provider:snowflake, provider:openlineage, provider:sftp, provider:standard, area:task-sdk\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\nEnable Even More PyDocStyle Checks #40567 \r\n@ferruzzi, sorry this PR might be a little bigger than you were asking for, was just wanting to get the number of errors to a nice even number, next PRs will be smaller\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56394\nTitle: fix: Add max_retry_delay to MappedOperator model\nState: closed\nAuthor: dabla\nLabels: area:providers, area:dev-tools, kind:documentation, backport-to-v3-1-test\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: [#56184](https://github.com/apache/airflow/issues/56184)\r\nrelated: [#56185](https://github.com/apache/airflow/pull/56185)\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\nFixes issue [#56184](https://github.com/apache/airflow/issues/56184)\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56393\nTitle: Use Task Display Name in Grid if existing\nState: closed\nAuthor: jscheffl\nLabels: area:API, type:bug-fix, area:UI, backport-to-v3-1-test\nBody:\nWhile I was prepping PR #56392 I noticed that the Task Display Name is not used (anymore?) in the Grid view. Might be a victim of refactoring... this PR adds the Task Display name back again such that \"beautiful\" task names as introduced in Airflow 2.10 are back again in Grid view.\r\n\r\nBefore:\r\n<img width=\"1215\" height=\"587\" alt=\"image\" src=\"https://github.com/user-attachments/assets/dc92a4ca-e860-44cc-a839-938ab22995b4\" />\r\n\r\nAfter:\r\n<img width=\"1215\" height=\"584\" alt=\"image\" src=\"https://github.com/user-attachments/assets/c35026ea-6b27-424d-a792-7ce7cfd65644\" />",
  "Requirement ID: ISSUE-56392\nTitle: Make Dag Run ID visible in Dag Header Card\nState: closed\nAuthor: jscheffl\nLabels: type:bug-fix, area:UI, backport-to-v3-1-test\nBody:\nI noticed that in the Dag header card there is at no place the \"Run ID\" being visible anymore. You need to click on the \"Details\" tab to see it. If you use (like us) the Run ID as an identifier to know which run a run is (and not a logical date which anyway is optional) then it is hard to see (except looking at URL) which Dag run you are looking at.\r\n\r\nAfter short discussion in https://apache-airflow.slack.com/archives/C0809U4S1Q9/p1758748483974429 I understood that this was not intended/concept but is likely nobody complained about until now.\r\n\r\nThis PR replaces the \"run after\" date with the \"run ID\".\r\n\r\nBefore:\r\n<img width=\"1350\" height=\"395\" alt=\"image\" src=\"https://github.com/user-attachments/assets/9fbb7756-5c6c-4694-ae71-2bbebdd799f3\" />\r\n\r\nAfter:\r\n<img width=\"1350\" height=\"393\" alt=\"image\" src=\"https://github.com/user-attachments/assets/43e19b78-a871-4ff4-b6ff-4cc64466d31c\" />",
  "Requirement ID: ISSUE-56391\nTitle: SSHHook - Add option to use poll\nState: open\nAuthor: punx120\nLabels: area:providers, kind:feature, needs-triage\nBody:\n### Description\n\nHi,\n\nRelated to #56372 and #56366, where i had some custom logic that kept trigger log file opened passe the completion of the trigger, one consequence was that `select` started to fail because it only supports FD up to 1024. I believe `poll` is not limited by that limitation, so I think it could be a good idea to have an option to switch to `poll` from `select` in `SSHHook`.\n\nI worked on a proof of concept and happy to create a PR if people agree.\n\nThanks,\nSylvain\n\n### Use case/motivation\n\n_No response_\n\n### Related issues\n\n#56366 #56372\n\n### Are you willing to submit a PR?\n\n- [x] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56390\nTitle: Replace start-airflow with stop_airflow in contributing docs\nState: closed\nAuthor: anujarora0502\nLabels: area:dev-tools, backport-to-v3-1-test\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\nThis Pull Request intends to replace the incorrect instruction to type `start-airflow` to exit Breeze with the correct command `stop_airflow` in the contributors quick start guide.",
  "Requirement ID: ISSUE-56389\nTitle: fix(docs): handle array parameter flattening in _get_params\nState: open\nAuthor: Programmer-RD-AI\nLabels: area:helm-chart, kind:documentation\nBody:\nThis PR addresses a TODO comment in `chart/docs/conf.py` by improving the `_get_params` function to correctly handle parameters defined as arrays in the Helm chart schema.\r\n\r\n#### **Summary**\r\n\r\n* Adds logic to detect when a schema node is of type `array`.\r\n* Ensures that array parameter names are properly suffixed with `[]` when flattened for documentation.\r\n* Refactors the existing loop to include consistent section mapping and output generation.\r\n\r\n#### **Why**\r\n\r\nPreviously, `_get_params` skipped over array-type properties, resulting in incomplete or inconsistent parameter documentation in chart docs.\r\nThis change improves schema coverage and makes the documentation generator output align with actual Helm values structure.",
  "Requirement ID: ISSUE-56388\nTitle: Intermittent DAG Task Failures with Missing Logs (\"No host supplied\") using CeleryExecutor\nState: open\nAuthor: sinaa328\nLabels: kind:bug, area:Scheduler, priority:high, area:core, needs-triage\nBody:\n### Apache Airflow version\n\n3.1.0\n\n### What happened?\n\n### Deployment Details\n- Executor: CeleryExecutor  \n- Installation method: Installed via pip (not Docker or Kubernetes)  \n\n### Issue Description\nTasks occasionally fail immediately at the first task without producing any logs. The Airflow UI shows the following error:\n`Could not read served logs: Invalid URL 'http://:8793/log/dag_id=.../task_id=.../attempt=1.log': No host supplied`\nAfter restarting Airflow services (webserver and scheduler), the problem temporarily resolves, but it reoccurs after days.\n\n### Impact\nThis issue is intermittent but recurring, making it difficult to rely on Airflow for production workloads. Even when tasks appear to succeed after a service restart, the problem resurfaces after some days, causing sudden DAG failures with no logs\n\n### Observations / Notes\n- Restarting services temporarily fixes the issue.  \n- The problem seems related to hostname resolution or worker URL caching in the webserver.  \n- I monitored memory usage and noticed that the issue appears when system memory is nearly full \u2014 it seems that hostnames or related metadata are lost or not resolved correctly when memory pressure increases.  \n- The issue occurs intermittently and is hard to reproduce consistently, but becomes more likely over time as memory usage grows.\n\n### What you think should happen instead?\n\nThe Airflow webserver should consistently resolve and serve valid log URLs for all tasks, regardless of uptime or restart state.  \nTasks should not fail immediately due to missing or malformed log endpoints, and restarts should not be required to restore normal behavior.\n\n\n### Operating System\n\nUbuntu 24.04.2 LTS\n\n### Versions of Apache Airflow Providers\n\n3.0.1\n\n### Deployment\n\nOfficial Apache Airflow Helm Chart\n\n### Deployment details\n\n_No response_\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56387\nTitle: Status of testing Providers that were prepared on October 04, 2025\nState: closed\nAuthor: eladkal\nLabels: area:providers, kind:meta, testing status\nBody:\n### Body\n\nI have a kind request for all the contributors to the latest provider distributions release.\nCould you please help us to test the RC versions of the providers?\n\nThe guidelines on how to test providers can be found in\n\n[Verify providers by contributors](https://github.com/apache/airflow/blob/main/dev/README_RELEASE_PROVIDERS.md#verify-the-release-candidate-by-contributors)\n\nLet us know in the comments, whether the issue is addressed.\n\nThese are providers that require testing as there were some substantial changes introduced:\n\n\n## Provider [fab: 3.0.0rc2](https://pypi.org/project/apache-airflow-providers-fab/3.0.0rc2)\n   - [x] [Upgrade FAB to FAB 5 (#50960)](https://github.com/apache/airflow/pull/50960): @potiuk\n   - [ ] [#55856: add if_not_exists=True to FAB migration (#56100)](https://github.com/apache/airflow/pull/56100): @plutaniano\n     Linked issues:\n       - [Linked Issue #55856](https://github.com/apache/airflow/issues/55856): @plutaniano\n   - [ ] [Add if_not_exists to index creation in migrations (#56328)](https://github.com/apache/airflow/pull/56328): @meher1993\n     Linked issues:\n       - [Linked Issue #56100](https://github.com/apache/airflow/pull/56100): @plutaniano\n       - [Linked Issue #56327](https://github.com/apache/airflow/issues/56327): @meher1993\n\n<!--\n\nNOTE TO RELEASE MANAGER:\n\nYou can move here the providers that have doc-only changes or for which changes are trivial, and\nyou could assess that they are OK.\n\n-->\nAll users involved in the PRs:\n@meher1993 @plutaniano @potiuk\n\n\n### Committer\n\n- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",
  "Requirement ID: ISSUE-56386\nTitle: [v3-1-test] Temporarily limit fastapi to less than 0.118.0 to fix CI (#56239)\nState: closed\nAuthor: jscheffl\nLabels: \nBody:\n(cherry picked from commit 0931482db14a30492662187123020d18a705799d)\r\n\r\nSame problem on main also showing in 3.1 branch\r\n\r\nSee https://github.com/apache/airflow/actions/runs/18234330395/job/51926396233",
  "Requirement ID: ISSUE-56385\nTitle: Fix providers release manager docs\nState: closed\nAuthor: eladkal\nLabels: area:dev-tools, changelog:skip\nBody:\nsome of variables were defined but not used. This PR fixes that",
  "Requirement ID: ISSUE-56384\nTitle: Prepare fab provider rc2 to release (October 2025)\nState: closed\nAuthor: eladkal\nLabels: area:providers, kind:documentation, provider:fab\nBody:",
  "Requirement ID: ISSUE-56383\nTitle: include task instance id in log printed by supervisor\nState: closed\nAuthor: rawwar\nLabels: area:task-sdk\nBody:\nCurrently, when a task finishes, the supervisor log does not provide any information about which task it is related to. Therefore, this PR includes the task instance ID. Ideally, I think the external ID makes sense here in the case of the Celery executor. However, across executors, the task instance ID makes sense.\r\n\r\nLog before change: \r\n\r\n```\r\n2025-10-04T03:24:40.460195Z [info     ] Task finished  [supervisor] duration=1.8142216550004377 exit_code=0 final_state=failed loc=supervisor.py:1889\r\n```\r\n\r\nLog after change:\r\n\r\n```\r\n2025-10-04T07:22:01.924062Z [info     ] Task finished  [supervisor] duration=2.9439707239998825 exit_code=-9 final_state=failed loc=supervisor.py:1891 task_instance_id=0199ae19-4f9a-709e-8af4-6b17ecca57c2\r\n```",
  "Requirement ID: ISSUE-56382\nTitle: Improve copy button usability on code blocks\nState: closed\nAuthor: RoyLee1224\nLabels: type:bug-fix, area:UI, backport-to-v3-1-test\nBody:\n## Related issue\r\n\r\npart of #56250\r\n\r\n## Why\r\nPreviously, for long lines of code, the copy button was pushed off-screen, forcing users to scroll horizontally to access it.\r\n\r\n## What\r\nNow, if you hover on the code block, the button is always visible on the right edge of the container, regardless of horizontal scroll position.\r\n\r\n## Screenshots\r\n\r\n\r\nhttps://github.com/user-attachments/assets/7d212c0b-c9c8-4c72-9622-f3fb3318d43b\r\n\r\n\r\nhttps://github.com/user-attachments/assets/e7b86093-979f-4cc1-9f48-40db1f44b686\r\n\r\n\r\n\r\n\r\n\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56381\nTitle: Remove broken check-providers-init-file hook from pre-commit config\nState: closed\nAuthor: valeriahernandez1\nLabels: area:dev-tools, backport-to-v3-1-test\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\nThis PR removes the 'check-providers-init-file' hook from '.pre-commit-config.yaml' because the script it references ('check_providers_init_file.py') no longer exists in the repo. Running the hook causes a failure, and this cleanup prevents future confusion for contributors.\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56380\nTitle: Fix: Correctly parse JSON for --dag_run_conf in airflow dags backfill CLI\nState: closed\nAuthor: HsiuChuanHsu\nLabels: area:CLI, backport-to-v3-1-test\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n### Description\r\nThe configuration passed to the `airflow dags backfill` command using the --dag_run_conf argument was not being correctly parsed as a JSON string.\r\n\r\n> Problem: Cause the `ValueError: not enough values to unpack (expected 2, got 1)`\r\n<img width=\"873\" height=\"679\" alt=\"Screenshot 2025-10-04 at 9 31 45\u202fAM\" src=\"https://github.com/user-attachments/assets/aa23664f-d004-45aa-b900-71e0dfc29947\" />\r\n\r\n\r\n### Changes\r\n- Parse `dag_run_conf` from JSON string to dict when provided\r\n- Add proper error handling for invalid JSON\r\n\r\nClose: #56230\r\n\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56379\nTitle: style: modify log highlight color\nState: open\nAuthor: RoyLee1224\nLabels: type:bug-fix, area:UI\nBody:\n## Related \r\n\r\npart of: #56250\r\nrelated:#56216\r\n\r\n## Why\r\n> The log highlight color is extremely low contrast.\r\n\r\n## Notes\r\n\r\n- Open to all feedback \u2014 if you have a preferred color or another suggestion, feel free to comment.\r\n- Should we also update the `Code` tab highlight (for consistency) at the same time?\r\n\r\n## Sceenshots\r\n### dark before\r\n<img width=\"1489\" height=\"810\" alt=\"dark_before\" src=\"https://github.com/user-attachments/assets/1f995676-8935-4061-aad8-389df6ed08dc\" />\r\n\r\n### dark after\r\n<img width=\"1489\" height=\"810\" alt=\"dark_after\" src=\"https://github.com/user-attachments/assets/5c146d7e-17e3-4ec0-92fe-ea44d558b013\" />\r\n\r\n### light before\r\n<img width=\"1489\" height=\"810\" alt=\"light_before\" src=\"https://github.com/user-attachments/assets/d46b9dc6-d6bc-4ac9-b667-5a86537413e6\" />\r\n\r\n### light after\r\n<img width=\"1489\" height=\"810\" alt=\" light_after\" src=\"https://github.com/user-attachments/assets/0dff0cd7-ea88-4716-8bba-fb7b0af1227a\" />\r\n\r\n\r\n\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56378\nTitle: modify grid view min width for task names\nState: closed\nAuthor: RoyLee1224\nLabels: type:bug-fix, area:UI\nBody:\n## Related issue\r\npart of #56250\r\n\r\n## Why\r\n\r\n> The minimum width of the task names seems a bit too narrow. When you load more dag runs the text gets aggressively cutoff\r\n\r\n## Screenshots\r\n\r\n\r\nhttps://github.com/user-attachments/assets/5f16d569-668b-4c51-8932-87164350c718\r\n\r\n\r\nhttps://github.com/user-attachments/assets/25161785-ff19-4986-b43e-46ed3d0358b0\r\n\r\n\r\n\r\n\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56376\nTitle: [v3-1-test] Restrict universal-pathlib 0.3.0 (#56370)\nState: closed\nAuthor: jscheffl\nLabels: \nBody:\n(cherry picked from commit 6a71094fb3377ce0323014077e702f123c71846d)",
  "Requirement ID: ISSUE-56375\nTitle: Upgrade tools on v3-1-test\nState: closed\nAuthor: jscheffl\nLabels: area:dev-tools, kind:documentation, backport-to-v3-1-test\nBody:\nTools need to be upgraded to fix CI on v3-1-test branch.\r\n\r\nSee failed run in https://github.com/apache/airflow/actions/runs/18216716053/job/51867602332",
  "Requirement ID: ISSUE-56374\nTitle: Airflow `generate_run_id` with custom table doesn't work for backfill and manual trigger dag runs\nState: open\nAuthor: infamax\nLabels: kind:bug, area:core, area:backfill, needs-triage\nBody:\n### Apache Airflow version\n\n3.1.0\n\n### If \"Other Airflow 2 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\nHello!\n\nI create custom Timetable. In [doc](https://airflow.apache.org/docs/apache-airflow/stable/howto/timetable.html#changing-generated-run-id)  timetable has method for generate custom run_id. This feature works for scheduled runs but not working for manual and backfill trigger dag runs. \n\n### What you think should happen instead?\n\nMethod `generate_run_id` for custom timetable work correct in backfill and manual triggers modes.\n\n### How to reproduce\n\nWrite airflow plugins with custom Timetable. File location in plugins folder and name `my_timetable.py`. Example like this\n\n```python\nfrom airflow.plugins_manager import AirflowPlugin\nfrom airflow.timetables.base import DataInterval\nfrom airflow.timetables.events import EventsTimetable\nfrom airflow.utils.types import DagRunType\nfrom pendulum import DateTime\n\nclass MyTimeTable(EventsTimetable):\n    def generate_run_id(self, *, run_type: DagRunType, run_after: DateTime, data_interval: DataInterval, **kwargs):\n        return data_interval.end.format(\"YYYY-MM-DD\")\n\nclass MyTimeTablePlugin(AirflowPlugin):\n    name = \"my_timetable_plugin\"\n    timetables = [MyTimeTable]\n```\n\nWrite a simple dag using this timetable. Example like this\n```\nimport pendulum\n\nfrom airflow.sdk import DAG\nfrom airflow.providers.standard.operators.bash import BashOperator\nfrom my_timetable import MyTimeTable\n\nevent_dates = [\n    pendulum.datetime(2025, 9, 5, 8, 27, 15),\n    pendulum.datetime(2025, 9, 10, 10, 27, 15),\n    pendulum.datetime(2025, 10, 5, 8, 27, 15),\n]\n\nstart_date = pendulum.datetime(2022, 1, 1)\n\n\nwith DAG(dag_id=\"my_dag\", start_date=start_date, schedule=EventsTimetable(event_dates=event_dates, restrict_to_events=True):\n    bash_operator = BashOperator(task_id=\"hello\", bash_command=\"echo hello\")\n    bash_operator\n```\n\nWhen run this code for schedule `run_id` for generate dag is correct. But when is using backfill added prefix `backfill__`  but timetable method `generate_run_id` not add this prefix :( \n\n### Operating System\n\nUbuntu 22.04\n\n### Versions of Apache Airflow Providers\n\n_No response_\n\n### Deployment\n\nOfficial Apache Airflow Helm Chart\n\n### Deployment details\n\n_No response_\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56373\nTitle: Fix custom timetable generate_run_id not called for manual triggers\nState: closed\nAuthor: NilsJPWerner\nLabels: area:API, backport-to-v3-1-test\nBody:\nIn Airflow 3.x, commit 035060d7f3 (PR #46616) changed the trigger_dag module to use DagRun.generate_run_id() instead of\r\ndag.timetable.generate_run_id() for manual DAG runs. This bypassed custom timetable logic, causing a regression from Airflow 2.x behavior.\r\n\r\nThis commit restores the use of dag.timetable.generate_run_id() for manual triggers, allowing custom timetables to control run_id generation for both scheduled and manually triggered runs.\r\n\r\nChanges:\r\n- airflow/api/common/trigger_dag.py: Changed to call dag.timetable.generate_run_id() with run_after and data_interval parameters instead of DagRun.generate_run_id()\r\n- tests/api_fastapi/core_api/routes/public/test_dag_run.py: Added test to verify custom timetable generate_run_id is called for manual triggers with both logical_date provided and null\r\n\r\nThis pattern matches how manual triggers are handled in other parts of the codebase (e.g., assets.py, scheduler_job_runner.py).\r\n\r\nFixes: #55908\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56372\nTitle: Trigger cleanup\nState: open\nAuthor: punx120\nLabels: kind:bug, pending-response, area:core, area:Triggerer, needs-triage\nBody:\n### Apache Airflow version\n\n3.1.0\n\n### If \"Other Airflow 2 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\nHi,\n\nThis is related to #56366, I would like to do some clean up in `BaseTrigger.cleanup`, the doc says that the cleanup method is run in the triggerer main event loop, but I don't think that's always the case, e.g when I mark a task as failed, I believe it gets cancelled via `future.cancel` (triggerer_job_runner.py:976), which triggers a `asyncio.CancelledError`, when i log `ayncio.current_task()` in my `cleanup` method, it shows it still the coroutine of the trigger which has just been cancelled which means it's not run in the main triggerer event loop unless i'm missing something?\n\nThanks\nSylvain\n\n### What you think should happen instead?\n\n_No response_\n\n### How to reproduce\n\nRun a trigger, mark the task as failed\n\n### Operating System\n\nlinux\n\n### Versions of Apache Airflow Providers\n\n_No response_\n\n### Deployment\n\nOfficial Apache Airflow Helm Chart\n\n### Deployment details\n\n_No response_\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [x] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56371\nTitle: Add unit tests for SQS hook\nState: open\nAuthor: TyrellHaywood\nLabels: provider:amazon, area:providers\nBody:\n* Add unit tests for create_queue and send_message methods\r\n* Add tests for queue creation with and without attributes\r\n* Add tests for message sending with attributes and delay parameters\r\n* Follow test patterns from test_ses.py and test_sns.py\r\n\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56370\nTitle: Restrict universal-pathlib 0.3.0\nState: closed\nAuthor: gopidesupavan\nLabels: \nBody:\nhttps://github.com/apache/airflow/issues/56369 created issue here, the lates version causing issue with accessing path in `__str__` function.\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56369\nTitle: Rework universal-pathlib in ObjectStoragePath\nState: open\nAuthor: gopidesupavan\nLabels: kind:meta, area:core\nBody:\n### Body\n\nhttps://github.com/apache/airflow/pull/56270#pullrequestreview-3298356544\n\nRework universal-pathlib usage, we are seeing latest version usage causing recursive error, it is due to accessing `path` in the `__str__` function in the https://github.com/apache/airflow/blob/main/task-sdk/src/airflow/sdk/io/path.py#L416\n\n### Committer\n\n- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",
  "Requirement ID: ISSUE-56367\nTitle: Improve error messages in Teradata provider\nState: closed\nAuthor: sc250072\nLabels: area:providers, kind:documentation, provider:teradata\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\nThis PR introduces changes to improve clarity and consistency of error messages in the Teradata provider to help users better understand failure contexts and assist in debugging.\r\n\r\nKey Changes:\r\n\r\n Error message updated\r\n\r\nTesting:\r\n    \r\n    Existing unit/integration tests passed successfully.\r\n    Added/updated tests to cover error message changes\r\n\r\nStatic checks, unit tests and documentation verified with \r\nhttps://github.com/Teradata/airflow/actions/runs/18223296969/job/51890694010\r\n\r\nSystem tests are getting failed due to https://github.com/apache/airflow/issues/56287. Alternatively, verified system tests as DAGs and are working fine\r\n\r\nhttps://teradata.github.io/airflow/ - System Dashboard - has failed metrics due to issue https://github.com/apache/airflow/issues/56287.\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56366\nTitle: Triggerer keeps log file handle open\nState: closed\nAuthor: punx120\nLabels: kind:bug, area:logging, area:core, area:Triggerer, needs-triage\nBody:\n### Apache Airflow version\n\n2.11.0\n\n### If \"Other Airflow 2 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\nHi,\n\nThe triggerer process keeps log file open even after the trigger is completed. This creates issues with too many files open after a few days or running. Increasing the max number of open file doesn't fix all the problem, as it breaks the `select` method which can only handle FD up to 1024 (https://github.com/python/cpython/blob/12805ef9dac1d564fef222d632dcb4063117bc32/Modules/selectmodule.c#L175).\n\nI'm not very familiar in our python manager the logger and when it closes the file it initially opened.\n\n\nThanks\nSylvain\n\n### What you think should happen instead?\n\n_No response_\n\n### How to reproduce\n\nRun a trigger and check that the file descriptor is still opened:\n```\n$ ls -lart /proc/<trigger pid>/fd\n```\n\n### Operating System\n\nlinux\n\n### Versions of Apache Airflow Providers\n\n_No response_\n\n### Deployment\n\nOfficial Apache Airflow Helm Chart\n\n### Deployment details\n\n_No response_\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [x] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56365\nTitle: feature: Add Open Lineage support for CloudDataFusionStartPipelineOperator\nState: closed\nAuthor: pawelgrochowicz\nLabels: provider:google, area:providers\nBody:\nThis PR adds OpenLineage support for CloudDataFusionStartPipelineOperator. I opened a new PR as the old one was too messy with changes not related to my code(https://github.com/apache/airflow/pull/55573)\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56364\nTitle: Virtualenv Operator Supervisor Comms bug on Connection.get\nState: open\nAuthor: ddp-ro\nLabels: kind:bug, area:core, needs-triage, area:task-sdk, affected_version:3.0, affected_version:3.1\nBody:\n### Apache Airflow version\n\n3.1.0\n\n### If \"Other Airflow 2 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\nWhen using `Connection.get('connection_name')` in a `task.virtualenv` operator, the following exception is raised: \n```\ncannot import name 'SUPERVISOR_COMMS' from 'airflow.sdk.execution_time.task_runner' (/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py)\n```\n\n### What you think should happen instead?\n\n`Connection.get('connection_name')` should return a `Connection` instance if one exists.\n\n### How to reproduce\n\n- Create a connection to test with\n```python\nfrom airflow.sdk import DAG, task\n\nfrom datetime import datetime, timedelta\n\ndefault_args = {\n    \"depends_on_past\": False,\n    \"email_on_failure\": True,\n    \"email_on_retry\": False,\n    \"retries\": 2,\n    \"retry_delay\": timedelta(seconds=1),\n}\nwith DAG(\n    \"Get_Conn\",\n    start_date=datetime(2025, 1, 1),\n    catchup=False,\n    schedule=None,\n    default_args=default_args,\n) as dag:\n\n    @task.virtualenv()\n    def get_conn():\n        from airflow.sdk import Connection\n\n        print(Connection.get(\"test_conn\"))\n\n    get_conn()\n```\n\n### Operating System\n\nDebian GNU/Linux 12 (bookworm)\n\n### Versions of Apache Airflow Providers\n\napache-airflow-providers-amazon==9.14.0\napache-airflow-providers-celery==3.12.3\napache-airflow-providers-cncf-kubernetes==10.0.0\napache-airflow-providers-common-compat==1.7.4\napache-airflow-providers-common-io==1.6.3\napache-airflow-providers-common-messaging==2.0.0\napache-airflow-providers-common-sql==1.28.1\napache-airflow-providers-docker==4.4.3\napache-airflow-providers-elasticsearch==6.3.3\napache-airflow-providers-fab==2.4.3\napache-airflow-providers-ftp==3.13.2\napache-airflow-providers-git==0.0.8\napache-airflow-providers-google==18.0.0\napache-airflow-providers-grpc==3.8.2\napache-airflow-providers-hashicorp==4.3.2\napache-airflow-providers-http==5.3.4\napache-airflow-providers-microsoft-azure==12.7.1\napache-airflow-providers-mysql==6.3.4\napache-airflow-providers-odbc==4.10.2\napache-airflow-providers-openlineage==2.7.1\napache-airflow-providers-postgres==5.11.2\napache-airflow-providers-redis==4.3.1\napache-airflow-providers-sendgrid==4.1.3\napache-airflow-providers-sftp==5.4.0\napache-airflow-providers-slack==9.3.0\napache-airflow-providers-smtp==2.2.1\napache-airflow-providers-snowflake==6.5.4\napache-airflow-providers-ssh==4.1.3\napache-airflow-providers-standard==1.8.0\n\n### Deployment\n\nDocker-Compose\n\n### Deployment details\n\ndockerfile deployed with docker compose:\n```docker\nFROM apache/airflow:3.1.0\n\nCOPY airflow_requirements.txt /opt/airflow/requirements.txt\n\nUSER root\nRUN apt-get update\nRUN apt-get -y install git\n\nUSER airflow\nRUN [\"/home/airflow/.local/bin/pip\", \"install\", \"--no-cache-dir\", \"-r\", \"/opt/airflow/requirements.txt\"]\n```\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56363\nTitle: Fix AsyncToSync call in Google provider\nState: open\nAuthor: VladaZakharova\nLabels: provider:google, area:providers\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56362\nTitle: Add a section in contributors guide about working with dags\nState: closed\nAuthor: amoghrajesh\nLabels: area:dev-tools, backport-to-v3-1-test\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\nNo guide mentions about working with dags, which is a very important aspect for development of Airflow. Adding it in the simpler guide so that developers can benefit from.\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56361\nTitle: V3 - [DISCUSS] - add variables linkage and make the schedule time react to tz changes\nState: open\nAuthor: Selva2294\nLabels: kind:feature, area:UI, needs-triage\nBody:\n### Description\nFor airflow v3.0.6\nFirstly, is that possible to add a feature to directly jump to/view the DAG's respective variable definition instead of searching for it if the dag is tagged to a variable? \n\nSecondly, the scheduled time in the UI doesn\u2019t update when I change the timezone. and show the timezone reference.\n\n--\n\n### Use case/motivation\n\n_No response_\n\n### Related issues\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56360\nTitle: \"Connection not found\" error in KubernetesPodOperator logs\nState: open\nAuthor: kosteev\nLabels: kind:bug, area:providers, area:core, provider:cncf-kubernetes, needs-triage\nBody:\n### Apache Airflow version\n\n3.1.0\n\n### If \"Other Airflow 2 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\nUsing KubernetesPodOperator in Airflow 3 generates some not-useful warning/errors.\n\n<img width=\"1866\" height=\"534\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/45aec7e4-cb09-44b9-9324-7b1f0ccf2e54\" />\n\nWhile running KubernetesPodOperator and using default kubernetes connection id \"kubernetes_default\", the Kubernetes hook will fallback to cluster-derived credentials:\nhttps://github.com/apache/airflow/blob/767ae70fbc12b80363c58bc8dfba020c5bfb26e2/providers/cncf/kubernetes/src/airflow/providers/cncf/kubernetes/hooks/kubernetes.py#L183\n\nThis works as expected, but generates warning/error messages - see screenshot above.\n\n### What you think should happen instead?\n\n_No response_\n\n### How to reproduce\n\n1. Run Airflow 3 instance\n2. Deploy DAG\n\n```\nimport datetime\n\nfrom airflow import models\nfrom airflow.providers.cncf.kubernetes.operators.pod import KubernetesPodOperator\n\nwith models.DAG(\n    dag_id=\"kpo_simple_echo\",\n    schedule=None,\n    start_date=datetime.datetime(2000, 1, 1),\n) as dag:\n  KubernetesPodOperator(\n      task_id=\"kpo_simple_echo\",\n      name=\"kpo-simple-echo\",\n      cmds=[\"bash\"],\n      arguments=[\"-c\", \"echo ZZZZZZZ\"],\n      image=\"gcr.io/gcp-runtimes/ubuntu_20_0_4\",\n      config_file=\"{path to kube config}\",\n  )\n```\n3. Observe logs\n\n### Operating System\n\nlinux\n\n### Versions of Apache Airflow Providers\n\n_No response_\n\n### Deployment\n\nGoogle Cloud Composer\n\n### Deployment details\n\n_No response_\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56359\nTitle: Add new PL translations.\nState: closed\nAuthor: potiuk\nLabels: area:UI, area:translations, translation:pl\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56358\nTitle: [v3-1-test] Fix install_java.sh (#56211)\nState: closed\nAuthor: github-actions[bot]\nLabels: area:dev-tools, backport-to-v3-1-test\nBody:\n(cherry picked from commit 767ae70fbc12b80363c58bc8dfba020c5bfb26e2)\n\nCo-authored-by: VladaZakharova <uladaz@google.com>\nCo-authored-by: Ulada Zakharava <vlada_zakharava@epam.com>",
  "Requirement ID: ISSUE-56357\nTitle: Update provider's metadata\nState: closed\nAuthor: potiuk\nLabels: \nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56356\nTitle: airflow-db-not-allowed error when importing PostgresqlHook in a jupyter notebook and executing with PapermillOperator\nState: closed\nAuthor: oggers\nLabels: kind:bug, area:providers, area:core, needs-triage\nBody:\n### Apache Airflow Provider(s)\n\npapermill\n\n### Versions of Apache Airflow Providers\n\napache-airflow-providers-papermill==3.11.3\n\n### Apache Airflow version\n\n3.0.6\n\n### Operating System\n\nDebian GNU/Linux 12 (bookworm)\n\n### Deployment\n\nOther Docker-based deployment\n\n### Deployment details\n\n_No response_\n\n### What happened\n\nI have a jupyter notebook that contains:\n\n```\nfrom airflow.providers.postgres.hooks.postgres import PostgresHook\n```\n\nwhen executing this notebook with a PapermillOperator I get the following error:\n\n```\n---------------------------------------------------------------------------\nArgumentError                             Traceback (most recent call last)\nCell In[1], line 1\n----> 1 from airflow.providers.postgres.hooks.postgres import PostgresHook\n\nFile [~/.local/lib/python3.12/site-packages/airflow/__init__.py:79](http://jupyter-new.local.zabik/user/airflow/lab/tree/bizak/dags/items_available/~/.local/lib/python3.12/site-packages/airflow/__init__.py#line=78)\n     73 # Perform side-effects unless someone has explicitly opted out before import\n     74 # WARNING: DO NOT USE THIS UNLESS YOU REALLY KNOW WHAT YOU'RE DOING.\n     75 # This environment variable prevents proper initialization, and things like\n     76 # configs, logging, the ORM, etc. will be broken. It is only useful if you only\n     77 # access certain trivial constants and free functions (e.g. `__version__`).\n     78 if not os.environ.get(\"_AIRFLOW__AS_LIBRARY\", None):\n---> 79     settings.initialize()\n     81 # Things to lazy import in form {local_name: ('target_module', 'target_name', 'deprecated')}\n     82 __lazy_imports: dict[str, tuple[str, str, bool]] = {\n     83     \"DAG\": (\".models.dag\", \"DAG\", False),\n     84     \"Asset\": (\".assets\", \"Asset\", False),\n   (...)     89     \"Dataset\": (\".sdk.definitions.asset\", \"Dataset\", True),\n     90 }\n\nFile [~/.local/lib/python3.12/site-packages/airflow/settings.py:618](http://jupyter-new.local.zabik/user/airflow/lab/tree/bizak/dags/items_available/~/.local/lib/python3.12/site-packages/airflow/settings.py#line=617), in initialize()\n    616     is_worker = os.environ.get(\"_AIRFLOW__REEXECUTED_PROCESS\") == \"1\"\n    617     if not is_worker:\n--> 618         configure_orm()\n    619 configure_action_logging()\n    621 # mask the sensitive_config_values\n\nFile [~/.local/lib/python3.12/site-packages/airflow/settings.py:359](http://jupyter-new.local.zabik/user/airflow/lab/tree/bizak/dags/items_available/~/.local/lib/python3.12/site-packages/airflow/settings.py#line=358), in configure_orm(disable_connection_pool, pool_class)\n    353 if SQL_ALCHEMY_CONN.startswith(\"sqlite\"):\n    354     # FastAPI runs sync endpoints in a separate thread. SQLite does not allow\n    355     # to use objects created in another threads by default. Allowing that in test\n    356     # to so the `test` thread and the tested endpoints can use common objects.\n    357     connect_args[\"check_same_thread\"] = False\n--> 359 engine = create_engine(SQL_ALCHEMY_CONN, connect_args=connect_args, **engine_args, future=True)\n    360 async_engine = create_async_engine(SQL_ALCHEMY_CONN_ASYNC, future=True)\n    361 AsyncSession = sessionmaker(\n    362     bind=async_engine,\n    363     autocommit=False,\n   (...)    366     expire_on_commit=False,\n    367 )\n\nFile <string>:2, in create_engine(url, **kwargs)\n\nFile [~/.local/lib/python3.12/site-packages/sqlalchemy/util/deprecations.py:375](http://jupyter-new.local.zabik/user/airflow/lab/tree/bizak/dags/items_available/~/.local/lib/python3.12/site-packages/sqlalchemy/util/deprecations.py#line=374), in deprecated_params.<locals>.decorate.<locals>.warned(fn, *args, **kwargs)\n    368     if m in kwargs:\n    369         _warn_with_version(\n    370             messages[m],\n    371             versions[m],\n    372             version_warnings[m],\n    373             stacklevel=3,\n    374         )\n--> 375 return fn(*args, **kwargs)\n\nFile [~/.local/lib/python3.12/site-packages/sqlalchemy/engine/create.py:514](http://jupyter-new.local.zabik/user/airflow/lab/tree/bizak/dags/items_available/~/.local/lib/python3.12/site-packages/sqlalchemy/engine/create.py#line=513), in create_engine(url, **kwargs)\n    511 kwargs.pop(\"empty_in_strategy\", None)\n    513 # create url.URL object\n--> 514 u = _url.make_url(url)\n    516 u, plugins, kwargs = u._instantiate_plugins(kwargs)\n    518 entrypoint = u._get_entrypoint()\n\nFile [~/.local/lib/python3.12/site-packages/sqlalchemy/engine/url.py:738](http://jupyter-new.local.zabik/user/airflow/lab/tree/bizak/dags/items_available/~/.local/lib/python3.12/site-packages/sqlalchemy/engine/url.py#line=737), in make_url(name_or_url)\n    725 \"\"\"Given a string or unicode instance, produce a new URL instance.\n    726 \n    727 \n   (...)    734 \n    735 \"\"\"\n    737 if isinstance(name_or_url, util.string_types):\n--> 738     return _parse_url(name_or_url)\n    739 else:\n    740     return name_or_url\n\nFile [~/.local/lib/python3.12/site-packages/sqlalchemy/engine/url.py:799](http://jupyter-new.local.zabik/user/airflow/lab/tree/bizak/dags/items_available/~/.local/lib/python3.12/site-packages/sqlalchemy/engine/url.py#line=798), in _parse_url(name)\n    796     return URL.create(name, **components)\n    798 else:\n--> 799     raise exc.ArgumentError(\n    800         \"Could not parse SQLAlchemy URL from string '%s'\" % name\n    801     )\n\nArgumentError: Could not parse SQLAlchemy URL from string 'airflow-db-not-allowed:///'\n```\n\n\n### What you think should happen instead\n\nIt should allow using dbhooks and airflow connection inside a jupyter notebook.\n\n### How to reproduce\n\nCreate a notebook with only:\n\n```\nfrom airflow.providers.postgres.hooks.postgres import PostgresHook\n```\n\nand execute it with **PapermillOperator**\n\nOther hooks like **MsSqlHook** also fails.\n\n### Anything else\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56355\nTitle: Bump ruff to 0.13.3 and prek to 0.2.3\nState: closed\nAuthor: amoghrajesh\nLabels: area:dev-tools, kind:documentation, backport-to-v3-1-test\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\nFix to failing CI: https://github.com/apache/airflow/actions/runs/18212127219/job/51854721856\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56354\nTitle: Update Airflow SDK Param text input to use textarea with text wrapping\nState: closed\nAuthor: dheerajturaga\nLabels: area:UI\nBody:\nLong input textstrings need to wrap so that it is easy to view the contents\r\nChanged FieldString component from Input to Textarea to enable text\r\nwrapping for better user experience when entering longer string values.\r\nRemoved datalist support as it's incompatible with textarea elements.\r\n\r\nBefore:\r\n<img width=\"1093\" height=\"718\" alt=\"image\" src=\"https://github.com/user-attachments/assets/8d57fa38-0fb9-4769-8703-668b2c22c6b6\" />\r\n\r\n\r\nAfter: \r\n<img width=\"1108\" height=\"736\" alt=\"image\" src=\"https://github.com/user-attachments/assets/0117afa5-daeb-45a6-a322-1671215c214e\" />",
  "Requirement ID: ISSUE-56353\nTitle: Add Thai UI translation\nState: open\nAuthor: zkan\nLabels: area:dev-tools, area:UI, backport-to-v3-1-test, area:translations\nBody:\nAirflow is very popular in Thailand and I think it'll be great to add Thai to the Airflow UI as well.\r\n\r\n<img width=\"1861\" height=\"929\" alt=\"Screenshot 2568-10-03 at 09 48 02\" src=\"https://github.com/user-attachments/assets/689731e0-cdde-4b62-8325-9f9d43bd8108\" />\r\n\r\n<img width=\"1268\" height=\"678\" alt=\"Screenshot 2568-10-06 at 08 56 36\" src=\"https://github.com/user-attachments/assets/052e29fe-5082-4e36-bb9f-cee717f73278\" />\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56352\nTitle: [v3-1-test] Fix failed image build in v3-1-test\nState: closed\nAuthor: potiuk\nLabels: area:providers, provider:common-sql, provider:smtp, provider:common-io, provider:common-compat, provider:standard\nBody:\nThe #56208 has broken v3-1-test because it added requirements for providers, where (at least curerntly) uv workspace in v3-1-test uses provider's depedencies from local \"providers\" folder. We are planning to remove providers from v3-1-test branch but this is not yet done and bumping providers only in \"airflow\" requireements without bumping them in pyproject.toml caused conflicts when installing airflow from local sources in CI image.\r\n\r\nbumping the versions in pyproject.toml should solve the problem until different mechanism is implemented in v3-* branches.\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56351\nTitle: Checkbox before clear task confirmation to prevent rerun of tasks in 'Running' states.\nState: open\nAuthor: KlarenceNicolasCatalan\nLabels: area:API, area:UI, area:translations, translation:default\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n### **Why**\r\n---\r\nThis pull request is a revised version of the pull request #55660 and so it is also based on issue #54379\r\n\r\n**Why another PR**?\r\nDue to the changes made in the previous pull request ( #55660 ), its design has deviated from the original PR. Therefore, it necessitated a change in PR following the new design and interaction with the user. \r\n\r\nThis PR's goal is **still the same** compared to the last one, however, its execution is now a bit different as **the modal is removed** and is replaced by a checkbox. The checkbox still serves the same purpose to prevent a running task instance from clearing, but without necessitating the user to read another prompt/ pop-up, and urging the user to make the decision before the dialog confirm instead of after.\r\n\r\n\r\n### **What**\r\n---\r\n**Difference of this PR compared to PR** #55660 :\r\n\r\n- Pop-up confirm modal is removed\r\n- Modal is replaced by a checkbox in the clear task dialog\r\n- Checkbox is checked (true) by default\r\n\r\nThis PR uses the same code for the backend flag from #55660 to enable the functionalities of the checkbox.\r\n\r\nHere is a video example of how the checkbox works:\r\n\r\nhttps://github.com/user-attachments/assets/526c4a2d-1021-4e8e-9da6-8886e2e93bcf\r\n\r\nHere is what happens when the checkbox is unchecked and goes back to the default behavior:\r\n\r\nhttps://github.com/user-attachments/assets/e064877f-3bf9-41aa-b102-50c86ce31dfa",
  "Requirement ID: ISSUE-56350\nTitle: Improve exception handling in AzureDataFactoryTrigger\nState: closed\nAuthor: m1racoli\nLabels: provider:microsoft-azure, area:providers\nBody:\nThere are instances when `AzureDataFactoryOperator` in deferrable mode fails with the following error:\r\n\r\n```\r\n[2025-09-28 09:34:12] ERROR - Task failed with exception source=task loc=task_runner.py:972\r\nAirflowException: (PipelineRunNotRunning) Pipeline run has already completed with status 'Succeeded'.Only pipeline runs that are in progress or queued can be canceled.\r\nCode: PipelineRunNotRunning\r\nMessage: Pipeline run has already completed with status 'Succeeded'.Only pipeline runs that are in progress or queued can be canceled.\r\nFile \"/usr/local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py\", line 920 in run\r\n\r\nFile \"/usr/local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py\", line 1307 in _execute_task\r\n\r\nFile \"/usr/local/lib/python3.12/site-packages/airflow/sdk/bases/operator.py\", line 1632 in resume_execution\r\n\r\nFile \"/usr/local/lib/python3.12/site-packages/airflow/providers/microsoft/azure/operators/data_factory.py\", line 256 in execute_complete\r\n```\r\n\r\nUnfortunately this error is only a result of [another error](https://github.com/apache/airflow/blob/41078f1525cccfa439af0527a46f75cc2015abdf/providers/microsoft/azure/src/airflow/providers/microsoft/azure/triggers/data_factory.py#L231) resulting in an [attempt to cancel an existing pipeline run](https://github.com/m1racoli/airflow/blob/dc2d629c4ea0b04984eea36701d1774f5695ce4e/providers/microsoft/azure/src/airflow/providers/microsoft/azure/triggers/data_factory.py#L236](https://github.com/apache/airflow/blob/41078f1525cccfa439af0527a46f75cc2015abdf/providers/microsoft/azure/src/airflow/providers/microsoft/azure/triggers/data_factory.py#L234)). The shown error message indicates that this attempt failed, because the run itself was already successful. Unfortunately that second error is the only error [reported back to the operator](https://github.com/apache/airflow/blob/41078f1525cccfa439af0527a46f75cc2015abdf/providers/microsoft/azure/src/airflow/providers/microsoft/azure/operators/data_factory.py#L256) even though it is not the initial cause of failure (which is unknown).\r\n\r\nThis PR addresses this by:\r\n\r\n* logging the initial error with stacktrace in the trigger\r\n* in case of failure when cancelling the second error is only logged, while the initial error will be reported back to the operator as a failure reason\r\n\r\nAdditionally we do not obfuscate the exception when cancelling fails, such that we get better visibility on the actual exception thrown, which may allow us to handle failed cancellations for already successful pipelines more gracefully in the future.",
  "Requirement ID: ISSUE-56349\nTitle: termination_grace_period of KubernetesPodOperator seems broken\nState: closed\nAuthor: atzannes\nLabels: kind:bug, area:providers, provider:cncf-kubernetes, needs-triage\nBody:\n### Apache Airflow Provider(s)\n\ncncf-kubernetes\n\n### Versions of Apache Airflow Providers\n\napache-airflow-providers-cncf-kubernetes\t10.5.0\n\n### Apache Airflow version\n\n3.0.2\n\n### Operating System\n\nUbuntu 24.04.3 LTS\n\n### Deployment\n\nOfficial Apache Airflow Helm Chart\n\n### Deployment details\n\n_No response_\n\n### What happened\n\nSay I pass `termination_grace_period=3` to my KubernetesPodOperator task in a DAG, and I run the DAG. When I get the pod configuration using `kubectl pod my-pod -o yaml`, I see that `terminationGracePeriodSeconds` has the default value (30), and not the value 3. When I run `kubectl delete pod my-pod`, the grace period is indeed the default amount and not the amount I specify. The same thing happens if I set `termination_grace_period=3000`, I still get the default value.\n\n### What you think should happen instead\n\nI am not sure, but I think that `termination_grace_period` should control the `terminationGracePeriodSeconds` of the pod that is created. I am able to circumvent this by using `full_pod_spec=k8s.V1PodSpec(...`, but that approach seems to have some other limitations, and at the very least it feels clunky to me. \n\n### How to reproduce\n\nReproducing is pretty straight-forward. Create a pod using a KubernetesPodOperator and set `termination_grace_period` then run the dag and inspect the pod that was created.\n\n### Anything else\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56348\nTitle: feature: Add OpenLineage support for SpannerQueryDatabaseInstanceOperator\nState: closed\nAuthor: pawelgrochowicz\nLabels: provider:google, area:providers\nBody:\nThis PR adds OpenLineage support for SpannerQueryDatabaseInstanceOperator.\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56347\nTitle: Centralize semantic tokens for Airflow UI\nState: open\nAuthor: odaneau-astro\nLabels: area:UI\nBody:\nThis PR aims to allow changing all UI colors using only semantic tokens defined in theme.ts.\r\n\r\n- Create a new structure in theme.ts following the Airflow components and move all color tokens from individual components to semantic tokens\r\n    - **Cards:** card.default.bg, card.default.border, card.default.shadow, card.elevated.bg, card.elevated.border, card.elevated.shadow, card.header.bg, card.header.text, card.header.border\r\n\r\n    - **Forms:** form.input.bg, form.input.border, form.input.focus, form.input.error, form.input.text, form.input.placeholder, form.label.text, form.label.required, form.help.text\r\n\r\n    - **Data Tables:** data-table.header.bg, data-table.header.text, data-table.header.border, data-table.row.bg.default, data-table.row.bg.hover, data-table.row.border, data-table.cell.text, data-table.cell.muted\r\n\r\n    - **Graphs:** graph.node.success, graph.node.failed, graph.node.running, graph.node.queued, graph.node.skipped, graph.node.default, graph.edge.default, graph.edge.active, graph.background, graph.bg, graph.pattern, graph.controls.bg.default, graph.controls.bg.hover, graph.minimap.bg, graph.minimap.group.odd, graph.minimap.group.even, graph.selected.stroke\r\n\r\n    - **Asset Graphs:** asset-graph.bg, asset-graph.pattern, asset-graph.controls.bg.default, asset-graph.controls.bg.hover, asset-graph.minimap.bg, asset-graph.selected.stroke\r\n\r\n    - **Charts:** trend-count-chart.success.bg, trend-count-chart.success.line, trend-count-chart.failed.bg, trend-count-chart.failed.line\r\n\r\n    - **Gantt:** gantt.grid.color, gantt.bg.selected, gantt.bg.hover\r\n\r\n    - **Navigation:** nav.bg, nav.button.bg.default, nav.button.bg.active\r\n\r\n    - **Calendar:** calendar.hour-label.color\r\n\r\n    - **UI Components:** toaster.spinner.color, filter-bar.pill.close.color, filter-bar.pill.close.bg.hover, editable-markdown-button.indicator.bg, editable-markdown-button.header.bg\r\n\r\n    - **Pages:** test-connection.icon.connected, test-connection.icon.disconnected, import-variables.spinner.color\r\n\r\n- Remove unnecessary explicit defaults\r\n\r\n- Use `colorPalette.*` tokens instead of re-specifying colors.\r\n    ```tsx\r\n    <HStack\r\n      colorPalette=\"brand\"\r\n      borderColor=\"colorPalette.emphasized\"\r\n      _hover={{ bg: \"colorPalette.subtle\" }}\r\n    >\r\n      <ActionButton colorPalette=\"danger\" />\r\n    ....\r\n    </HStack>\r\n    ```\r\n\r\n\r\n- Replace usage of `\"var(--chakra-colors-*)\"` with `useToken` and `resolveTokenValue` from `theme.ts`. This allows specifying a fallback value if the color is not defined, instead of displaying `#000`.\r\n    ```tsx\r\n    import { useToken } from \"@chakra-ui/react\";\r\n    import { resolveTokenValue } from \"src/theme\";\r\n    \r\n    const [brandColor] = useToken(\"colors\", [\"brand.solid\"])\r\n      .map(token => resolveTokenValue(token || \"oklch(0.5 0 0)\"));\r\n    ```\r\n\r\n\r\nThis is a prerequisite for https://github.com/apache/airflow/issues/53443",
  "Requirement ID: ISSUE-56346\nTitle: fix: enable api to clear ti instances by specifying map indexes\nState: open\nAuthor: kevinhongzl\nLabels: area:serialization, area:API\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\ncloses: #56289 (also close #43635)\r\n\r\n## Why this change\r\n\r\nThis PR resolves the issue where the api endpoint `clearTaskInstance` fails to operate on mapped TIs. Previously, there were cases that we could not clear mapped tasks individually even when their map indices were provided in payload.\r\n\r\n## Solution\r\n* Fix the logic for how relative tasks (upstream & downstream) are added to `task_ids`. This was the root cause of the failure to clear single mapped tasks.\r\n* Add corresponding tests.\r\n* Explicitly describe the usage of `task_ids` (i.e. how to include map indices to reference specific mapped task instances)\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56345\nTitle: Centralized Semantic Tokens\nState: closed\nAuthor: odaneau-astro\nLabels: area:UI\nBody:",
  "Requirement ID: ISSUE-56344\nTitle: Add resize function for Dag Documentation\nState: open\nAuthor: RoyLee1224\nLabels: area:UI\nBody:\n## Related\r\ncloses: #56325\r\n\r\n## Changes\r\n\r\n- Added `react-resizable` dependency to handle the resizing logic.\r\n- Set `maxConstraints`={[1200, 800]}, `minConstraints`={[512, 600]}\r\n- Improved the resize indicator visibility, as the default indicator was not easily noticeable.\r\n- Use `brand.muted` to align with Dag Note UI design\r\n\r\n## Screenshots\r\n### Before\r\n<img width=\"1920\" height=\"992\" alt=\"current\" src=\"https://github.com/user-attachments/assets/728d0d8b-371c-4d71-a4a8-66490800d9c8\" />\r\n\r\n### After\r\n\r\n\r\nhttps://github.com/user-attachments/assets/2517f0c6-96aa-4a79-bfed-19a17df8bd9c\r\n\r\n\r\n## Default resize indicator\r\n<img width=\"650\" height=\"640\" alt=\"doc_before\" src=\"https://github.com/user-attachments/assets/d3907538-832a-4610-8c5c-065adf6efd7f\" />\r\n\r\n## Styled resize indicator\r\n<img width=\"650\" height=\"640\" alt=\"doc_after\" src=\"https://github.com/user-attachments/assets/877993aa-6942-4e48-90aa-81485bb9ac55\" />\r\n\r\n\r\n\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56343\nTitle: api server config is never mounted in api server\nState: open\nAuthor: nathaliaceballoshurtado\nLabels: kind:bug, area:helm-chart, needs-triage\nBody:\n### Official Helm Chart version\n\n1.18.0 (latest released)\n\n### Apache Airflow version\n\n3.1.0\n\n### Kubernetes Version\n\n1.33\n\n### Helm Chart configuration\n\n  apiServerConfig:\n    apiServerConfig: |\n      from airflow.www.fab_security.manager import AUTH_OAUTH\n      from airflow import configuration as conf\n      SQLALCHEMY_DATABASE_URI = conf.get('core', 'SQL_ALCHEMY_CONN')\n      basedir = os.path.abspath(os.path.dirname(__file__))\n      # Flask-WTF flag for CSRF\n      WTF_CSRF_ENABLED = True\n      \n      OIDC_SCOPES = ['openid', 'email', 'profile', 'groups']\n      AUTH_TYPE = AUTH_OAUTH\n      AUTH_USER_REGISTRATION = True\n      AUTH_ROLE_ADMIN = \"Admin\"\n      PERMANENT_SESSION_LIFETIME = 43200 # in seconds(12 hours)\n      \n      # use your Airflow\u2019s Host URI\n      OVERWRITE_REDIRECT_URI = \"https://dash-airflow.cashmind.mx/oidc_callback\"\n      \n      # Here, set your Okta creds -\n      # the names of the credentials are self-explanatory. Also, we took advantage of\n      # Airflow\u2019s 'secret' section in the helm chart to retrieve those secrets from K8s-secrets.\n      # You will probably need your IT Team to get those first and then store them as K8s secrets.\n      \n      OAUTH_PROVIDERS = [\n      {\n        \"name\": \"okta\",\n        \"icon\": \"fa-circle-o\",\n        \"token_key\": \"access_token\",\n        \"remote_app\": {\n          \"client_id\": \"ofuscate\",\n          \"client_secret\": \"ofuscate\",\n          \"api_base_url\": os.getenv(\"example.okta.com/oauth2/v1/\"),\n          \"client_kwargs\": {\n            \"scope\": \"openid profile email groups\"\n          },\n          \"access_token_url\": \"https://example.okta.com/oauth2/v1/token\",\n          \"authorize_url\": \"https://example.okta.com/oauth2/v1/authorize\",\n          \"jwks_uri\": \"https://example.okta.com/oauth2/v1/keys\"\n        }\n      }]\n      \n      FAB_ADMIN_ROLE = \"Admin\"\n      FAB_VIEWER_ROLE = \"Viewer\"\n      \n      # your custom roles mapping, in the format - {Okta-Group: [Airflow-Role1,...]}\n      # here is our map of Okta Group(team) to its Airflow Role which will be used and explained later on\n      AUTH_ROLES_MAPPING = {\n        \"aws#cashmind-prod#platform-team-role#939276888979\":[\"Admin\"],\n        \"aws#cashmind-staging#stream-aligned-team-role#269031123106\":[\"stream-aligned-team-role\"],\n      }\n      \n      from airflow.www.security import AirflowSecurityManager\n      import logging\n      from typing import Dict, Any, List, Union\n      import os\n      \n      log = logging.getLogger(__name__)\n      \n      # AirflowSecurityManager Class Implementation - our custom Okta Authorizer Manager class\n      # which allows us to relate a new user into his Airflow role and save it to Airflow\u2019s DB\n      class OktaTeamAuthorizer(AirflowSecurityManager):\n      \n          def get_airflow_role(self, user_groups: List[str]) -> str:\n            for okta_group, airflow_role in AUTH_ROLES_MAPPING.items():\n              if okta_group in user_groups:\n                return okta_group\n      \n            return 'Viewer'\n      \n          def get_oauth_user_info(\n              self, provider: str, resp: Any\n          ) -> Dict[str, Union[str, List[str]]]:\n      \n              remote_app = self.appbuilder.sm.oauth_remotes[provider]\n              me = remote_app.get(\"userinfo\")\n              user_data = me.json()\n              user_groups = user_data.get(\"groups\")\n      \n              role = self.get_airflow_role(user_groups)\n      \n              # How a new user record will be saved in the DB\n              return {\n                  \"username\": \"okta_\" + user_data.get(\"preferred_username\", \"\"),\n                  \"first_name\": user_data.get(\"given_name\", \"\"),\n                  \"last_name\": user_data.get(\"family_name\", \"\"),\n                  \"email\": user_data.get(\"email\", \"\"),\n                  \"role_keys\": [role]\n              }\n      \n      # set your custom class as the security_manager_class of airflow\n      SECURITY_MANAGER_CLASS = OktaTeamAuthorizer\n\n### Docker Image customizations\n\n```FROM apache/airflow:3.1.0\n\nRUN pip install --no-cache-dir \\\n    flask-oidc2==1.5.0 \\\n    fab-oidc2==0.0.3\n```\n\n### What happened\n\nconfigmap is not being mounted in the pod despited of being specificate in the values, the configmap is never create also\n\n### What you think should happen instead\n\nconfigmap should be created\nconfigmap should be mounted in api server \n\n### How to reproduce\n\ncreating a deployment with the values and the configmap template \n\n### Anything else\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [x] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56342\nTitle: DAG run start_date and end_date set to date 55 days after task execution\nState: open\nAuthor: 1fanwang\nLabels: kind:bug, area:Scheduler, area:core, needs-triage\nBody:\n### Apache Airflow version\n\nOther Airflow 2 version (please specify below)\n\n### If \"Other Airflow 2 version\" selected, which one?\n\n2.9.2.32\n\n### What happened?\n\nWe observed a DAG run where all tasks executed and completed on July 16, 2025, but the `dag_run` table shows `start_date` and `end_date` values from September 9, 2025 - 55 days later. The scheduler logs for DAG run completion also only appear on September 9, not on the original execution date.\n\n### Observed Case\n\n**Run ID:** `manual__2025-07-16T19:50:37.477631+00:00`\n\n### Database Evidence\n\n**1. dag_run table:**\n```sql\nSELECT execution_date, start_date, end_date, state, queued_at, updated_at\nFROM dag_run \nWHERE dag_id = 'example_dag'\n  AND run_id = 'manual__2025-07-16T19:50:37.477631+00:00';\n```\n\n| execution_date | start_date | end_date | state | queued_at | updated_at |\n|---|---|---|---|---|---|\n| 2025-07-16 19:50:37.477631 | 2025-09-09 20:08:11.865410 | 2025-09-09 20:08:58.517820 | failed | 2025-09-09 18:48:25 | 2025-09-09 20:08:58.571346 |\n\n**Key observations:**\n- `execution_date`: July 16 (correct)\n- `start_date`: September 9 (55 days later)\n- `end_date`: September 9 (55 days later)\n- `queued_at`: September 9 (55 days later)\n- `updated_at`: September 9\n\n**2. task_instance table:**\n```sql\nSELECT task_id, start_date, end_date, state, duration, try_number, queued_dttm\nFROM task_instance\nWHERE dag_id = 'example_dag'\n  AND run_id = 'manual__2025-07-16T19:50:37.477631+00:00'\nORDER BY start_date;\n```\n\nAll 5 tasks show:\n- `start_date`: 2025-07-16 19:50:xx through 20:57:xx\n- `end_date`: 2025-07-16 19:50:xx through 20:57:xx  \n- `state`: failed\n- Duration: ~4000 seconds (all tasks ran for about 1 hour)\n- `try_number`: 1 (no retries)\n\n**No task activity on September 9** - all task timestamps remain July 16.\n\n### Duplicated event for the same FAILED state in our event\n\n| datepartition | dag_run_start | dag_run_end | dagrunstate | dagid | runid | host |\n|---|---|---|---|---|---|---|\n| 2025-07-16-00 | 2025-07-16T19:50:38.000 | 2025-07-16T20:57:47.000 | FAILED | example_dag | manual__2025-07-16T19:50:37.477631+00:00 | airflow-scheduler-b6b876979-rvx4d |\n| 2025-07-16-00 | 2025-07-16T19:50:38.000 | 2025-07-16T20:57:47.000 | FAILED | example_dag | manual__2025-07-16T19:50:37.477631+00:00 | airflow-scheduler-b6b876979-vr2br |\n| 2025-07-16-00 | 2025-07-16T19:50:38.000 | 2025-07-16T20:57:48.000 | FAILED | example_dag | manual__2025-07-16T19:50:37.477631+00:00 | airflow-scheduler-b6b876979-jztf9 |\n| 2025-07-16-00 | 2025-07-16T19:50:38.000 | 2025-07-16T20:57:48.000 | FAILED | example_dag | manual__2025-07-16T19:50:37.477631+00:00 | airflow-scheduler-b6b876979-4b75v |\n| 2025-07-16-00 | 2025-07-16T19:50:38.000 | 2025-07-16T20:57:49.000 | FAILED | example_dag | manual__2025-07-16T19:50:37.477631+00:00 | airflow-scheduler-b6b876979-9x5cg |\n| **2025-09-09-00** | **2025-09-09T20:08:11.000** | **2025-09-09T20:08:58.000** | FAILED | example_dag | manual__2025-07-16T19:50:37.477631+00:00 | airflow-scheduler-66b9c8684f-qbj58 |\n\n- Same `runid` (manual__2025-07-16T19:50:37.477631+00:00) appears 6 times\n- First 5 entries: July 16 timestamps from different scheduler pods\n- Last entry: September 9 timestamps (55 days later) from different scheduler pod\n- This demonstrates listener hook events (`on_dag_run_failed()`) were emitted **multiple times on July 16 - DUPLICATED** with correct timestamps, then **again on September 9** with incorrect timestamps\n\n### Scheduler Logs\n\n**September 9, 2025 20:08 (logs available):**\n```\n[2025-09-09T20:08:58.517+0000] {dagrun.py:869} ERROR - Marking run <DagRun example_dag @ 2025-07-16 19:50:37.477631+00:00: \n  manual__2025-07-16T19:50:37.477631+00:00, state:running, queued_at: 2025-09-09 18:48:25+00:00. externally triggered: True> failed\n\n[2025-09-09T20:08:58.570+0000] {dagrun.py:951} INFO - DagRun Finished: \n  dag_id=example_dag, \n  execution_date=2025-07-16 19:50:37.477631+00:00, \n  run_id=manual__2025-07-16T19:50:37.477631+00:00, \n  run_start_date=2025-09-09 20:08:11.865410+00:00, \n  run_end_date=2025-09-09 20:08:58.517820+00:00, \n  run_duration=46.65241, \n  state=failed\n```\n\nall available logs\n```\nairflow-scheduler-66b9c8684f-qbj58\t9/9/2025, 1:08:58.5178885\u202fPM\t[2025-09-09T20:08:58.517+0000] {dagrun.py:869} ERROR - Marking run <DagRun example_dag @ 2025-07-16 19:50:37.477631+00:00: manual__2025-07-16T19:50:37.477631+00:00, state:running, queued_at: 2025-09-09 18:48:25+00:00. externally triggered: True> failed\nairflow-scheduler-66b9c8684f-qbj58\t9/9/2025, 1:08:58.5708506\u202fPM\t[2025-09-09T20:08:58.570+0000] {dagrun.py:951} INFO - DagRun Finished: dag_id=example_dag, execution_date=2025-07-16 19:50:37.477631+00:00, run_id=manual__2025-07-16T19:50:37.477631+00:00, run_start_date=2025-09-09 20:08:11.865410+00:00, run_end_date=2025-09-09 20:08:58.517820+00:00, run_duration=46.65241, state=failed, external_trigger=True, run_type=manual, data_interval_start=2025-07-16 07:29:00+00:00, data_interval_end=2025-07-16 17:29:00+00:00, dag_hash=e43446634a74294f3c5a805b28d768f6\nairflow-webserver-b9c5bf7b9-zl9t5\t9/16/2025, 12:01:11.9502801\u202fPM\t2620:119:5000:1003:1cf1:1c35:38d8:8810 - - [16/Sep/2025:19:01:11 +0000] \"GET /api/v1/dags/example_dag/dagRuns/manual__2025-07-16T19:50:37.477631+00:00/taskInstances/fiftyTbShuffle4 HTTP/1.1\" 200 924 \"https://airflow.company.com/dags/example_dag/grid?execution_date=2025-09-12T06%3A12%3A44.648119%2B00%3A00&tab=details&dag_run_id=manual__2025-07-16T19%3A50%3A37.477631%2B00%3A00&num_runs=365&task_id=fiftyTbShuffle4\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36\" 0.654790 seconds\n```\n\n**July 16, 2025 (logs not available - outside retention window):**\nWe cannot retrieve scheduler logs from July 16 due to log retention policies. We do not know if any \"DagRun Finished\" or state transition logs were emitted at that time.\n\n### Audit Log Evidence\n\n```sql\nSELECT dttm, event, owner, extra\nFROM log \nWHERE dag_id = 'example_dag'\n  AND dttm BETWEEN '2025-07-15' AND '2025-09-10'\n  AND event IN ('paused', 'unpaused')\nORDER BY dttm;\n```\n\nSelected entries:\n- 2025-07-15 07:15:47 | paused | user1 | is_paused=false\n- 2025-07-15 18:21:02 | paused | user2 | is_paused=true\n- 2025-07-15 18:21:07 | paused | user2 | is_paused=false\n- 2025-07-16 21:02:38 | paused | user1 | is_paused=false\n- 2025-07-17 07:05:48 | paused | user1 | is_paused=true\n- 2025-07-25 20:13:23 | paused | user2 | is_paused=false\n- 2025-07-30 20:40:45 | paused | user1 | is_paused=false\n- 2025-07-30 20:41:32 | paused | user1 | is_paused=true\n- 2025-07-30 23:40:05 | paused | user2 | is_paused=false\n- 2025-07-31 23:00:31 | paused | user2 | is_paused=true\n- 2025-08-04 22:44:25 | paused | user2 | is_paused=false\n- 2025-08-14 18:16:57 | paused | user1 | is_paused=true\n- 2025-08-14 21:35:31 | paused | user1 | is_paused=false\n- 2025-08-25 17:18:32 | paused | user1 | is_paused=true\n- 2025-09-04 22:58:03 | paused | user1 | is_paused=true\n- 2025-09-09 03:51:45 | paused | user2 | is_paused=true\n\nThe DAG experienced multiple pause/unpause cycles between July 17 and September 9.\n\n**No manual state changes found:**\n```sql\nSELECT COUNT(*) \nFROM log \nWHERE dag_id = 'example_dag'\n  AND run_id = 'manual__2025-07-16T19:50:37.477631+00:00'\n  AND event IN ('clear', 'mark_success', 'mark_failed');\n-- Result: 0\n```\n\n### Listener Hook Observations\n\nWe have custom listener plugins that capture DAG run state changes via hooks like `on_dag_run_failed()`. For this specific run, we observed multiple \"failed\" events emitted on **July 16** (the original execution date) according to our downstream event store. However, the `dag_run` table shows the state was still \"running\" on July 16.\n\nThis raises questions about:\n- Timing of when listener hooks fire relative to database commits\n- Whether there's a race condition between event emission and state persistence\n- Why multiple \"failed\" events would fire for the same state transition\n\n### Broader Pattern\n\nQuery across all DAG runs:\n```sql\nSELECT dag_id, \n       DATE(execution_date) as exec_date,\n       DATE(start_date) as start_date,\n       DATEDIFF(start_date, execution_date) as days_delay, \n       state\nFROM dag_run \nWHERE DATEDIFF(start_date, execution_date) > 30\n  AND execution_date >= '2025-07-01'\nORDER BY days_delay DESC \nLIMIT 10;\n```\n\nResults show 10+ different DAGs exhibiting the same pattern with 30-80 day delays between `execution_date` and `start_date`/`end_date`.\n\n### What you think should happen instead?\n\nThe dag_run.start_date and dag_run.end_date fields should reflect when the tasks actually executed (July 16), not when the scheduler finally processed the run state (September 9).\nExpected behavior:\n\ndag_run.start_date should be July 16 19:50:37 (when tasks started)\ndag_run.end_date should be July 16 20:57:48 (when last task ended)\nThese timestamps should not change when the scheduler finalizes the run state weeks/months later\n\n### How to reproduce\n\nWe have not been able to reliably reproduce this, but the pattern we observed was:\n\n1. Manual DAG trigger on July 16\n2. All tasks execute and fail between July 16 19:50-20:57\n3. DAG has multiple pause/unpause operations between July 17 and September 9\n4. On September 9, scheduler emits \"DagRun Finished\" log with `run_start_date` and `run_end_date` set to September 9\n5. `dag_run` table shows September 9 for `start_date`, `end_date`, `queued_at`, and `updated_at`\n\nWe suspect this may be related to the DAG being paused while the run was in \"running\" state, but we cannot confirm this without July logs.\n\n### Operating System\n\nNAME=\"Common Base Linux Mariner\" VERSION=\"2.0.20241006\" ID=mariner VERSION_ID=\"2.0\" PRETTY_NAME=\"CBL-Mariner/Linux\"\n\n### Versions of Apache Airflow Providers\n\napache-airflow-providers-cncf-kubernetes 8.4.2\napache-airflow-providers-common-compat   1.6.1\napache-airflow-providers-common-io       1.5.4\napache-airflow-providers-common-sql      1.26.0\napache-airflow-providers-fab             1.5.3\napache-airflow-providers-ftp             3.12.2\napache-airflow-providers-http            5.2.0\napache-airflow-providers-imap            3.8.2\napache-airflow-providers-smtp            2.0.3\napache-airflow-providers-sqlite          3.3.1\n\n### Deployment\n\nOfficial Apache Airflow Helm Chart\n\n### Deployment details\n\n- Kubernetes-based deployment with separate scheduler and worker pods\n- MySQL backend database  \n- Using listener plugins to capture DAG run state changes\n\n### Anything else?\n\n**What we need help understanding:**\n\n1. **What could cause `start_date`/`end_date` to be updated 55 days after execution?** Where in the codebase would this timestamp update occur?\n\n2. **Why does `queued_at` show September 9?** The log says \"queued_at: 2025-09-09 18:48:25\" but the run was triggered manually on July 16.\n\n3. **What causes the DAG run to remain in \"running\" state for 55 days?** Task instances all completed on July 16, so what prevents the scheduler from finalizing the run?\n\n4. **Are pause/unpause operations relevant?** The audit log shows the DAG was paused the day after execution and had multiple pause cycles before September 9.\n\n5. **Why would listener hooks fire multiple times on July 16?** Our event store shows multiple \"failed\" notifications on July 16, but the DB shows \"running\" state at that time.\n\n6. **Is this expected behavior?** Should `start_date`/`end_date` represent actual task execution time or scheduler processing time?\n\n**What we've confirmed:**\n- \u2705 Tasks did not re-execute (all task_instance timestamps are July 16)\n- \u2705 No manual state changes (no clear/mark_success/mark_failed operations)\n- \u2705 Pattern affects multiple unrelated DAGs\n- \u2705 `updated_at` field confirms the record was modified on September 9\n\nWe're happy to provide additional database queries, configuration details, or test cases. We can also contribute a fix once we understand the root cause and expected behavior.\n\n### Are you willing to submit PR?\n\n- [x] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56341\nTitle: Add is_favorite to ui dags list\nState: closed\nAuthor: bbovenzi\nLabels: area:API, area:UI, backport-to-v3-1-test\nBody:\nThere was an issue that favoriting a dag in the dags list would break the sorting of the dags page. That was because we were querying the dags list inside of each Favorite Dag button and it affected the dags list cache. Instead, we should juse include `is_favorite` on the Dag response.\r\n\r\nAlso, make the Star icon stroke match the fill.\r\n\r\n\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56340\nTitle: Custom logging handler not sending logs to ClickHouse when set via LOGGING_CONFIG_CLASS\nState: open\nAuthor: serviliosouza\nLabels: kind:bug, area:logging, kind:documentation, needs-triage\nBody:\nI\u2019m trying to configure a custom logging handler in Airflow to send logs directly to ClickHouse. I followed the official documentation on advanced logging configuration:\n\nhttps://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/advanced-logging-configuration.html\n\n**Environment**\n\nImage: apache/airflow:3.1.0-python3.12\n\nExecutor: CeleryExecutor (with Redis + Postgres)\n\nDocker Compose deployment\n\nCustom handler defined in /opt/airflow/plugins/clickhouse_logging.py\n\nConfig in /opt/airflow/plugins/log_config.py, referenced with:\n\nAIRFLOW__LOGGING__LOGGING_CONFIG_CLASS=log_config.LOGGING_CONFIG (compose container environment variable)\n\n**What works:**\n\nDirect Python test inside container with ClickHouseHTTPHandler() correctly inserts rows into ClickHouse.\n\nIf I manually add the handler in a DAG (using logging.getLogger(\"airflow.task\").addHandler(ClickHouseHTTPHandler()) before send logs), logs are sent successfully to clickhouse.\n\n**What doesn\u2019t work:**\n\nWhen relying on the global LOGGING_CONFIG_CLASS setup, no logs are pushed to ClickHouse.\n\nAirflow does parse the config file: if I change the class name to a non-existent one, Airflow raises the expected error, but apparently it does not append the handler to the loggers.\n\n**Tests already done:**\n\nVerified sys.path includes /opt/airflow/plugins.\n\nCompared DEFAULT_LOGGING_CONFIG vs custom LOGGING_CONFIG.\n\nTested ClickHouseHTTPHandler manually (works).\n\nConfirmed Airflow recognizes the LOGGING_CONFIG_CLASS path (with command `airflow config get-value logging logging_config_class`).\n\nVerified connectivity and schema: inserts succeed manually into db_logs.tb_logs_airflow.\n\n**Question:**\n\nWhy are logs not being pushed when configured via LOGGING_CONFIG_CLASS, even though:\n\nThe handler is discoverable and testable.\n\nManual DAG-level attachment works.\n\nAirflow parses the config without errors.\n\nAny insights on whether there\u2019s a nuance in how Airflow attaches custom handlers globally (vs. per DAG) would be greatly appreciated.\n\nThanks in advance!\n\n### Solving the problem\n\n_No response_\n\n### Anything else\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56337\nTitle: Identify duplicate kubernetes section airflow configuration and mark them as deprecated #36389 Add deprecation warnings for Kubernetes configuration options\nState: open\nAuthor: JohannesJungbluth\nLabels: area:providers, provider:cncf-kubernetes\nBody:\nThis commit introduces a test script to verify that deprecation warnings are issued for deprecated Kubernetes configuration options. The following options are now marked as deprecated in the `[kubernetes_executor]` section: `worker_container_repository`, `worker_container_tag`, and `namespace`. Users are advised to use `pod_template_file` instead.\r\n\r\nAdditionally, the `KubeConfig` class has been updated to issue warnings when these deprecated options are used, and corresponding unit tests have been added to ensure the warnings are correctly triggered.\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56334\nTitle: [v3-1-test] Expand and collapse group component (#56293)\nState: closed\nAuthor: github-actions[bot]\nLabels: area:UI\nBody:\n* Extract component for expand and collapse button group\n\n* Refactor toggle groups buttons\n(cherry picked from commit 6d1991d2c84309a719c8e50f74dd25510ce6d47d)\n\nCo-authored-by: Pierre Jeambrun <pierrejbrun@gmail.com>",
  "Requirement ID: ISSUE-56333\nTitle: Update Breeze Option B quick setup with Buildx & Compose installation\nState: closed\nAuthor: andwct\nLabels: area:dev-tools, backport-to-v3-1-test\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56332\nTitle: Pydantic warning on Variable deserialization (PydanticSerializationUnexpectedValue)\nState: open\nAuthor: hugoarnal\nLabels: kind:bug, area:core, needs-triage\nBody:\n### Apache Airflow version\n\n3.1.0\n\n### If \"Other Airflow 2 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\nI'm currently having warnings whenever there is a deserialization of a Variable which stores a JSON array with dicts inside.\n\nI tried with also a simple array and I'm getting the same errors.\n\n```\n[2025-10-02 15:56:59] WARNING - Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected `str` - serialized value may not be as expected [input_value=<cropped>, input_type=list]) category=UserWarning source=py.warnings loc=/<cropped>/.venv/lib/python3.12/site-packages/pydantic/main.py:463\n```\n\nEdit: the `pydantic/main.py:463` goes to the `model_dump` function, just in case\n\n### What you think should happen instead?\n\nI started getting this error on all of my dags & tasks after migrating a codebase from Airflow 2.x to Airflow 3.x.\nI don't think this should happen as this didn't before on Airflow 2.x.\n\n### How to reproduce\n\nThe following example is based on a Variable storing a JSON array inside of it.\n\nHere's a code example:\n\n```py\nfrom airflow.providers.standard.operators.empty import EmptyOperator\nfrom airflow.sdk import TaskGroup, Variable, dag, task\n\nimport logging\nfrom datetime import datetime\n\nlogger = logging.getLogger(\"airflow.task\")\n\n@task(retries=3)\ndef log_stuff(stuff: any) -> None:\n    logger.info(stuff)\n\n@dag(\n    dag_id=\"variable_test\",\n    start_date=datetime(2000, 1, 1),\n    tags=[\"test\"],\n)\ndef variable_test():\n    split_stuff = EmptyOperator(task_id=\"split_stuff\")\n\n    stuff_list = Variable.get(\n        \"list_of_stuff\",\n        default=[],\n        deserialize_json=True,\n    )\n\n    for stuff in stuff_list:\n        with TaskGroup(\n            group_id=stuff\n        ) as task_group:\n            log_stuff(stuff)\n\n        split_stuff >> task_group\n\nvariable_test()\n```\n\nVariable: `list_of_stuff`\n`[\"hello_world\", \"testing\", \"last_one\"]`\n\n\nWhen using `json.loads` instead of `deserialize_json`, it doesn't show the warning. But I'm not sure it's a great idea to replace `deserialize_json` with `json.loads`. Here's the diff, if needed:\n```diff\n3a4\n> import json\n23,24c24,25\n<         default=[],\n<         deserialize_json=True,\n---\n>         default=\"[]\",\n>         deserialize_json=False,\n25a27,28\n>\n>     stuff_list = json.loads(stuff_list)\n```\n\n### Operating System\n\nUbuntu 24.04.3 LTS\n\n### Versions of Apache Airflow Providers\n\n`pip freeze | grep airflow`:\n```ini\napache-airflow==3.1.0\napache-airflow-core==3.1.0\napache-airflow-providers-common-compat==1.7.4\napache-airflow-providers-common-io==1.6.3\napache-airflow-providers-common-sql==1.28.1\napache-airflow-providers-smtp==2.3.1\napache-airflow-providers-standard==1.9.0\napache-airflow-task-sdk==1.1.0\n```\n\n`pip freeze | grep pydantic`:\n```ini\npydantic==2.11.9\npydantic_core==2.33.2\n```\n\n### Deployment\n\nVirtualenv installation\n\n### Deployment details\n\n_No response_\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56331\nTitle: API authorization issue\nState: open\nAuthor: asemelianov\nLabels: kind:bug, area:API, area:helm-chart, area:auth, area:core, needs-triage\nBody:\n### Official Helm Chart version\n\n1.15.0\n\n### Apache Airflow version\n\n2.10.3\n\n### Kubernetes Version\n\n1.30\n\n### Helm Chart configuration\n\n```\nconfig:\n    core:\n      dags_folder: '{{ include \"airflow_dags\" . }}'\n      load_examples: 'False'\n      executor: '{{ .Values.executor }}'\n      colored_console_log: 'True'\n      remote_logging: '{{- ternary \"True\" \"False\" .Values.elasticsearch.enabled }}'\n```\n\n### Docker Image customizations\n\n_No response_\n\n### What happened\n\nWhen trying to make a request, we get an error:\n\n```\nTraceback (most recent call last):\n  File \"/Users/.../.../airflow_kc/request.py\", line 34, in <module>\n    api_response.raise_for_status()\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^\n  File \"/usr/local/lib/python3.13/site-packages/requests/models.py\", line 1024, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 401 Client Error: UNAUTHORIZED for url: https://test.fart.com/api/v1/dags\n```\nOur request.py:\n\n```\nimport requests\n\nkeycloak_url = \"https://.../auth/realms/.../protocol/openid-connect/token\"\nclient_id = \"airflow\"\nclient_secret = \"\"  #\nusername = \"eas\"\npassword = ''\n\ndata = {\n    'grant_type': 'password',\n    'client_id': client_id,\n    'username': username,\n    'password': password,\n    'client_secret': client_secret,\n}\n\nif client_secret:\n    data['client_secret'] = client_secret\n\nresponse = requests.post(keycloak_url, data=data)\nresponse.raise_for_status()\n\ntoken = response.json().get('access_token')\n\nairflow_api = 'https://.../api/v1/dags'\nheaders = {\n    'Content-Type': 'application/x-www-form-urlencoded',\n    'Authorization': f'Bearer {token}'\n}\n\nprint(token)\n\napi_response = requests.get(airflow_api, headers=headers)\napi_response.raise_for_status()\n\nprint(api_response.json())\n\n```\n\nOur configmap:\n\n## airflow-cm.yaml\n```\napiVersion: v1\nkind: ConfigMap\nmetadata:\n    name: airflow-prod-api-server-config\n    namespace: airflow  # Change this to your target namespace\n\ndata:\n    webserver_config.py: |\n        from airflow.providers.fab.auth_manager.security_manager.override import FabAirflowSecurityManagerOverride\n        from base64 import b64decode\n        from cryptography.hazmat.primitives import serialization\n        from flask_appbuilder.security.manager import AUTH_DB, AUTH_OAUTH\n        from airflow import configuration as conf\n        from airflow.www.security import AirflowSecurityManager\n        import jwt\n        import logging\n        import os\n        import requests\n\n        log = logging.getLogger(__name__)\n\n\n        AUTH_TYPE = AUTH_OAUTH\n        AUTH_USER_REGISTRATION = True\n        AUTH_ROLES_SYNC_AT_LOGIN = True\n        AUTH_USER_REGISTRATION_ROLE = \"Viewer\"\n\n\n        # Make sure you create these roles on Keycloak\n        AUTH_ROLES_MAPPING = {\n            \"airflow_admin\": [\"Admin\"],\n            \"airflow_op\": [\"Op\"],\n            \"airflow_public\": [\"Public\"],\n            \"airflow_user\": [\"User\"],\n            \"airflow_viewer\": [\"Viewer\"],\n        }\n\n        PROVIDER_NAME = 'keycloak'\n        CLIENT_ID = 'airflow'\n        CLIENT_SECRET = ''\n        OIDC_ISSUER = \"https://.../auth/realms/...\"\n        AIRFLOW__API__BASE_URL = \"https://...\"\n        OIDC_BASE_URL = f\"{OIDC_ISSUER}/protocol/openid-connect\"\n        OIDC_TOKEN_URL = f\"{OIDC_BASE_URL}/token\"\n        OIDC_AUTH_URL = f\"{OIDC_BASE_URL}/auth\"\n        OIDC_METADATA_URL = f\"{OIDC_ISSUER}/.well-known/openid-configuration\"\n\n        OAUTH_PROVIDERS = [\n            {\n                \"name\": PROVIDER_NAME,\n                \"token_key\": \"access_token\",\n                \"icon\": \"fa-key\",\n                \"remote_app\": {\n                    \"api_base_url\": OIDC_BASE_URL,\n                    \"access_token_url\": OIDC_TOKEN_URL,\n                    \"authorize_url\": OIDC_AUTH_URL,\n                    \"server_metadata_url\": OIDC_METADATA_URL,\n                    \"request_token_url\": None,\n                    \"client_id\": CLIENT_ID,\n                    \"client_secret\": CLIENT_SECRET,\n                    \"client_kwargs\": {\"scope\": \"email profile\"},\n                },\n            }\n        ]\n\n        # Fetch public key\n        req = requests.get(OIDC_ISSUER)\n        key_der_base64 = req.json()[\"public_key\"]\n        key_der = b64decode(key_der_base64.encode())\n        public_key = serialization.load_der_public_key(key_der)\n\n        class CustomSecurityManager(AirflowSecurityManager):\n            def get_oauth_user_info(self, provider, response):\n                if provider == \"keycloak\":\n                    token = response[\"access_token\"]\n                    me = jwt.decode(token, public_key, algorithms=[\"HS256\", \"RS256\"])\n\n                    # Extract roles from resource access\n                    realm_access = me.get(\"realm_access\", {})\n                    groups = realm_access.get(\"roles\", [])\n\n                    log.info(f\"groups: {groups}\")\n\n                    if not groups:\n                        groups = [\"Viewer\"]\n\n                    userinfo = {\n                        \"username\": me.get(\"preferred_username\"),\n                        \"email\": me.get(\"email\"),\n                        \"first_name\": me.get(\"given_name\"),\n                        \"last_name\": me.get(\"family_name\"),\n                        \"role_keys\": groups,\n                    }\n\n                    log.info(\"user info: {0}\".format(userinfo))\n\n                    return userinfo\n                else:\n                    return {}\n\n\n        # Make sure to replace this with your own implementation of AirflowSecurityManager class\n        SECURITY_MANAGER_CLASS = CustomSecurityManager\n```\nWe also have a client \"airflow\" defined in Keyclock, and the user \"eas\" has the **airflow_admin** role assigned. I'm attaching the generated JWT:\n```\n{\n  \"alg\": \"RS256\",\n  \"typ\": \"JWT\",\n  \"kid\": \"1yFHYurR0PlyH0GbrJz3ejA9OlHE0hp83R625Uv335\"\n}\n```\n```\n{\n  \"exp\": 1759411534,\n  \"iat\": 1759411234,\n  \"jti\": \"2833b7af-ead2-4a31-9167-4b3cef387d5\",\n  \"iss\": \"https://.../auth/realms/...\",\n  \"aud\": \"airflow\",\n  \"sub\": \"eaa6b5e5-5093-4caf-b682-48effe66515\",\n  \"typ\": \"Bearer\",\n  \"azp\": \"airflow\",\n  \"sid\": \"4e221e78-9e36-40ca-9fa0-6875ffd429e\",\n  \"acr\": \"1\",\n  \"resource_access\": {\n    \"airflow\": {\n      \"roles\": [\n        \"airflow\",\n        \"airflow_admin\"\n      ]\n    }\n  },\n  \"scope\": \"email profile\",\n  \"email_verified\": false,\n  \"roles\": [\n    \"airflow\",\n    \"airflow_admin\"\n  ],\n  \"name\": \"Alex\",\n  \"preferred_username\": \"eas\",\n  \"given_name\": \"Alex,\n  \"family_name\": \"E\",\n  \"email\": \"aleks@bubu.com\"\n}\n```\n\n### What you think should happen instead\n\n_No response_\n\n### How to reproduce\n\nUsing a custom script or curl request\n\n### Anything else\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)\n\nAuthorization works through the UI, but it doesn't work through the API.",
  "Requirement ID: ISSUE-56330\nTitle: Update models to use SQLA2 annotations and synced model nullable constraints\nState: closed\nAuthor: vincbeck\nLabels: area:serialization, full tests needed, area:Triggerer, all versions\nBody:\nThis PR includes 2 changes (hence 2 commits):\r\n- Update all models to use sqlalchemy 2 annotations. The exact same change as #55954 before it got reverted in #56296\r\n- Update models to sync `nullable`  parameter with the annotation. That's the root cause why the CI started to fail after #55954 got merged.\r\n\r\nIf you have already reviewed #55954, you need to review only the [2nd commit of this PR](https://github.com/apache/airflow/pull/56330/commits/34829b9974cd16ac43ebd398b6a4ea473015ebad).\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56329\nTitle: [v3-1-test] Use TI duration from db instead of UI calculated (#56310)\nState: closed\nAuthor: github-actions[bot]\nLabels: area:UI\nBody:\n* Use TI duration from db instead of UI calculated\n\n* Fix barchart heights\n(cherry picked from commit 8847e646b802857e6665e63a010bfb24d45c1f96)\n\nCo-authored-by: Brent Bovenzi <brent@astronomer.io>",
  "Requirement ID: ISSUE-56328\nTitle: Add if_not_exists to index creation in migrations\nState: closed\nAuthor: meher1993\nLabels: area:providers, provider:fab\nBody:\ncloses:  #56327 . This is an extension of #56100\r\n\r\nAdds if_not_exists=True during index creation ensuring completion of the migration script\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56327\nTitle: FAB migration failing for existing relationships during Alter Table\nState: closed\nAuthor: meher1993\nLabels: kind:bug, area:providers, needs-triage, provider:fab, area:db-migrations\nBody:\n### Apache Airflow Provider(s)\n\nfab\n\n### Versions of Apache Airflow Providers\n\napache-airflow-providers-fab==2.4.3\n\n### Apache Airflow version\n\n3.1.0\n\n### Operating System\n\nDebian GNU/Linux 12 (bookworm)\n\n### Deployment\n\nOfficial Apache Airflow Helm Chart\n\n### Deployment details\n\nHelm official chart 1.18.0 on AKS moving airflow app version from 2.10.4 to 3.1.0.\n\n### What happened\n\nThis is an extension of #55856 \n\nI'm upgrading an Airflow instance from 2.10.4 to 3.1.0 while setting auth_manager = airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager as detailed in the migration guide.\n\nDuring the deployment, the airflow-run-airflow-migrations container is errors out with the following. As a result, the migration doesn't complete and the waitformigrations init container will keep waiting preventing the pods from coming up.\n\n\n```\nsqlalchemy.exc.ProgrammingError: (psycopg2.errors.DuplicateTable) relation \"idx_group_id\" already exists\n[SQL: CREATE INDEX idx_group_id ON ab_group_role (group_id)]\n(Background on this error at: https://sqlalche.me/e/14/f405)`\n```\n\n### What you think should happen instead\n\nFor all existing indexes,  the migration should finish successfully.\n\n### How to reproduce\n\nUpgrade an existing airflow 2.10.x version with FabAuthManager  to 3.1.0\n\n### Anything else\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [x] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56326\nTitle: feat: add async jira notifier\nState: open\nAuthor: dondaum\nLabels: area:providers, provider:atlassian-jira\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\nAdd an asynchronous version of `JiraNotfier`. As discussed in #55237, the asynchronous Notfier uses the Jira REST API and the `HttpAsyncHook` since the Jira package `atlassian-python-api` only supports sync API calls.\r\n\r\n\r\nrelated: #55237 \r\n\r\n\r\nI sucessfully tested the `JiraNotifier` with the following `Dag`:\r\n\r\n```Python\r\nfrom datetime import datetime, timedelta\r\nfrom airflow import DAG\r\nfrom airflow.sdk.definitions.deadline import AsyncCallback, DeadlineAlert, DeadlineReference\r\nfrom airflow.providers.atlassian.jira.notifications.jira import JiraNotifier\r\nfrom airflow.providers.standard.operators.empty import EmptyOperator\r\nfrom airflow.sdk import task\r\n\r\nwith DAG(\r\n    dag_id=\"deadline_alert_example\",\r\n    deadline=DeadlineAlert(\r\n        reference=DeadlineReference.DAGRUN_QUEUED_AT,\r\n        interval=timedelta(seconds=20),\r\n        callback=AsyncCallback(\r\n            JiraNotifier,\r\n            kwargs={\r\n                \"description\": \"\ud83d\udea8 Dag {{ dag_run.dag_id }} missed deadline at {{ deadline.deadline_time }}. DagRun: {{ dag_run }}\",\r\n                \"summary\": \"This is a great summary.\",\r\n                \"project_id\": XXX,\r\n                \"issue_type_id\": XXX,\r\n            },\r\n        ),\r\n    ),\r\n):\r\n    c = EmptyOperator(task_id=\"example_task\")\r\n\r\n    @task()\r\n    def wait():\r\n        import time\r\n        time.sleep(60*5)\r\n\r\n    \r\n    c >> wait()\r\n\r\n\r\n```\r\n\r\n\r\n<img width=\"1959\" height=\"559\" alt=\"Screenshot 2025-10-02 135536\" src=\"https://github.com/user-attachments/assets/aa633d2e-88f2-40a1-92f6-a77ef47512ec\" />\r\n\r\n---------------\r\n\r\n\r\n<img width=\"1327\" height=\"826\" alt=\"Screenshot 2025-10-02 135631\" src=\"https://github.com/user-attachments/assets/1159f030-d6ba-4ce2-8207-a0f9352921ec\" />\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56325\nTitle: Add option to be able to modify how the Dag Documentation pop-up is displayed\nState: open\nAuthor: Ferdinanddb\nLabels: kind:feature, good first issue, area:UI\nBody:\n### Description\n\nRight now, in Airflow 3.1.0, clicking on the Dag Documentation button of a DAG will open a pop-up window to display the DAG's documentation, but it is not possible to modify the size of this pop-up.\n\nIt would be nice to be able to modify the pop-up window.\n\nHere is a screenshot to show how the pop-up is displayed on my instance:\n\n<img width=\"1336\" height=\"457\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/11579c8d-4f74-4deb-9030-9219db773cee\" />\n\n\n### Use case/motivation\n\nI would like to be able to extend/reduce the size of the Dag Documentation pop-up window as I wish, if possible.\n\n### Related issues\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56324\nTitle: Feature/add quota project support\nState: open\nAuthor: Ankurdeewan\nLabels: provider:google, area:providers, kind:documentation\nBody:\nAdd quota/billing project support for Google Cloud providers\r\n\r\nCurrently, Airflow's Google Cloud providers lack support for specifying a quota/billing project when using Google services. This PR adds the ability to configure a separate project for API quota and billing purposes, which is particularly useful for organizations using shared service accounts.\r\n\r\nChanges:\r\n- Added quota_project_id parameter to GoogleBaseHook\r\n- Added support for quota project configuration via connection extras\r\n- Implemented proper validation of quota project IDs\r\n- Added comprehensive error handling with helpful messages\r\n- Updated UI with tooltips and descriptions\r\n- Added documentation and examples\r\n\r\nThe quota project can be specified in two ways:\r\n1. Via connection configuration (in extras):\r\n   \"quota_project_id\": \"billing-project-123\"\r\n\r\n2. Via operator parameter:\r\n   task = BigQueryExecuteQueryOperator(\r\n       quota_project_id=\"billing-project-123\",\r\n       ...\r\n   )\r\n\r\nThe implementation uses Google Auth library's with_quota_project() method to properly set the x-goog-user-project header required by Google Cloud APIs.\r\n\r\nTesting:\r\n- Added unit tests for parameter and connection-based configuration\r\n- Added validation tests\r\n- Tested with actual Google Cloud services\r\n- Verified backward compatibility\r\n\r\nDocumentation has been updated to reflect the new functionality.\r\n\r\nCloses: https://github.com/apache/airflow/issues/56311",
  "Requirement ID: ISSUE-56323\nTitle: Refactor CloudSQLDatabaseHook.create_connection \nState: closed\nAuthor: Crowiant\nLabels: provider:google, area:providers\nBody:\nIn issue: https://github.com/apache/airflow/issues/49881 it was stated that the new Connection object from airflow.sdk should not be constructed from uri as it was before but by passing all the parameters explicitly. This PR introduces refactored method create_connection in CloudSQLDatabseHook that will use new Connection starting from Airflow 3.1. For other versions of Airflow create_connection still uses the old way Connection from airflow.models\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56322\nTitle: Deployment fails with postgresql.enabled=true due to Bitnami image deprecation\nState: open\nAuthor: jjangsungwon\nLabels: kind:bug, area:helm-chart, needs-triage\nBody:\n### Official Helm Chart version\n\n1.10.0\n\n### Apache Airflow version\n\n2.9.3\n\n### Kubernetes Version\n\n1.26.4\n\n### What happened\n\nI\u2019d like to report an issue we encountered when deploying Airflow 2.9.3 with the Helm chart.\n(This is my first issue report, so please let me know if I need to provide additional information! \ud83e\udd23)\n\n#### Environment\n- Airflow version: 2.9.3\n- Helm chart version: 1.1.0\n\n#### Problem\nWhen using the default setting postgresql.enabled: true, the chart pulls the Bitnami PostgreSQL image. Recently Bitnami (VMware) restricted access and moved many images to legacy, which leads to:\n\nErrImagePull / ImagePullBackOff during deployment or rollout if the legacy registry is not explicitly configured\n\nUsing images without ongoing security updates (legacy only)\n\nIn our case, the deployment failed until we manually reconfigured the chart to point at the legacy Bitnami registry. This workaround resolves the immediate deployment issue but is not sustainable, since legacy images will no longer receive security patches and may eventually become unavailable.\n\n### How to reproduce\n\n1. Set the value postgresql.enabled: true in values.yaml.\n2. Deploy the Helm chart.\n3. Observe that the deployment fails with ErrImagePull / ImagePullBackOff due to the Bitnami PostgreSQL image being pulled from the legacy registry.\n\n### Are you willing to submit PR?\n\n- [x] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56321\nTitle: Fix grid view task ordering by correcting topological_sort implementation\nState: open\nAuthor: dheerajturaga\nLabels: kind:bug, area:serialization, type:bug-fix\nBody:\nThe SerializedTaskGroup.topological_sort() method had a critical bug in its\r\ntopological sorting algorithm. After checking if an upstream dependency's parent\r\ntask group was still in the unsorted graph, the code failed to verify whether\r\nsuch a parent was found before proceeding. This caused the else clause to execute\r\neven when nodes had unresolved parent task group dependencies, resulting in tasks\r\nbeing sorted out of dependency order in the grid view.\r\n\r\n  The fix adds the missing logic:\r\n  1. Check if a parent task group dependency exists (if tg:) and break if found\r\n  2. Track progress with an acyclic flag to detect cycles or stuck states\r\n  3. Break the loop if no nodes are resolved in an iteration\r\n\r\n  Also added the missing hierarchical_alphabetical_sort() method to support the\r\n  alternative grid_view_sorting_order configuration option.\r\n\r\n  This ensures tasks are displayed in the correct dependency order in the grid view,\r\n  matching how they are executed.\r\n\r\nBefore:\r\n\r\n<img width=\"1332\" height=\"341\" alt=\"tg_sort_graph\" src=\"https://github.com/user-attachments/assets/d2eba3fb-45dc-4e87-9913-d6e69939ac97\" />\r\n\r\n<img width=\"671\" height=\"381\" alt=\"tg_sort_grid_view_bad\" src=\"https://github.com/user-attachments/assets/dc0b6571-bd98-47d9-8ab4-794feb717f92\" />\r\n\r\n\r\nAfter:\r\n\r\n<img width=\"622\" height=\"362\" alt=\"image\" src=\"https://github.com/user-attachments/assets/91c36d2b-4439-4200-ba5b-9da4e1346c3a\" />",
  "Requirement ID: ISSUE-56320\nTitle: Enable PT011 rule to prvoider tests\nState: closed\nAuthor: xchwan\nLabels: area:providers, provider:databricks, provider:dbt-cloud, provider:dingding\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\nissue: Enable Even More PyDocStyle Checks #40567\r\n@ferruzzi\r\nThis PR is for enable PT011 rule:\r\nPT011: all instances of pytest.raises should include a match term to make sure they are catching the right error.\r\nhttps://docs.astral.sh/ruff/rules/pytest-raises-too-broad/\r\nThere are 102 files changes is need.\r\nSo, I separate to many PR, which contains about 5 file changes for easy review.\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56319\nTitle: Default parameter can create a new version of DAG after every run\nState: open\nAuthor: amit-mittal\nLabels: kind:bug, kind:feature, area:core, needs-triage\nBody:\n### Description\n\nUsing below `Param` with default value as today's date as mentioned in the [official doc](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/params.html#use-params-to-provide-a-trigger-ui-form) will create a new DAG version as part of every run on a different day.\n```\nParam(f\"{datetime.date.today()}\", type=\"string\", format=\"date\")\n```\n\nThe issue gets even worse if you accidentally use `time.time()` as the default parameter.\n\nWe should also add a `ruff lint` check to catch this.\n\n### Use case/motivation\n\n- Avoid accidental version creation as `serialized_dag` table can become really heavy\n- Official docs recommend this, but I think either it shouldn't create a version or there should be some other best practice for dates\n\n\n### Related issues\n\nA lot of versions get created for the DAG even though engineer didn't update DAG's code\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56318\nTitle: attempt to update 3.0.1 release notes for serialization interface change\nState: closed\nAuthor: sjyangkevin\nLabels: \nBody:\nRelated to: #56215 \r\n\r\nAttempt to update the release note to include the update to `deserialize` interface in the registered deserializer in `airflow.serialization.serializers` namespace as a significant change.\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56317\nTitle: [v3-1-test] Fix FAB provider name in auth manager section of release notes (#56301)\nState: closed\nAuthor: jedcunningham\nLabels: \nBody:\n(cherry picked from commit c0deb211096c260e4e8f6cd9c1e7433892932a0f)",
  "Requirement ID: ISSUE-56316\nTitle: Fix config section in warning about db scheme\nState: closed\nAuthor: jedcunningham\nLabels: \nBody:",
  "Requirement ID: ISSUE-56315\nTitle: DAG folder-level monitoring in Airflow UI\nState: closed\nAuthor: Rubackiy71\nLabels: kind:feature, area:UI\nBody:\n### Description\n\nCurrently, the Airflow UI displays DAGs as independent entities, without preserving the folder structure in which DAGs are organized on the filesystem.\n\n### Use case/motivation\n\n### Motivation\nIn many real-world projects, DAGs are organized into folders by domain (e.g.,`sales/` , `marketing/` , `finance/`). It would be very useful to see aggregated monitoring at the folder level in the Airflow UI. For example:\n\n- Show all DAGs from the folder `sales/`  grouped together.\n- Provide a \u201cRecent Tasks\u201d style view at the folder level (similar to how we currently see for each DAG).\n- Quickly see if any DAG in a folder had recent failures.\n\nThis would allow teams to monitor business-domain pipelines more effectively and would reduce the need to open multiple DAG pages.\n\n\n### Proposed behavior (high-level):\n- Display DAG folders in the UI tree, alongside individual DAGs.\n- For each folder, show aggregate DAG health (number of failed/success tasks in the last N runs).\n- Allow filtering/search by folder in addition to existing DAG tags.\n\n### Alternatives considered:\n- DAG tags (already supported), but they do not provide aggregated \u201crecent tasks\u201d views, only filtering.\n- External dashboards (Grafana/Prometheus), but it would be more convenient to have this directly in Airflow UI.\n\n### Impact:\nThis feature would improve observability and usability for teams managing dozens or hundreds of DAGs across multiple business domains.\n\n### Related issues\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56314\nTitle: Update recent runs bar chart and improve responsiveness\nState: closed\nAuthor: bbovenzi\nLabels: area:UI\nBody:\nNOTE: Merge after https://github.com/apache/airflow/pull/56310\r\n\r\nCloses https://github.com/apache/airflow/issues/56072\r\n\r\nUpdate our recent runs charts to include a state icon to be more accessible. Also fix some responsiveness with favorite dags and stats\r\n\r\n<img width=\"670\" height=\"578\" alt=\"Screenshot 2025-10-01 at 4 13 02\u202fPM\" src=\"https://github.com/user-attachments/assets/b0ca84b1-58a0-470f-b0a2-ab9e8d05fcba\" />\r\n \r\n<img width=\"1126\" height=\"1119\" alt=\"Screenshot 2025-10-01 at 4 11 02\u202fPM\" src=\"https://github.com/user-attachments/assets/bc432f32-90ef-42e7-926c-1f0dedb3500b\" />\r\n\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56313\nTitle: airflow.logging_config: set default for REMOTE_TASK_LOG\nState: open\nAuthor: mattp-\nLabels: \nBody:\nif not set, a stacktrace like the following can occur when the no-exception path is executed:\r\n```\r\n2025-10-01T20:14:42.384531Z [info     ] Remote task logs will not be available due to an error:  module 'mse.airflow.log_config' has no attribute 'REMOTE_TASK_LOG' [airflow.logging_config] loc=logging_config.py:82\r\n...\r\n    if (remote := load_remote_log_handler()) and (remote_processors := getattr(remote, \"processors\")):\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/bb/bin/package/m/mse-airflow-base/0.0.25+devise/libexec/mse-airflow-base/python/lib/python3.12/site-packages/airflow/sdk/log.py\", line 171, in load_remote_log_handler\r\n    return airflow.logging_config.REMOTE_TASK_LOG\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/bb/bin/package/m/mse-airflow-base/0.0.25+devise/libexec/mse-airflow-base/python/lib/python3.12/site-packages/airflow/logging_config.py\", line 43, in __getattr__\r\n    return REMOTE_TASK_LOG\r\n           ^^^^^^^^^^^^^^^\r\nNameError: name 'REMOTE_TASK_LOG' is not defined\r\n```\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56312\nTitle: Postgresql Error when running Airflow with official Docker Compose YAML\nState: closed\nAuthor: sunilkunchoor\nLabels: kind:bug, area:MetaDB, area:core, needs-triage\nBody:\n### Apache Airflow version\n\n3.1.0\n\n### If \"Other Airflow 2 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\nI encountered an error when trying to run Apache Airflow using the official docker-compose.yaml file provided in the [documentation](https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html). The error message is shown below:\n\n```\npostgres-1  | 2025-10-01 20:05:44.297 UTC [1] FATAL:  database files are incompatible with server\npostgres-1  | 2025-10-01 20:05:44.297 UTC [1] DETAIL:  The data directory was initialized by PostgreSQL version 13, which is not compatible with this version 16.10 (Debian 16.10-1.pgdg13+1).\npostgres-1 exited with code 1 (restarting)\ndependency failed to start: container airflow-postgres-1 is unhealthy\n```\n\n**Error Screenshot:**\n<img width=\"1153\" height=\"603\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/73873e41-dcb4-450e-8236-542dece0a8e6\" />\n\n### What you think should happen instead?\n\nRunning docker compose up should start all Airflow components without any errors.\n\n### How to reproduce\n\nStep 1: Download the docker-compose.yaml as below.\n```\ncurl -LfO 'https://airflow.apache.org/docs/apache-airflow/3.1.0/docker-compose.yaml'\n```\n\nStep 2: Run the docker compose up command.\n```\ndocker compose up\n```\n\n### Operating System\n\nUbuntu 22.04.5 LTS\n\n### Versions of Apache Airflow Providers\n\n_No response_\n\n### Deployment\n\nDocker-Compose\n\n### Deployment details\n\n_No response_\n\n### Anything else?\n\nThis is my first time creating a GitHub Issue, and I'm excited to contribute to open source. Apologies in advance if this turns out to be a simple mistake\u2014I'm eager to learn and help improve the project!\n\n### Are you willing to submit PR?\n\n- [x] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56311\nTitle: Add quota/billing project support for Google Service Account connection & hooks (x-goog-user-project header in REST APIs)\nState: open\nAuthor: adamides\nLabels: provider:google, area:providers, kind:feature, needs-triage\nBody:\n### Description\n\nCurrently, Airflow\u2019s Google Cloud provider does not allow users to specify a quota project/billing project when using Google services (such as Dataflow, BigQuery, GCS, etc) via hooks/operators. Google Cloud APIs require the x-goog-user-project header to be set when the quota project is different from the project_id. There is no way to set this in Airflow via connection or operator arguments.\n\n_Setting the project_id does not work when the service account is from a different project_\n\n### Use case/motivation\n\nMany organizations use shared service accounts that cannot be billed directly and require a separate billing project.\n\nSome workflows require explicit quota/billing project overrides depending on how APIs are enabled and/or where service accounts are maintained.  Current workaround requires subclassing operators/hooks, copying code, etc.\n\nJust like how the project_id can be overridden when creating a google cloud connection, and in the Operator itself.  There should also be an option to specify the quota project.\n\nIn addition to the project_id, one could specify quota_project_id in the Operator.  Ultimately this results in an HTTP call against the GCP REST API which supports this as the \"x-goog-user-project\" header.\n\nThe google auth library for python also supports with_quota_project which returns a copy of the credentials containing the modified quota project.\n\n### Related issues\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56310\nTitle: Use TI duration from db instead of UI calculated\nState: closed\nAuthor: bbovenzi\nLabels: area:UI, backport-to-v3-1-test\nBody:\nFixes https://github.com/apache/airflow/issues/55144 where the calculated duration in the frontend wasn't true to the Task Instance duration in the database.\r\n\r\nAlso, changes our `renderDuration()` function to return undefined to make it easier to spot missing durations in a table.\r\n\r\nFinally, add `duration` as the accessor on the TaskInstances table to allow a user to sort by it.\r\n\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56308\nTitle: [AWS System Tests] Add task retries to deletion of EKS resources\nState: closed\nAuthor: ramitkataria\nLabels: provider:amazon, area:providers\nBody:\nEKS test sometimes fails with exceptions like:\r\n```\r\nbotocore.errorfactory.ResourceInUseException: An error occurred (ResourceInUseException) when calling the DeleteFargateProfile operation: Cannot Delete Fargate Profile [profile name] because cluster [cluster name] currently has an update in progress\r\n```\r\n\r\nThis is (a potentially temporary) fix for that error.\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56307\nTitle: Default checkPendingRuns to false\nState: closed\nAuthor: bbovenzi\nLabels: area:UI\nBody:\nIn my auto refresh updates, I missed that `enabled: undefined` will default to true which is not what we want in this case. Now we explicitly default to false.\r\n\r\n\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56306\nTitle: DAG import error after v3 upgrade\nState: open\nAuthor: snowsky\nLabels: kind:bug, area:MetaDB, area:db-migrations\nBody:\nHi community,\n\nI run into the same error after Airflow was upgraded to version 3, and worked around it by updating the `dag_id` parameter with a different value. I did delete all entries in `serialized_dag` table but not `dag_code` table. I guess both ways may trigger the same logic in the codes to work around the error. Just to see if someone happens to know the cause.\n\n> Is there a way to fix this manually?\n> \n> airflow db migrate was failing (no dag_id in the dag_code table). I ended up deleting rows in the dag_code and serialized_dag tables but now my DAG processor has these errors:\n> \n> ```\n>     latest_ser_dag._data = new_serialized_dag._data\n>     ^^^^^^^^^^^^^^^^^^^^\n> AttributeError: 'NoneType' object has no attribute '_data'\n> ```\n> \n> I tried entering the dag processor pod and running `airflow dags reserialize` but no luck.\n\nIf you have access to DB, truncate `dag_code` and `serialized_dag` table.\n\n_Originally posted by @kaxil in https://github.com/apache/airflow/issues/49563#issuecomment-2857985521_",
  "Requirement ID: ISSUE-56305\nTitle: Build correct SQLAlchemy URI in Teradata Hook\nState: closed\nAuthor: sc250072\nLabels: area:providers, provider:teradata\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\nRelated: https://github.com/apache/airflow/issues/38195\r\n\r\nWhy\r\nThe default implementation of DbApiHook.get_uri does not conform to the standard Teradata connection format.\r\n\r\nHow\r\nOverride sqlalchemy_url property to follow the official Teradata SQLAlchemy URI format documented here:\r\nhttps://pypi.org/project/teradatasqlalchemy/#ConnectionParameters\r\n\r\nReturn the properly rendered SQLAlchemy URL in get_uri.",
  "Requirement ID: ISSUE-56304\nTitle: Avoid using rem for icons for safari compatibility\nState: closed\nAuthor: bbovenzi\nLabels: area:UI\nBody:\nFixes #56235\r\n\r\nAvoid using `rem` values in our icons because of compatibility issues with safari.\r\n\r\nAlso added a new eslint rule to enforce and fix it.\r\n\r\n\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56303\nTitle: 56058: Validating latestRun before accessing its properties\nState: closed\nAuthor: viveknanda\nLabels: area:UI, backport-to-v3-1-test\nBody:\ncloses: #[56058](https://github.com/apache/airflow/issues/56058)\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\nThe change in this PR aims to validate that `latestRun` is defined before invoking the `isStatePending` function with its _state_ prop. The `isStatePending` function has a broad `falsy` check and returns `true` for states which are `null` or `undefined`. This lead the `isRefreshing` prop to be set to `true` and for the loading indicator to show up.",
  "Requirement ID: ISSUE-56302\nTitle: nit: Bump required OL client to 1.38 for Openlineage provider\nState: closed\nAuthor: kacpermuda\nLabels: area:providers, provider:openlineage\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\nJust a regular bump of OL client dependency.\r\n\r\nAlso fixing an OL related dbt cloud test, our mock of API response was not full, it does contain a name ([api doc](https://docs.getdbt.com/dbt-cloud/api-v3#/operations/Retrieve%20Project)).\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56301\nTitle: Fix FAB provider name in auth manager section of release notes\nState: closed\nAuthor: jedcunningham\nLabels: backport-to-v3-1-test\nBody:",
  "Requirement ID: ISSUE-56300\nTitle: [v3-1-test] Fix multi-line drag selection in task log view (#56238)\nState: closed\nAuthor: github-actions[bot]\nLabels: area:UI\nBody:\n(cherry picked from commit 5f8eff1d34d60a5d98d21b770f2b215f64e9a6ab)\n\nCo-authored-by: Brunda10 <brunda.n@zemosolabs.com>\nCo-authored-by: Brent Bovenzi <brent@astronomer.io>",
  "Requirement ID: ISSUE-56299\nTitle: Airflowignore - order of parsing is inverted\nState: open\nAuthor: kennethz3\nLabels: kind:bug, area:core, needs-triage\nBody:\n### Apache Airflow version\n\n3.1.0\n\n### If \"Other Airflow 2 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\nUpgraded from Airflow 3.0.7 to Airflow 3.1.0 and the negation has to be before the exclusion for my DAG to appear in the UI.\n\nBehaviour before:\n```\n*.py\n!dag_to_appear.py\n```\n\nBehaviour after:\n```\n!dag_to_appear.py\n*.py\n```\n\nIf I understand correctly in [the docs](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/dags.html#airflowignore) negations should come after the generic exclusion.\n\n### What you think should happen instead?\n\n_No response_\n\n### How to reproduce\n\nWrite the negation after exclusion pattern.\n\n### Operating System\n\nMacOS with Podman Desktop\n\n### Versions of Apache Airflow Providers\n\n_No response_\n\n### Deployment\n\nDocker-Compose\n\n### Deployment details\n\n_No response_\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56298\nTitle: Fix: Handle SQLAlchemy_URI for PrestoHook\nState: open\nAuthor: Pyasma\nLabels: area:providers, provider:presto\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\nFix: Implement Robust SQLAlchemy URI Construction for PrestoHook\r\n\r\nThis PR resolves a critical issue where the **`PrestoHook`** failed to reliably construct a valid SQLAlchemy connection URI. The dependency on the generic and flawed `DbApiHook.get_uri()` prevented the hook from correctly translating complex connection parameters required by Presto/Trino.\r\n\r\nThis change is implemented by **adding the `sqlalchemy_url` property and updating the `get_uri()` method** to rely on this new, correct logic. This guarantees compliant URL formatting by:\r\n\r\n1.  **Correct Path Construction:** Safely builds the mandatory path as `catalog/schema`.\r\n2.  **Parameter Translation:** Ensures essential configuration details (like `protocol` and `source`) are extracted from the `extra` field and mapped to URL query parameters.\r\n\r\n### Added/Modified Functions\r\n* `sqlalchemy_url` (New property): Constructs the correct SQLAlchemy `URL` object.\r\n* `get_uri()` (Modified method): Returns the string representation of the fixed URL.\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n\r\n\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).\r\n\r\n### Related Issues\r\nrelated: #38195 (Specifically Presto portion)\r\n\r\n### Checklist\r\n* [x] I have rebased my branch to the latest `main` branch.\r\n* [x] I have run `prek` and all checks passed.\r\n* [x] I have written tests that cover the added `sqlalchemy_url` and modified `get_uri` methods.\r\n* [x] I have checked my tests cover the fix (verified)",
  "Requirement ID: ISSUE-56297\nTitle: [v3-1-test] UI: Fix Grid for cleared runs when tasks were removed (#56085)\nState: closed\nAuthor: github-actions[bot]\nLabels: area:API\nBody:\nEnsure removed/historical tasks from selected runs are visible in\nGrid even if they no longer exist in the current DAG version.\n\nWe now:\n- Include synthetic leaf nodes for task_ids present in TIs but\nmissing from the serialized DAG in both grid/structure and grid/ti_summaries.\n- Aggregate TI states for these synthetic nodes\n\nAdd tests covering structure and TI summaries for removed tasks.\n(cherry picked from commit 77fae41380dc9f1a8fc8a449beea0e63056c291c)\n\nCo-authored-by: Ephraim Anierobi <splendidzigy24@gmail.com>",
  "Requirement ID: ISSUE-56296\nTitle: Revert \"Update all models to use `sqlalchemy` 2 annotations (#55954)\"\nState: closed\nAuthor: vincbeck\nLabels: area:serialization, full tests needed, area:Triggerer\nBody:\nThis reverts commit 3150430b5d976d32784860f3b6cdb5ff7605a7cc.\r\n\r\nThis PR makes the CI fail. The actual fix is currently being worked on in #56212, but in the meantime, reverting this commit will unblock the CI.\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56295\nTitle: Inconsistent PythonVirtualEnvOperator behaviour with **kwargs\nState: closed\nAuthor: martinbikandi\nLabels: kind:bug, area:core, area:core-operators, needs-triage\nBody:\n### Apache Airflow version\n\n2.11.0\n\n### If \"Other Airflow 2 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\nI define two functions to be run in the operator:\n\n```python\ndef run_kwarg(**kwargs):\n    print(kwargs)\n\ndef run_arg(context):\n    print(context)\n```\n\nThen **the task running the `run_kwarg` function fails**, while **the other works fine**. **When system_site_packages=False, both work fine**.\n\n```python\n# fails\ntask_run_kwarg = PythonVirtualenvOperator(\n    task_id=\"run_kwarg\",\n    python_callable=run_kwarg,\n    op_kwargs={'context': \"{{dag_run.conf}}\"},\n    system_site_packages=True\n)\n\n# works\ntask_run_arg = PythonVirtualenvOperator(\n    task_id=\"run_arg\",\n    python_callable=run_arg,\n    op_kwargs={'context': \"{{dag_run.conf}}\"},\n    system_site_packages=True\n)\n\n# works\nrun_kwarg_no_site_packages = PythonVirtualenvOperator(\n    task_id=\"run_kwarg_no-site-packages\",\n    python_callable=run_kwarg,\n    op_kwargs={'context': \"{{dag_run.conf}}\"},\n    system_site_packages=False,\n)\n\n# works\nrun_arg_no_site_packages = PythonVirtualenvOperator(\n    task_id=\"run_arg_no-site-packages\",\n    python_callable=run_arg,\n    op_kwargs={'context': \"{{dag_run.conf}}\"},\n    system_site_packages=False,\n)\n```\n\nAdditionally, when `outlets` is specified with a dataset, the task running `run_kwargs` fails, when system_site_packages is either true or false, but the task running `run_args` runs properly.\n\n```python\n# fails\nrun_kwarg_outlet = PythonVirtualenvOperator(\n    task_id=\"run_kwarg_outlet\",\n    python_callable=run_kwarg,\n    op_kwargs={'context': \"{{dag_run.conf}}\"},\n    system_site_packages=True,\n    outlets=[Dataset('dataset://run_kwarg_outlet')]\n)\n\n# works\nrun_arg_outlet = PythonVirtualenvOperator(\n    task_id=\"run_arg_outlet\",\n    python_callable=run_arg,\n    op_kwargs={'context': \"{{dag_run.conf}}\"},\n    system_site_packages=True,\n    outlets=[Dataset('dataset://run_arg_outlet')]\n)\n\n# fails\nrun_kwarg_no_site_packages_outlet = PythonVirtualenvOperator(\n    task_id=\"run_kwarg_no-site-packages_outlet\",\n    python_callable=run_kwarg,\n    op_kwargs={'context': \"{{dag_run.conf}}\"},\n    system_site_packages=False,\n    outlets=[Dataset('dataset://run_kwarg_no-site-packages_outlet')]\n)\n\n# works\nrun_arg_no_site_packages_outlet = PythonVirtualenvOperator(\n    task_id=\"run_arg_no-site-packages_outlet\",\n    python_callable=run_arg,\n    op_kwargs={'context': \"{{dag_run.conf}}\"},\n    system_site_packages=False,\n    outlets=[Dataset('dataset://run_arg_no-site-packages_outlet')]\n)\n```\n\nOn the webUI I see the following:\n\n<img width=\"260\" height=\"317\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/8e1abe48-50c6-45a1-bf86-69abe2fc8299\" />\n\n### What you think should happen instead?\n\n**All defined tasks should work properly**.\n\n### How to reproduce\n\nFresh python=3.11.8 environment (I used miniconda)\n`conda create -n test-environment python=3.11.8`\n\nInstall airflow and virtualenv in the environment:\n`pip install apache-airflow==2.11.0 virtualenv=20.34.0`\n\nRun airflow standalone, disable example dags and put the following dag into the dags folder:\n\n```python\nfrom airflow import models\nfrom airflow.operators.python import PythonVirtualenvOperator\n\nfrom airflow.datasets import Dataset\n\ndef run_kwarg(**kwargs):\n    print(kwargs)\n\ndef run_arg(context):\n    print(context)\n\nwith models.DAG(\"dag_test\",\n                schedule=None,\n                catchup=False) as dag:\n\n    # fails\n    task_run_kwarg = PythonVirtualenvOperator(\n        task_id=\"run_kwarg\",\n        python_callable=run_kwarg,\n        op_kwargs={'context': \"{{dag_run.conf}}\"},\n        system_site_packages=True,\n        # outlets=[dummy_dataset]\n    )\n\n    # works\n    task_run_arg = PythonVirtualenvOperator(\n        task_id=\"run_arg\",\n        python_callable=run_arg,\n        op_kwargs={'context': \"{{dag_run.conf}}\"},\n        system_site_packages=True,\n        # outlets=[dummy_dataset]\n    )\n\n    # works\n    run_kwarg_no_site_packages = PythonVirtualenvOperator(\n        task_id=\"run_kwarg_no-site-packages\",\n        python_callable=run_kwarg,\n        op_kwargs={'context': \"{{dag_run.conf}}\"},\n        system_site_packages=False,\n        # outlets=[dummy_dataset]\n    )\n\n    # works\n    run_arg_no_site_packages = PythonVirtualenvOperator(\n        task_id=\"run_arg_no-site-packages\",\n        python_callable=run_arg,\n        op_kwargs={'context': \"{{dag_run.conf}}\"},\n        system_site_packages=False,\n        # outlets=[dummy_dataset]\n    )\n\n    # fails\n    run_kwarg_outlet = PythonVirtualenvOperator(\n        task_id=\"run_kwarg_outlet\",\n        python_callable=run_kwarg,\n        op_kwargs={'context': \"{{dag_run.conf}}\"},\n        system_site_packages=True,\n        outlets=[Dataset('dataset://run_kwarg_outlet')]\n    )\n\n    # works\n    run_arg_outlet = PythonVirtualenvOperator(\n        task_id=\"run_arg_outlet\",\n        python_callable=run_arg,\n        op_kwargs={'context': \"{{dag_run.conf}}\"},\n        system_site_packages=True,\n        outlets=[Dataset('dataset://run_arg_outlet')]\n    )\n\n    # fails\n    run_kwarg_no_site_packages_outlet = PythonVirtualenvOperator(\n        task_id=\"run_kwarg_no-site-packages_outlet\",\n        python_callable=run_kwarg,\n        op_kwargs={'context': \"{{dag_run.conf}}\"},\n        system_site_packages=False,\n        outlets=[Dataset('dataset://run_kwarg_no-site-packages_outlet')]\n    )\n\n    # works\n    run_arg_no_site_packages_outlet = PythonVirtualenvOperator(\n        task_id=\"run_arg_no-site-packages_outlet\",\n        python_callable=run_arg,\n        op_kwargs={'context': \"{{dag_run.conf}}\"},\n        system_site_packages=False,\n        outlets=[Dataset('dataset://run_arg_no-site-packages_outlet')]\n    )\n```\n\n### Operating System\n\nWindows 10 / WSL2 (Ubuntu 24.04)\n\n### Versions of Apache Airflow Providers\n\n_No response_\n\n### Deployment\n\nVirtualenv installation\n\n### Deployment details\n\n```bash\nAIRFLOW_HOME=$(pwd)\nconda activate airflow-tests\nairflow standalone\n```\n\nI ran the dag with the webUI.\n\n### Anything else?\n\nThe error when `system_site_packages=True` is:\n\n`TypeError: cannot pickle 'module' object`\n\nThe error when `system_site_packages=False` and the outlet is specified is:\n\n `ModuleNotFoundError: No module named 'airflow'`\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56293\nTitle: Expand and collapse group component\nState: closed\nAuthor: pierrejeambrun\nLabels: backport-to-v3-1-test\nBody:\nExtract a reusable component from `expand/collapse` button groups which is used at multiple places.\r\n\r\n\r\nhttps://github.com/user-attachments/assets/092def89-4ac8-4da3-a923-7b4d95d531d2",
  "Requirement ID: ISSUE-56292\nTitle: Fix model type mismatch\nState: closed\nAuthor: vincbeck\nLabels: \nBody:\nMismatch between the annotation and the actual type.\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56291\nTitle: Cannot run SparkKubernetesOperator in deferrable mode\nState: open\nAuthor: Ferdinanddb\nLabels: kind:bug, area:providers, good first issue, provider:cncf-kubernetes, area:async-operators\nBody:\n### Apache Airflow Provider(s)\n\ncncf-kubernetes\n\n### Versions of Apache Airflow Providers\n\nSee deployment details, but cncf-kubernetes provider version is 10.8.1, for info.\n\n### Apache Airflow version\n\n3.1.0\n\n### Operating System\n\nOfficial Airflow image: docker.io/apache/airflow:3.1.0-python3.12\n\n### Deployment\n\nOfficial Apache Airflow Helm Chart\n\n### Deployment details\n\nI use helm with a custom image built via this Dockerfile:\n\n```\nFROM docker.io/apache/airflow:3.1.0-python3.12\n\nUSER root\n\n# Copy requirements to working directory\nCOPY requirements.txt /var/airflow/requirements.txt\n\n# Set the working directory in the container\nWORKDIR /var/airflow\n\n\nUSER airflow\n\nRUN pip install --upgrade pip\n\n# Install the necessary dependencies\nRUN pip install \\\n    --no-cache-dir \\\n    --constraint \"https://raw.githubusercontent.com/apache/airflow/constraints-3.1.0/constraints-3.12.txt\" \\\n    -r /var/airflow/requirements.txt\n```\n\nThe requirements.txt file is:\n\n```\napache-airflow[amazon,google,postgres,async,cncf.kubernetes,celery,slack,http,fab,standard,openlineage]==3.1.0\n```\n\n### What happened\n\nI am using the `SparkKubernetesOperator` as follow:\n\n```\nfull_reload_job = SparkKubernetesOperator(\n        task_id=\"full_reload_job\",\n        namespace=\"spark-operator\",\n        application_file=\"spark_app/full_reload_job/spark_application_config.yml\",\n        kubernetes_conn_id=\"kubernetes_default\",\n        random_name_suffix=True,\n        get_logs=True,\n        reattach_on_restart=True,\n        delete_on_termination=True,\n        do_xcom_push=False,\n        deferrable=True,\n        retries=0,\n        on_execute_callback=upload_spark_config_to_gcs,\n    )\n```\n\nThe job succeeds when `deferrable=False,` but fails when it is `True`.\n\nThe error I get is:\n\n```\nERROR - Task failed with exception\nAttributeError: 'SparkKubernetesOperator' object has no attribute 'launcher'\n\nFile \"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py\", line 920 in run\nFile \"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py\", line 1307 in _execute_task\nFile \"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/bases/operator.py\", line 1632 in resume_execution\nFile \"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/cncf/kubernetes/operators/pod.py\", line 956 in trigger_reentry\nFile \"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/cncf/kubernetes/operators/pod.py\", line 978 in _clean\nFile \"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/cncf/kubernetes/operators/pod.py\", line 1013 in post_complete_action\nFile \"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/cncf/kubernetes/operators/pod.py\", line 1056 in cleanup\nFile \"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/cncf/kubernetes/operators/spark_kubernetes.py\", line 265 in process_pod_deletion\n```\n\nAny idea why this is happening? It was working fine in Airflow 2.11.0 and before.\n\n### What you think should happen instead\n\nThe task should succeeds.\n\n### How to reproduce\n\nRun a SparkKubernetesOperator task in deferrable mode.\n\n\n\n### Anything else\n\nThis issue https://github.com/apache/airflow/issues/55747 describes the same behavior but for a Snowflake operator. I suspect that this issue is not tied to a specific provider but is more generic (linked to how task are deferred maybe).\n\nAnother remark that might be important: I am using keda to autoscale the triggerer pod (as well as the worker pods), and the triggerer and worker pods downscale to 0 when there is nothing to execute.\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56290\nTitle: Add a prototype Edge Worker client to the Go SDK\nState: closed\nAuthor: ashb\nLabels: area:go-sdk\nBody:\nAs the Edeg API server is currently implemented we need to \"pretend\" to be  a\nspecific Airflow and Edge provider version. These default to the currently\nreleased versions, but can be changed via env vars to work elsewhere.\n\nThis works enough to run tasks, but there might need to be some changes to the\nEdge API to support non-python clients (for example, working out the\nversioning strategy to make it long-term supportible and not need the Airflow\nand Edge Provider version to match 100%).\n\nA chunk of the changes here are to make the config and global setup \"more well\nstructured\" -- so that they are suitable to be easily called from multiple\nworkers (Celery and Go, useful if we don't end up keeping the Celery worker)\n\n<!--\n Licensed to the Apache Software Foundation (ASF) under one\n or more contributor license agreements.  See the NOTICE file\n distributed with this work for additional information\n regarding copyright ownership.  The ASF licenses this file\n to you under the Apache License, Version 2.0 (the\n \"License\"); you may not use this file except in compliance\n with the License.  You may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing,\n software distributed under the License is distributed on an\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n KIND, either express or implied.  See the License for the\n specific language governing permissions and limitations\n under the License.\n -->\n\n<!--\nThank you for contributing! Please make sure that your code changes\nare covered with tests. And in case of new features or big changes\nremember to adjust the documentation.\n\nFeel free to ping committers for the review!\n\nIn case of an existing issue, reference it using one of the following:\n\ncloses: #ISSUE\nrelated: #ISSUE\n\nHow to write a good git commit message:\nhttp://chris.beams.io/posts/git-commit/\n-->\n\n\n\n<!-- Please keep an empty line above the dashes. -->\n---\n**^ Add meaningful description above**\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56289\nTitle: API - Clearing specific Ti map index\nState: open\nAuthor: pierrejeambrun\nLabels: kind:bug, area:API, good first issue, type:bug-fix, area:dynamic-task-mapping\nBody:\n### Body\n\nUpdate the API endpoint for clearing a TI to take into account the `map_indexes`. Currently all indexes will be cleared even if the map_index was specified in the payload.\n\nInstead we should take into account the passed map_index so that we can only clear them.\n\nThis was possible in AF2.\n\n### Committer\n\n- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",
  "Requirement ID: ISSUE-56288\nTitle: Update pyproject.toml\nState: closed\nAuthor: tmdi123\nLabels: area:production-image, area:docker-tests\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56287\nTitle: Breeze is not picking up the connections defined in environment file\nState: closed\nAuthor: sc250072\nLabels: kind:bug, area:providers, area:core, needs-triage, provider:teradata\nBody:\n### Apache Airflow version\n\nmain (development)\n\n### If \"Other Airflow 2 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\nI have configured AIRFLOW_CONN_TERADATA_DEFAULT='teradata://login:password@host/schema' in `files/airflow-breeze-config/environment_variables.env` but when running system test using `breeze testing system-tests providers/teradata/tests/system/teradata/example_teradata.py`, breeze considering default connection values (defined as localhost as host in source code) and not considering connection details defined in environment file. But when I start airflow using `breeze start-airflow`, example_teradata.py dag is working successfully. \nI tried to export AIRFLOW_CONN_TERADATA_DEFAULT='teradata://login:password@host/schema' at bash level and also tried at bash shell but both are not working. \n\nI also tried to change connection name from teradata_default to teradata_normal  at `teradata/tests/system/teradata/example_teradata.py` and defined AIRFLOW_CONN_TERADATA_NORMAL='teradata://login:password@host/schema' in `files/airflow-breeze-config/environment_variables.env` and breeze testing system-tests failing with _teradata_normal connection is not defined._\n\n### What you think should happen instead?\n\n- `breeze testing system-tests `should consider connections defined in environment variables and system tests should get success with provided connection details.\n\n### How to reproduce\n\n- `git clone https://github.com/apache/airflow.git`\n- cd airflow\n- `uv tool install -e ./dev/breeze`\n- create directory under `files/airflow-breeze-config` and create `environment_variables.env`\n- Define connection to Teradata database as `AIRFLOW_CONN_TERADATA_DEFAULT='teradata://login:password@host/schema'`\n- Run `./scripts/ci/testing/run_system_tests.sh providers/teradata/tests/system/teradata/example_teradata.py`\n\nNew Teradata database instance can be set using following steps.\n\n- Create a new ClearScape Analytics\u2122 Experience account.\n\n> If you don't already have one, sign up at:\n> \n> [Teradata ClearScape Analytics\u2122 Experience](https://www.teradata.com/getting-started/demos/clearscape-analytics)\n> \n\n- Login\n\n> Sign in with your new account at:\n> \n> [ClearScape Analytics\u2122 Experience Login](https://clearscape.teradata.com/sign-in)\n\n- Create new Teradata instance\n\n> use option `Create new environment` to create new Teradata instance\n\n> login: demo_user\n> schema: demo_user\n\n\n\n\n### Operating System\n\nUbuntu 22.04.5 LTS\n\n### Versions of Apache Airflow Providers\n\napache-airflow-providers-teradata - 3.2.1\n\n### Deployment\n\nDocker-Compose\n\n### Deployment details\n\ndocker Version          27.5.1\nDocker Compose version   v2.32.4\nPython 3.10.12\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56286\nTitle: Fix Dataflow Java system test and link inside the operator\nState: closed\nAuthor: VladaZakharova\nLabels: provider:google, area:providers, provider:apache-beam\nBody:\nThis PR:\r\n\r\n - fixes link in Dataflow Java operators\r\n - fixes system test\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56285\nTitle: [v3-1-test] UI: Add Expand/Collapse all to XComs page (#56083)\nState: closed\nAuthor: github-actions[bot]\nLabels: area:UI\nBody:\n* UI: Add Expand/Collapse all to XComs page\n\n* Restored few changes\n\n* Fixing lint issue\n\n* Made suggested Changes\n(cherry picked from commit 9c96500c4014c4460c97a48a23b342a33ab5eee8)\n\nCo-authored-by: Kavya Katal <KAVYAKATAL09@GMAIL.COM>",
  "Requirement ID: ISSUE-56284\nTitle: Status of testing Providers that were prepared on October 01, 2025\nState: closed\nAuthor: eladkal\nLabels: area:providers, kind:meta, testing status\nBody:\n### Body\n\nI have a kind request for all the contributors to the latest provider distributions release.\nCould you please help us to test the RC versions of the providers?\n\nThe guidelines on how to test providers can be found in\n\n[Verify providers by contributors](https://github.com/apache/airflow/blob/main/dev/README_RELEASE_PROVIDERS.md#verify-the-release-candidate-by-contributors)\n\nLet us know in the comments, whether the issue is addressed.\n\nThese are providers that require testing as there were some substantial changes introduced:\n\n\n## Provider [amazon: 9.15.0rc1](https://pypi.org/project/apache-airflow-providers-amazon/9.15.0rc1)\n   - [x] [Add async support for Amazon SNS Notifier (#56133)](https://github.com/apache/airflow/pull/56133): @ferruzzi\n   - [x] [Add async support for Amazon SQS Notifier (#56159)](https://github.com/apache/airflow/pull/56159): @ferruzzi\n   - [x] [Add a new Amazon Simple Email Service Notifier (#56106)](https://github.com/apache/airflow/pull/56106): @ferruzzi\n   - [ ] [Implement `filter_authorized_connections`, `filter_authorized_pools` and `filter_authorized_variables` in AWS auth manager (#55687)](https://github.com/apache/airflow/pull/55687): @vincbeck\n     Linked issues:\n       - [Linked Issue #55298](https://github.com/apache/airflow/pull/55298): @vincbeck\n   - [x] [Don't import from test code in the Lambda executor (#56280)](https://github.com/apache/airflow/pull/56280): @o-nikolas\n   - [ ] [Fix: Only defer EmrCreateJobFlowOperator when wait_policy is set (#56077)](https://github.com/apache/airflow/pull/56077): @laksh-krishna-sharma\n     Linked issues:\n       - [Linked Issue #40966](https://github.com/apache/airflow/issues/40966): @ibzx\n   - [ ] [Reducing memory footprint for synchronous `S3KeySensor` (#55070)](https://github.com/apache/airflow/pull/55070): @jroachgolf84\n## Provider [fab: 3.0.0rc1](https://pypi.org/project/apache-airflow-providers-fab/3.0.0rc1)\n   - [x] [Upgrade FAB to FAB 5 (#50960)](https://github.com/apache/airflow/pull/50960): @potiuk\n   - [ ] [#55856: add if_not_exists=True to FAB migration (#56100)](https://github.com/apache/airflow/pull/56100): @plutaniano\n     Linked issues:\n       - [Linked Issue #55856](https://github.com/apache/airflow/issues/55856): @plutaniano\n\n<!--\n\nNOTE TO RELEASE MANAGER:\n\nYou can move here the providers that have doc-only changes or for which changes are trivial, and\nyou could assess that they are OK.\n\n-->\nAll users involved in the PRs:\n@laksh-krishna-sharma @jroachgolf84 @vincbeck @plutaniano @potiuk @ferruzzi @o-nikolas\n\n### Committer\n\n- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",
  "Requirement ID: ISSUE-56282\nTitle: Deprecate CreateAutoMLVideoTrainingJobOperator in google provider\nState: closed\nAuthor: Crowiant\nLabels: provider:google, area:providers, kind:documentation\nBody:\nAutoML Video model was deprecated https://cloud.google.com/vertex-ai/docs/deprecations on 31th of July 2025 but still available for old model training. CreateAutoMLVideoTrainingJobOperator was based on working with AutoML Video. and will be removed after 6 months according to the depreciation policy. Removed system tests for video tracking and video training. Update generative_model_tuning system test. Update documentation for vertex ai.\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56280\nTitle: Don't import from test code in the Lambda executor\nState: closed\nAuthor: o-nikolas\nLabels: provider:amazon, area:providers\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56278\nTitle: Fix datetime not available for Sqlalchemy 2.0 type introspection\nState: closed\nAuthor: potiuk\nLabels: all versions\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56277\nTitle: Enable PT011 rule to prvoider tests\nState: closed\nAuthor: xchwan\nLabels: area:providers, area:logging, provider:elasticsearch, provider:ftp, provider:exasol, provider:edge\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\nissue: Enable Even More PyDocStyle Checks #40567\r\n@ferruzzi\r\nThis PR is for enable PT011 rule:\r\nPT011: all instances of pytest.raises should include a match term to make sure they are catching the right error.\r\nhttps://docs.astral.sh/ruff/rules/pytest-raises-too-broad/\r\nThere are 102 files changes is need.\r\nSo, I separate to many PR, which contains about 5 file changes for easy review.\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56276\nTitle: Firehose and kinesis hook in independent file\nState: closed\nAuthor: xchwan\nLabels: provider:amazon, area:providers\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\nTo handle \"AIP-82: Add common messaging system providers #52712\" kinesis part, I found there are firehose class which should exist in firehose.py. As issue \"Amazon kinesis hook contains firehose class #55955\" discussed, I seperate firehose hook and build kinesis hook\r\n\r\n\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56275\nTitle: PythonVirtualenvOperator Warning Failed to Delete venv\nState: closed\nAuthor: RaphCodec\nLabels: kind:bug, area:core, area:core-operators, needs-triage\nBody:\n### Apache Airflow version\n\n3.1.0\n\n### If \"Other Airflow 2 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\nI am running Airflow 3.1.0 with the LocalExecutor, using Docker Compose.  I have a simple DAG shown below just to test out the PythonVirtualenvOperator.  When I run the DAG Airflow automatically uses UV to create the venv in a tmp directory.  The DAG then runs sucessfully, but at the end there is a warning that the venv failed to be deleted becuase Airflow for some reason is now looking at a different directory for the venv.\n\nOriginally the venv was create in /tmp but the airflow tried to delete the venv from /opt/airflow/tmp.  Therefore I tried setting the UV_Cache_DIR to /opt/airflow/tmp but now Airflow tries to delete the venv from /opt/airflow/opt/airflow/tmp. \n\nLog with venv delete failure:\n```\nINFO - DAG bundles loaded: dags-folder\nINFO - Filling up the DagBag from /opt/airflow/dags/duckdb_example.py\nINFO - Executing cmd: uv venv --allow-existing --seed --python python /opt/airflow/tmp/venvizdpi6sh\nINFO - Output:\nINFO - Using CPython 3.12.11 interpreter at: /usr/python/bin/python\nINFO - Creating virtual environment with seed packages at: tmp/venvizdpi6sh\nINFO -  + pip==25.2\nINFO - Executing cmd: uv pip install --python /opt/airflow/tmp/venvizdpi6sh/bin/python -r /opt/airflow/tmp/venvizdpi6sh/requirements.txt\nINFO - Output:\nINFO - Resolved 7 packages in 167ms\nINFO - Installed 7 packages in 3.50s\nINFO -  + duckdb==1.1.1\nINFO -  + numpy==2.3.0\nINFO -  + pandas==2.3.2\nINFO -  + python-dateutil==2.9.0.post0\nINFO -  + pytz==2025.2\nINFO -  + six==1.17.0\nINFO -  + tzdata==2025.2\nINFO - Executing cmd: /opt/airflow/tmp/venvizdpi6sh/bin/python /opt/airflow/tmp/venv-callm_tfnrbj/script.py /opt/airflow/tmp/venv-callm_tfnrbj/script.in /opt/airflow/tmp/venv-callm_tfnrbj/script.out /opt/airflow/tmp/venv-callm_tfnrbj/string_args.txt /opt/airflow/tmp/venv-callm_tfnrbj/termination.log /opt/airflow/tmp/venv-callm_tfnrbj/airflow_context.json\nINFO - Output:\nINFO - Query result:\nINFO -     answer\nINFO - 0      42\nWARNING - Fail to delete /opt/airflow/opt/airflow/tmp/venvizdpi6sh. The directory does not exist.\nINFO - Done. Returned value was: None\n```\n\n\n\n### What you think should happen instead?\n\nAirflow should look for the venv in the same directory where it is created instead of always prefixing the dir with /opt/airflow/ when it's time to delete the venv.\n\n```\nINFO - Deleted venv /opt/airflow/tmp/venvizdpi6sh.\n```\n\n\n### How to reproduce\n\nBelow is the example DAG that I am running, Along with a snippet of the Docker Compose File that I have.  The docker compose file is otherwise unchanged except for the fact that I removed flower and switched celery to LocalExecutor.\n\n\nExample DAG\n```python\nfrom airflow.sdk import DAG, task\nimport pendulum\n\nwith DAG(\n    dag_id=\"duckdb_example\",\n    description=\"A simple dag to show the use of a python virtual env with duckdb\",\n    start_date=pendulum.datetime(2025, 8, 1, tz=\"EST\"),\n    schedule=\"@daily\",\n    catchup=False,\n):\n    @task.virtualenv(\n        task_id=\"virtualenv_duckdb\", \n        requirements=[\"duckdb==1.1.1\", \"numpy==2.3.0\", \"pandas==2.3.2\"], \n        system_site_packages=False,\n    )\n    def run_query():\n        import duckdb\n        import time\n        result = duckdb.query(\"SELECT 42 AS answer\").to_df()\n        print(\"Query result:\\n\", result)\n\n    run_query()\n```\n\n\n\n### Operating System\n\nLinux - Ubuntu\n\n### Versions of Apache Airflow Providers\n\n_No response_\n\n### Deployment\n\nDocker-Compose\n\n### Deployment details\n\nI'm using Docker Compose with the below DockerFile.\n\n```\nFROM apache/airflow:3.1.0\nADD requirements.txt .\nRUN pip install apache-airflow==${AIRFLOW_VERSION} -r requirements.txt\n```\n\nThe docker compose file is based off the offical Airflow file. I switched celery for LocalExecutor since I'm just testing Airflow locally.  The below section is what I mainly changed in the docker compose file. The only other things I chaged were removing parts of the file that celery would use such as Flower\n\n```yaml\nx-airflow-common:\n  &airflow-common\n  # In order to add custom dependencies or upgrade provider distributions you can use your extended image.\n  # Comment the image line, place your Dockerfile in the directory where you placed the docker-compose.yaml\n  # and uncomment the \"build\" line below, Then run `docker-compose build` to build the images.\n  # image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:3.1.0}\n  build: .\n  environment:\n    &airflow-common-env\n    AIRFLOW__CORE__EXECUTOR: LocalExecutor\n    AIRFLOW__CORE__AUTH_MANAGER: airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager\n    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow\n    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow\n    AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0\n    AIRFLOW__CORE__FERNET_KEY: ''\n    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'\n    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'\n    AIRFLOW__CORE__EXECUTION_API_SERVER_URL: 'http://airflow-apiserver:8080/execution/'\n    AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'\n    # WARNING: Use _PIP_ADDITIONAL_REQUIREMENTS option ONLY for a quick checks\n    # for other purpose (development, test and especially production usage) build/extend Airflow image.\n    _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-}\n    AIRFLOW_CONFIG: '/opt/airflow/config/airflow.cfg'\n    # Set temp directory to use mounted volume\n    TMPDIR: '/opt/airflow/tmp'\n    UV_CACHE_DIR: '/opt/airflow/tmp'\n    UV_LINK_MODE: 'copy'\n  volumes:\n    - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags\n    - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs\n    - ${AIRFLOW_PROJ_DIR:-.}/config:/opt/airflow/config\n    - ${AIRFLOW_PROJ_DIR:-.}/plugins:/opt/airflow/plugins\n  user: \"${AIRFLOW_UID:-50000}:0\"\n  depends_on:\n    &airflow-common-depends-on\n    redis:\n      condition: service_healthy\n    postgres:\n      condition: service_healthy\n\n```\n\n### Anything else?\n\nThis issue occurs with every PythonVirtualenvOperator DAG I've tried recently \n\n### Are you willing to submit PR?\n\n- [x] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56274\nTitle: I18n: Add calendar translation keys and remove fallback\nState: closed\nAuthor: RoyLee1224\nLabels: area:UI, area:translations, translation:default, translation:zh-TW\nBody:\n## Why \r\nNow the translation freeze is over, these keys can be merged.\r\n## Changes\r\n\r\n- **Remove Translation Fallbacks in CalendarTooltip**\r\n- **Add Missing zh-TW Translation Keys**: `noFailedRuns` and `mixed`\r\n- **Refine zh-TW Translations**: Use \"\u7121\" (no) instead of \"\u6c92\u6709\" (no/didn't). I think this will prevent potential misunderstandings. \r\n \r\nFor example:\r\n\r\n\"\u6c92\u6709\u57f7\u884c\" (No/Didn't have runs) -> \"\u7121\u57f7\u884c\" (No runs)\r\n\r\n\"\u6c92\u6709\u7b26\u5408\u689d\u4ef6\u7684\u57f7\u884c\" -> \"\u7121\u7b26\u5408\u689d\u4ef6\u7684\u57f7\u884c\"\r\n\r\n\r\n## Screenshots\r\n<img width=\"1112\" height=\"469\" alt=\"en\" src=\"https://github.com/user-attachments/assets/3e38ef60-0b62-4444-a66c-7005e09c3dc4\" />\r\n<img width=\"1112\" height=\"469\" alt=\"tw\" src=\"https://github.com/user-attachments/assets/0e74bcab-d7a7-4100-b913-afd1ebf32117\" />\r\n\r\n\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56273\nTitle: Add timeout context to DagRun failure state listener hook\nState: open\nAuthor: 1fanwang\nLabels: kind:feature, needs-triage, area:Listeners\nBody:\n### Description\n\nCurrently, when a DAG run times out and is marked as failed, the notify_dagrun_state_changed() call doesn't include any context about the timeout reason. This makes it difficult for listeners and observability systems to distinguish between timeout failures and other types of DAG failures.\n\n### Use case/motivation\n\nProblem: DAG timeout failures are indistinguishable from other failure types in listener, limiting observability and error classification capabilities.\nSolution: Pass a \"timed_out\" message to notify_dagrun_state_changed() when a DAG run fails due to timeout, providing context to listeners about the specific failure reason.\nBenefits:\nEnables better error categorization in observability systems\nAllows listeners to handle timeout failures differently from other failure types\nImproves debugging and monitoring capabilities\nMaintains backward compatibility (msg parameter is optional)\n\n### Related issues\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [x] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56272\nTitle: Propogate dagrun timeout failure context\nState: open\nAuthor: 1fanwang\nLabels: area:Scheduler\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\ncloses: https://github.com/apache/airflow/issues/56273\r\n\r\n## Description\r\nPropogate dagrun timeout failure context\r\nAdd timeout context to DagRun failure state listener hook\r\nPass \"timed_out\" message to notify_dagrun_state_changed() when a DAG run fails due to exceeding its dagrun_timeout. This provides context to listeners about the specific failure reason, enabling better error classification and observability.\r\n\r\n## Testing and Impact\r\nListeners: Can now distinguish timeout failures from other failure types via the msg parameter\r\n\r\nCode path walkthrough:\r\n1. Propagate from notify_dagrun_state_changed call when dagrun_timeout trigger\r\n\r\nhttps://github.com/apache/airflow/blob/8966d288271952464b0f375beb940aafc4c59964/airflow-core/src/airflow/jobs/scheduler_job_runner.py#L1888-L1925\r\n\r\n2. Propogate to DagRun listener hook call\r\n\r\nhttps://github.com/apache/airflow/blob/8966d288271952464b0f375beb940aafc4c59964/airflow-core/src/airflow/models/dagrun.py#L1363-L1369\r\n\r\n3. Example on how listener plugins can further receive process this info\r\n```\r\nfrom airflow.plugins_manager import AirflowPlugin\r\nfrom airflow.listeners import hookimpl\r\nfrom airflow.models import DagRun\r\nimport logging\r\n\r\nlogger = logging.getLogger(__name__)\r\n\r\n\r\nclass ExampleDagRunListener:\r\n    @hookimpl\r\n    def on_dag_run_failed(self, dag_run: DagRun, msg: str) -> None:\r\n        \"\"\"\r\n        Example listener hook that is triggered when a DAG run transitions\r\n        into a FAILED state.\r\n\r\n        :param dag_run: The DagRun instance that failed.\r\n        :param msg: An optional message with failure context, e.g. \"timed_out\".\r\n        \"\"\"\r\n        event = {\r\n            \"type\": \"dag_run_failed\",\r\n            \"dag_id\": dag_run.dag_id,\r\n            \"run_id\": dag_run.run_id,\r\n            \"reason\": msg or \"unspecified\",\r\n        }\r\n\r\n        # Example: emit the event to an external system\r\n        self._send_event(event)\r\n\r\n    def _send_event(self, event: dict) -> None:\r\n        \"\"\"\r\n        Stub for emitting events externally.\r\n        Replace this with custom logic (e.g. send to metrics, webhook, or queue).\r\n        \"\"\"\r\n        logger.info(\"Emitting DAG run failure event: %s\", event)\r\n\r\n\r\nclass ExampleListenerPlugin(AirflowPlugin):\r\n    name = \"example_dagrun_listener\"\r\n    listeners = [ExampleDagRunListener()]\r\n```\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56271\nTitle: KubernetesExecutor feature may be broken in 3.1.0\nState: open\nAuthor: XD-DENG\nLabels: kind:bug, area:core, area:task-sdk, affected_version:3.1\nBody:\n### Apache Airflow version\n\n3.1.0\n\n### If \"Other Airflow 2 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\nI upgraded from 3.0.3 to 3.1.0, then my jobs started to fail.\n\nIn my configuration, I'm using value `CeleryExecutor,KubernetesExecutor` for `AIRFLOW__CORE__EXECUTOR`. Previously in `3.0.3` this has been working fine: all TIs will be executed via `CeleryExecutor`, except those operators where I specified `executor='KubernetesExecutor'`.\n\nBut in 3.1.0, those operators where I specified `executor='KubernetesExecutor'` always fail, with the error below:\n```\n[2025-09-30 15:13:00] INFO - Filling up the DagBag from /tmp/s3_dag_bundle_ri5yvvpv/dags/tenant_test/xd_test.py source=airflow.models.dagbag.DagBag loc=dagbag.py:593\n[2025-09-30 15:13:00] ERROR - Failed to bag_dag: /tmp/s3_dag_bundle_ri5yvvpv/dags/tenant_test/xd_test.py source=airflow.models.dagbag.DagBag loc=dagbag.py:518\nUnknownExecutorException: Task 'xd_asked_for_another_task' specifies executor 'KubernetesExecutor', which is not available. Make sure it is listed in your [core] executors configuration, or update the task's executor to use one of the configured executors.\nFile \"/usr/local/lib/python3.12/site-packages/airflow/models/dagbag.py\", line 513 in _process_modules\nFile \"/usr/local/lib/python3.12/site-packages/airflow/models/dagbag.py\", line 156 in _validate_executor_fields\nUnknownExecutorException: Unknown executor being loaded: KubernetesExecutor\nFile \"/usr/local/lib/python3.12/site-packages/airflow/models/dagbag.py\", line 154 in _validate_executor_fields\nFile \"/usr/local/lib/python3.12/site-packages/airflow/executors/executor_loader.py\", line 245 in lookup_executor_name_by_str\n[2025-09-30 15:13:00] ERROR - Dag not found during start up dag_id=xd_test bundle=BundleInfo(name='s3-dags', version=None) path=dags/tenant_test/xd_test.py source=task loc=task_runner.py:633\n[2025-09-30 15:13:05] WARNING - Process exited abnormally exit_code=1 source=task\n```\n\nSeems it's not recognizing the 2nd executors specified under `AIRFLOW__CORE__EXECUTOR`.\n\nThis is breaking the [multi executor feature](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/executor/index.html#using-multiple-executors-concurrently)\n\n-----------------------------------\n**UPDATE:**\n\n**Actually seems it's not the multi executor feature being broken, instead, it's the KubernetesExecutor feature.**\n\n**In the Pod Template file, if I put value `LocalExecutor,KubernetesExecutor` for `AIRFLOW__CORE__EXECUTOR` in the Pod Template, it can work. But it should only require `LocalExecutor`, according to the doc https://airflow.apache.org/docs/apache-airflow-providers-cncf-kubernetes/stable/kubernetes_executor.html#example-pod-templates**\n\n**So something seems wrong and broken. This will impact a lot KubernetesExecutor users.**\n\n### What you think should happen instead?\n\n_No response_\n\n### How to reproduce\n\nThe DAG I used\n\n```\nfrom __future__ import annotations\n\n# [START tutorial]\n# [START import_module]\nimport textwrap\nfrom datetime import datetime, timedelta\n\n# Operators; we need this to operate!\nfrom airflow.providers.standard.operators.bash import BashOperator\n\n# The DAG object; we'll need this to instantiate a DAG\nfrom airflow.sdk import DAG\n\n# [END import_module]\n\n\n# [START instantiate_dag]\nwith DAG(\n    \"xd_test\",\n    # [START default_args]\n    # These args will get passed on to each operator\n    # You can override them on a per-task basis during operator initialization\n    default_args={\n        \"depends_on_past\": False,\n        \"retries\": 1,\n        \"retry_delay\": timedelta(minutes=5),\n        # 'queue': 'bash_queue',\n        # 'pool': 'backfill',\n        # 'priority_weight': 10,\n        # 'end_date': datetime(2016, 1, 1),\n        # 'wait_for_downstream': False,\n        # 'execution_timeout': timedelta(seconds=300),\n        # 'on_failure_callback': some_function, # or list of functions\n        # 'on_success_callback': some_other_function, # or list of functions\n        # 'on_retry_callback': another_function, # or list of functions\n        # 'sla_miss_callback': yet_another_function, # or list of functions\n        # 'on_skipped_callback': another_function, #or list of functions\n        # 'trigger_rule': 'all_success'\n    },\n    # [END default_args]\n    description=\"A simple tutorial DAG\",\n    schedule=timedelta(days=1),\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [END instantiate_dag]\n\n    # t1, t2 and t3 are examples of tasks created by instantiating operators\n    # [START basic_task]\n    t1 = BashOperator(\n        task_id=\"print_date\",\n        bash_command=\"date\",\n    )\n\n    t2 = BashOperator(\n        task_id=\"sleep\",\n        depends_on_past=False,\n        bash_command=\"sleep 5\",\n        retries=3,\n    )\n    # [END basic_task]\n\n    # [START documentation]\n    t1.doc_md = textwrap.dedent(\n        \"\"\"\\\n    #### Task Documentation\n    You can document your task using the attributes `doc_md` (markdown),\n    `doc` (plain text), `doc_rst`, `doc_json`, `doc_yaml` which gets\n    rendered in the UI's Task Instance Details page.\n    ![img](https://imgs.xkcd.com/comics/fixing_problems.png)\n    **Image Credit:** Randall Munroe, [XKCD](https://xkcd.com/license.html)\n    \"\"\"\n    )\n\n    dag.doc_md = __doc__  # providing that you have a docstring at the beginning of the DAG; OR\n    dag.doc_md = \"\"\"\n    This is a documentation placed anywhere\n    \"\"\"  # otherwise, type it like this\n    # [END documentation]\n\n    # [START jinja_template]\n    templated_command = textwrap.dedent(\n        \"\"\"\n    {% for i in range(5) %}\n        echo \"{{ ds }}\"\n        echo \"{{ macros.ds_add(ds, 7)}}\"\n    {% endfor %}\n    \"\"\"\n    )\n\n    t3 = BashOperator(\n        task_id=\"templated\",\n        depends_on_past=False,\n        bash_command=templated_command,\n    )\n    # [END jinja_template]\n\n    t4 = BashOperator(\n        task_id=\"xd_asked_for_another_task\",\n        depends_on_past=False,\n        bash_command=\"echo Hi_dude~\",\n        executor='KubernetesExecutor'\n    )\n\n    t1 >> [t2, t3]\n# [END tutorial]\n\n\n\n```\n\n### Operating System\n\nNAME=\"Red Hat Enterprise Linux\" VERSION=\"9.6 (Plow)\" ID=\"rhel\" ID_LIKE=\"fedora\" VERSION_ID=\"9.6\" PLATFORM_ID=\"platform:el9\" PRETTY_NAME=\"Red Hat Enterprise Linux 9.6 (Plow)\"\n\n### Versions of Apache Airflow Providers\n\n_No response_\n\n### Deployment\n\nOfficial Apache Airflow Helm Chart\n\n### Deployment details\n\n_No response_\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56270\nTitle: Fix ObjectStoragePath compatible with universal-pathlib 0.3.0 to avoid maximum recursion depth exceeded\nState: closed\nAuthor: gopidesupavan\nLabels: area:task-sdk\nBody:\nuniversal-pathlib version 0.3.0 introduced several changes. One notable update is that the path property now uses str(self) internally https://github.com/fsspec/universal_pathlib/blob/v0.3.0/upath/core.py#L218-L234. This leads to a recursion error in our case because ObjectStoragePath implements a custom __str__ method, and accessing self.path triggers an infinite loop. To resolve this, we should switch to using _raw_urlpaths instead.\r\n\r\nhttps://pypi.org/project/universal-pathlib/\r\n\r\nhttps://github.com/apache/airflow/actions/runs/18141256490/job/51633607047\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56269\nTitle: update Serialization document to reflect the latest change in codebase\nState: open\nAuthor: sjyangkevin\nLabels: kind:documentation\nBody:\nRelated to: https://github.com/apache/airflow/issues/56215\r\n\r\n### Why\r\nThe documentation for [Serialization](https://airflow.apache.org/docs/apache-airflow/stable/authoring-and-scheduling/serializers.html#serialization) is out-dated after the changes introduced from the following PRs:\r\n1. https://github.com/apache/airflow/pull/52360\r\n2. https://github.com/apache/airflow/pull/51059\r\n\r\nThis PR updates the document to reflect those changes in the **Registered** section, as well as fixing a small issue in the **Airflow Object** section (In the `deserialize` method, the class should be initialized with both `a` and `v`).\r\n\r\nThe document update might need to be backport into 3.1 or 3.0.\r\n\r\n### Document Build Output\r\n\r\n**Airflow Object** section\r\n<img width=\"1003\" height=\"585\" alt=\"Screenshot from 2025-09-30 16-51-00\" src=\"https://github.com/user-attachments/assets/a2ff4e03-536d-4dd3-8512-87863e4f7776\" />\r\n\r\n**Registered** section\r\n<img width=\"948\" height=\"612\" alt=\"Screenshot from 2025-09-30 16-50-43\" src=\"https://github.com/user-attachments/assets/5c0b795d-87c4-4832-8e1c-f45a55ca8b88\" />\r\n\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56268\nTitle: Check if any tasks have HITL operator before fetching HITL details\nState: closed\nAuthor: bbovenzi\nLabels: area:UI\nBody:\nWe were fetching HITL details too often.\r\nI still feel like this is all too complicated and brittle. So I would love to explore ways to actually know more about a Dag/Run/Task so we only display necessary tabs. Or redesign the view to not need so many tabs.\r\n\r\n\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56267\nTitle: pyproject docstring tweaks\nState: closed\nAuthor: ferruzzi\nLabels: \nBody:\nJust a quick update on a couple of the comments.\r\n\r\n- `102` and `103` are being left as they are per discussion back in July ([here](https://lists.apache.org/thread/lcvocwxnrrzq9ofrg4b4hj0yvpzbnygf)) and I thought this comment was already updated.  \r\n- `203` I just added context on why we're not going to enable it, it conflicts with an existing rule.  `203` requires a black line before a class docstring, `D211` states no blank line; both can not be true and `211` is already implemented.\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56266\nTitle: Only import OL plugin listeners/hooks if OL is enabled\nState: closed\nAuthor: jedcunningham\nLabels: area:providers, provider:openlineage\nBody:\nThese imports ultimately aren't cheap - it results in ~80mb of memory use, so lets skip it unless the plugin is enabled.",
  "Requirement ID: ISSUE-56265\nTitle: Add google provider dataflow pipeline streaming system test\nState: open\nAuthor: olegkachur-e\nLabels: provider:google, area:providers, kind:documentation\nBody:\nAdd test and update the docs.\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56264\nTitle: Fix SQLA Mapped datetime imports in models\nState: closed\nAuthor: amoghrajesh\nLabels: \nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\nAlong similar vein as https://github.com/apache/airflow/pull/56253\r\n\r\nLatest issue: \r\n```\r\n  NameError: Could not de-stringify annotation 'datetime | None'\r\n  \r\n  The above exception was the direct cause of the following exception:\r\n  \r\n  Traceback (most recent call last):\r\n    File \"/usr/python/bin/airflow\", line 4, in <module>\r\n      from airflow.__main__ import main\r\n    File \"/opt/airflow/airflow-core/src/airflow/__main__.py\", line 37, in <module>\r\n      from airflow.cli import cli_parser\r\n    File \"/opt/airflow/airflow-core/src/airflow/cli/cli_parser.py\", line 40, in <module>\r\n      from airflow.cli.cli_config import (\r\n      ...<5 lines>...\r\n      )\r\n    File \"/opt/airflow/airflow-core/src/airflow/cli/cli_config.py\", line 36, in <module>\r\n      from airflow.utils.cli import ColorMode\r\n    File \"/opt/airflow/airflow-core/src/airflow/utils/cli.py\", line 39, in <module>\r\n      from airflow.dag_processing.dagbag import DagBag\r\n    File \"/opt/airflow/airflow-core/src/airflow/dag_processing/dagbag.py\", line 52, in <module>\r\n      from airflow.serialization.serialized_objects import LazyDeserializedDAG\r\n    File \"/opt/airflow/airflow-core/src/airflow/serialization/serialized_objects.py\", line 69, in <module>\r\n      from airflow.models.dag import DagModel\r\n    File \"/opt/airflow/airflow-core/src/airflow/models/dag.py\", line 55, in <module>\r\n      from airflow.models.dagrun import DagRun\r\n    File \"/opt/airflow/airflow-core/src/airflow/models/dagrun.py\", line 64, in <module>\r\n      from airflow.models.backfill import Backfill\r\n    File \"/opt/airflow/airflow-core/src/airflow/models/backfill.py\", line 114, in <module>\r\n      class Backfill(Base):\r\n      ...<39 lines>...\r\n              return f\"Backfill({self.dag_id=}, {self.from_date=}, {self.to_date=})\"\r\n    File \"/usr/python/lib/python3.13/site-packages/sqlalchemy/orm/decl_api.py\", line 198, in __init__\r\n      _as_declarative(reg, cls, dict_)\r\n      ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\r\n    File \"/usr/python/lib/python3.13/site-packages/sqlalchemy/orm/decl_base.py\", line 245, in _as_declarative\r\n      return _MapperConfig.setup_mapping(registry, cls, dict_, None, {})\r\n             ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n    File \"/usr/python/lib/python3.13/site-packages/sqlalchemy/orm/decl_base.py\", line 326, in setup_mapping\r\n      return _ClassScanMapperConfig(\r\n          registry, cls_, dict_, table, mapper_kw\r\n      )\r\n    File \"/usr/python/lib/python3.13/site-packages/sqlalchemy/orm/decl_base.py\", line 573, in __init__\r\n      self._extract_mappable_attributes()\r\n      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^\r\n    File \"/usr/python/lib/python3.13/site-packages/sqlalchemy/orm/decl_base.py\", line 1579, in _extract_mappable_attributes\r\n      raise orm_exc.MappedAnnotationError(\r\n      ...<4 lines>...\r\n      ) from ne\r\n  sqlalchemy.orm.exc.MappedAnnotationError: Could not resolve all types within mapped annotation: \"Mapped[datetime | None]\".  Ensure all types are written correctly and are imported within the module in use.\r\n  \r\n  Error: check_environment returned 1. Exiting.\r\n  \r\n```\r\n\r\n\r\nExample: https://github.com/apache/airflow/actions/runs/18135253335/job/51613587395\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56259\nTitle: [v3-1-test] Support Dynamic UI Alerts (#54677)\nState: closed\nAuthor: github-actions[bot]\nLabels: area:API, kind:documentation\nBody:\n* feat: Dynamic UI Alerts\n\n* docs: updated ui customization docs to include dynamic alerts\n\n* docs: updated dynamic alerts documentation\n\n* fix: corrected whitespace in customize-ui.rst\n\n* fix: corrected whitespace in customize-ui.rst\n\n---------\n(cherry picked from commit 12a9d7b5c5d301849c513dd4fa458fef7ed2a369)\n\nCo-authored-by: codecae <codecae@users.noreply.github.com>\nCo-authored-by: Curtis Bangert <bangert.curtis+git@gmail.com>",
  "Requirement ID: ISSUE-56258\nTitle: [v3-1-test] Replace defaultValue with value in TaskTrySelect (#56141)\nState: closed\nAuthor: github-actions[bot]\nLabels: area:UI\nBody:\n* Replace defaultValue with value in TaskTrySelect\n\nChanged the Select component to use the controlled 'value' prop instead of 'defaultValue' for try number selection, ensuring the selected value updates correctly with state changes.\n\n* fix: Reorder props in TaskTrySelect for linting compliance\n\nApplied prop ordering fix as requested in code review.\nProps are now alphabetically ordered in the Select.Root component.\n(cherry picked from commit 3f7991f30e8c3f2dd85755ee02267b9ec882bcbe)\n\nCo-authored-by: Vedant Mamgain <mamgainvedant@gmail.com>",
  "Requirement ID: ISSUE-56257\nTitle: [WEBSERVER] SerializedDagModel query bug fix impacting Grid view performance\nState: closed\nAuthor: hussein-awala\nLabels: area:API\nBody:\nThe performance was already improved by https://github.com/apache/airflow/pull/55942, but the issue in the original query comes from the `SerializedDagModel.id != latest_serdag.id` filter being misplaced inside the `IN` clause.\r\n```\r\nSELECT serialized_dag.id, serialized_dag.dag_id\r\nFROM serialized_dag\r\nWHERE serialized_dag.dag_version_id IN (\r\n  SELECT task_instance.dag_version_id\r\n    FROM task_instance JOIN dag_run ON dag_run.dag_id = task_instance.dag_id AND dag_run.run_id = task_instance.run_id\r\n    WHERE dag_run.id IN (10205, 10203, 10201, 10200, 10198, 9455, 9430, 9415, 9413, 9407)\r\n    AND serialized_dag.id != '019986ae-e948-79cc-b9cc-d49cde67a0e1'::uuid);\r\n                  id                  |       dag_id\r\n--------------------------------------+---------------------\r\n 01998540-a9c1-7d90-8b59-7fbe38294778 | some_dag\r\n\r\n\r\n                                                                     QUERY PLAN\r\n-----------------------------------------------------------------------------------------------------------------------------------------------------\r\n Seq Scan on serialized_dag  (cost=0.00..923463.34 rows=21476 width=1641) (actual time=9509.231..10509.646 rows=1 loops=1)\r\n   Filter: (SubPlan 1)\r\n   Rows Removed by Filter: 42876\r\n   Buffers: shared hit=8200223\r\n   SubPlan 1\r\n     ->  Result  (cost=0.70..41.65 rows=41 width=16) (actual time=0.007..0.225 rows=236 loops=42877)\r\n           One-Time Filter: (serialized_dag.id <> '019986ae-e948-79cc-b9cc-d49cde67a0e1'::uuid)\r\n           Buffers: shared hit=8189132\r\n           ->  Nested Loop  (cost=0.70..41.65 rows=41 width=16) (actual time=0.007..0.200 rows=236 loops=42876)\r\n                 Buffers: shared hit=8189132\r\n                 ->  Index Scan using dag_run_pkey on dag_run  (cost=0.29..15.20 rows=10 width=61) (actual time=0.001..0.013 rows=10 loops=42876)\r\n                       Index Cond: (id = ANY ('{10205,10203,10201,10200,10198,9455,9430,9415,9413,9407}'::integer[]))\r\n                       Buffers: shared hit=1286253\r\n                 ->  Index Scan using ti_dag_run on task_instance  (cost=0.41..2.63 rows=1 width=71) (actual time=0.005..0.011 rows=24 loops=428751)\r\n                       Index Cond: (((dag_id)::text = (dag_run.dag_id)::text) AND ((run_id)::text = (dag_run.run_id)::text))\r\n                       Buffers: shared hit=6902879\r\n Planning Time: 2.056 ms\r\n Execution Time: 10509.694 ms\r\n```\r\nAnd after the fix (without the mentioned PR):\r\n```\r\nSELECT serialized_dag.id, serialized_dag.dag_id\r\nFROM serialized_dag\r\nWHERE serialized_dag.dag_version_id IN (\r\n  SELECT task_instance.dag_version_id\r\n    FROM task_instance JOIN dag_run ON dag_run.dag_id = task_instance.dag_id AND dag_run.run_id = task_instance.run_id\r\n    WHERE dag_run.id IN (10205, 10203, 10201, 10200, 10198, 9455, 9430, 9415, 9413, 9407)\r\n) AND serialized_dag.id != '019986ae-e948-79cc-b9cc-d49cde67a0e1'::uuid;\r\n                  id                  |       dag_id\r\n--------------------------------------+---------------------\r\n 01998540-a9c1-7d90-8b59-7fbe38294778 | some_dag\r\n\r\n\r\n\r\n                                                                        QUERY PLAN\r\n----------------------------------------------------------------------------------------------------------------------------------------------------------\r\n Nested Loop  (cost=42.04..95.70 rows=41 width=1641) (actual time=0.398..0.400 rows=1 loops=1)\r\n   Buffers: shared hit=197\r\n   ->  HashAggregate  (cost=41.75..42.16 rows=41 width=16) (actual time=0.386..0.387 rows=2 loops=1)\r\n         Group Key: task_instance.dag_version_id\r\n         Batches: 1  Memory Usage: 24kB\r\n         Buffers: shared hit=191\r\n         ->  Nested Loop  (cost=0.70..41.65 rows=41 width=16) (actual time=0.031..0.339 rows=236 loops=1)\r\n               Buffers: shared hit=191\r\n               ->  Index Scan using dag_run_pkey on dag_run  (cost=0.29..15.20 rows=10 width=61) (actual time=0.012..0.036 rows=10 loops=1)\r\n                     Index Cond: (id = ANY ('{10205,10203,10201,10200,10198,9455,9430,9415,9413,9407}'::integer[]))\r\n                     Buffers: shared hit=30\r\n               ->  Index Scan using ti_dag_run on task_instance  (cost=0.41..2.63 rows=1 width=71) (actual time=0.007..0.020 rows=24 loops=10)\r\n                     Index Cond: (((dag_id)::text = (dag_run.dag_id)::text) AND ((run_id)::text = (dag_run.run_id)::text))\r\n                     Buffers: shared hit=161\r\n   ->  Index Scan using serialized_dag_dag_version_id_uq on serialized_dag  (cost=0.29..1.31 rows=1 width=1641) (actual time=0.005..0.005 rows=0 loops=2)\r\n         Index Cond: (dag_version_id = task_instance.dag_version_id)\r\n         Filter: (id <> '019986ae-e948-79cc-b9cc-d49cde67a0e1'::uuid)\r\n         Rows Removed by Filter: 0\r\n         Buffers: shared hit=6\r\n Planning:\r\n   Buffers: shared hit=99\r\n Planning Time: 2.678 ms\r\n Execution Time: 0.448 ms\r\n```",
  "Requirement ID: ISSUE-56256\nTitle: [v3-1-test] fix(api_fastapi): adjust model validator signature of TriggerDAGRunPostBody (#56025) (#56026)\nState: closed\nAuthor: github-actions[bot]\nLabels: area:API\nBody:\n(cherry picked from commit bfb7ecbb7091caf862b283f39b07170f34d056cb)\n\nCo-authored-by: Daniel Gellert <gellertd@protonmail.com>\nCo-authored-by: Daniel Gellert <dab.dada@gmail.com>",
  "Requirement ID: ISSUE-56255\nTitle: [v3-1-test] Fix cron expression display for Day-of-Month and Day-of-Week conflicts (#54644)\nState: closed\nAuthor: github-actions[bot]\nLabels: \nBody:\n* Fix cron expression display for Day-of-Month and Day-of-Week conflicts\n\n* Add test case for CronMixin description attribute\n\n* Add test case for CronMixin description attribute\n\n* Add test case for CronMixin description attribute\n\n* Add test case for CronMixin description attribute\n\n* Add test case for CronMixin description attribute\n\n---------\n(cherry picked from commit c6531bb06372c430e7ce4765319181f6b9ca6cfd)\n\nCo-authored-by: shreyaskj-0710 <shreyas.kj@zemosolabs.com>\nCo-authored-by: Ryan Hatter <25823361+RNHTTR@users.noreply.github.com>",
  "Requirement ID: ISSUE-56254\nTitle: Trigger form missing \"Select Recent Configurations\" from airflow 2\nState: open\nAuthor: NilsJPWerner\nLabels: kind:feature, good first issue, area:core, area:UI\nBody:\n### Apache Airflow version\n\n3.1.0\n\n### If \"Other Airflow 2 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\nAirflow 2 had a very useful dropdown with recent manual dag run configuration. This seems to be missing from airflow 3.\n\n<img width=\"646\" height=\"228\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/7deef8ca-8adc-4dcc-bb95-c7232912fea2\" />\n\n### What you think should happen instead?\n\n_No response_\n\n### How to reproduce\n\n.\n\n### Operating System\n\nDebian\n\n### Versions of Apache Airflow Providers\n\n_No response_\n\n### Deployment\n\nAstronomer\n\n### Deployment details\n\n_No response_\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56253\nTitle: Fix SQLA Mapped type import errors in model classes\nState: closed\nAuthor: amoghrajesh\nLabels: \nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\nSQLAlchemy requires the Mapped type to be available at runtime when processing\r\nclass annotations, but several model files imported it only in TYPE_CHECKING\r\nblocks. This caused NameError exceptions during database initialization. CI broken with error:\r\n\r\n```\r\n  /opt/airflow/airflow-core/src/airflow/dag_processing/dagbag.py:40 DeprecationWarning: airflow.exceptions.AirflowDagCycleException is deprecated. Use airflow.sdk.exceptions.AirflowDagCycleException instead.\r\n  Traceback (most recent call last):\r\n    File \"/usr/python/lib/python3.13/site-packages/sqlalchemy/util/typing.py\", line 306, in eval_name_only\r\n      return base_globals[name]\r\n             ~~~~~~~~~~~~^^^^^^\r\n  KeyError: 'Mapped'\r\n  \r\n  The above exception was the direct cause of the following exception:\r\n  \r\n  Traceback (most recent call last):\r\n    File \"/usr/python/lib/python3.13/site-packages/sqlalchemy/orm/util.py\", line 2221, in _cleanup_mapped_str_annotation\r\n      obj = eval_name_only(mm.group(1), originating_module)\r\n    File \"/usr/python/lib/python3.13/site-packages/sqlalchemy/util/typing.py\", line 314, in eval_name_only\r\n      raise NameError(\r\n          f\"Could not locate name {name} in module {module_name}\"\r\n      ) from ke\r\n  NameError: Could not locate name Mapped in module airflow.models.taskmap\r\n  \r\n  The above exception was the direct cause of the following exception:\r\n  \r\n  Traceback (most recent call last):\r\n    File \"/usr/python/lib/python3.13/site-packages/sqlalchemy/orm/util.py\", line 2323, in _extract_mapped_subtype\r\n      annotated = de_stringify_annotation(\r\n          cls,\r\n      ...<2 lines>...\r\n          str_cleanup_fn=_cleanup_mapped_str_annotation,\r\n      )\r\n    File \"/usr/python/lib/python3.13/site-packages/sqlalchemy/util/typing.py\", line 156, in de_stringify_annotation\r\n      annotation = str_cleanup_fn(annotation, originating_module)\r\n    File \"/usr/python/lib/python3.13/site-packages/sqlalchemy/orm/util.py\", line 2223, in _cleanup_mapped_str_annotation\r\n      raise _CleanupError(\r\n      ...<4 lines>...\r\n      ) from ne\r\n  sqlalchemy.orm.util._CleanupError: For annotation \"Mapped[str]\", could not resolve container type \"Mapped\".  Please ensure this type is imported at the module level outside of TYPE_CHECKING blocks\r\n  \r\n  The above exception was the direct cause of the following exception:\r\n  \r\n  Traceback (most recent call last):\r\n    File \"/usr/python/bin/airflow\", line 4, in <module>\r\n      from airflow.__main__ import main\r\n    File \"/opt/airflow/airflow-core/src/airflow/__main__.py\", line 37, in <module>\r\n      from airflow.cli import cli_parser\r\n    File \"/opt/airflow/airflow-core/src/airflow/cli/cli_parser.py\", line 40, in <module>\r\n      from airflow.cli.cli_config import (\r\n      ...<5 lines>...\r\n      )\r\n    File \"/opt/airflow/airflow-core/src/airflow/cli/cli_config.py\", line 36, in <module>\r\n      from airflow.utils.cli import ColorMode\r\n    File \"/opt/airflow/airflow-core/src/airflow/utils/cli.py\", line 39, in <module>\r\n      from airflow.dag_processing.dagbag import DagBag\r\n    File \"/opt/airflow/airflow-core/src/airflow/dag_processing/dagbag.py\", line 52, in <module>\r\n      from airflow.serialization.serialized_objects import LazyDeserializedDAG\r\n    File \"/opt/airflow/airflow-core/src/airflow/serialization/serialized_objects.py\", line 69, in <module>\r\n      from airflow.models.dag import DagModel\r\n    File \"/opt/airflow/airflow-core/src/airflow/models/dag.py\", line 55, in <module>\r\n      from airflow.models.dagrun import DagRun\r\n    File \"/opt/airflow/airflow-core/src/airflow/models/dagrun.py\", line 63, in <module>\r\n      from airflow.models import Deadline, Log\r\n    File \"<frozen importlib._bootstrap>\", line 1412, in _handle_fromlist\r\n    File \"/opt/airflow/airflow-core/src/airflow/models/__init__.py\", line 85, in __getattr__\r\n      val = import_string(f\"{path}.{name}\")\r\n    File \"/opt/airflow/airflow-core/src/airflow/utils/module_loading.py\", line 41, in import_string\r\n      module = import_module(module_path)\r\n    File \"/usr/python/lib/python3.13/importlib/__init__.py\", line 88, in import_module\r\n      return _bootstrap._gcd_import(name[level:], package, level)\r\n             ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n    File \"/opt/airflow/airflow-core/src/airflow/models/deadline.py\", line 35, in <module>\r\n      from airflow.models import Trigger\r\n    File \"<frozen importlib._bootstrap>\", line 1412, in _handle_fromlist\r\n    File \"/opt/airflow/airflow-core/src/airflow/models/__init__.py\", line 85, in __getattr__\r\n      val = import_string(f\"{path}.{name}\")\r\n    File \"/opt/airflow/airflow-core/src/airflow/utils/module_loading.py\", line 41, in import_string\r\n      module = import_module(module_path)\r\n    File \"/usr/python/lib/python3.13/importlib/__init__.py\", line 88, in import_module\r\n      return _bootstrap._gcd_import(name[level:], package, level)\r\n             ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n    File \"/opt/airflow/airflow-core/src/airflow/models/trigger.py\", line 36, in <module>\r\n      from airflow.models.taskinstance import TaskInstance\r\n    File \"/opt/airflow/airflow-core/src/airflow/models/taskinstance.py\", line 81, in <module>\r\n      from airflow.models.taskmap import TaskMap\r\n    File \"/opt/airflow/airflow-core/src/airflow/models/taskmap.py\", line 55, in <module>\r\n      class TaskMap(TaskInstanceDependencies):\r\n      ...<216 lines>...\r\n              return all_expanded_tis, total_expanded_ti_count - 1\r\n    File \"/usr/python/lib/python3.13/site-packages/sqlalchemy/orm/decl_api.py\", line 198, in __init__\r\n      _as_declarative(reg, cls, dict_)\r\n      ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\r\n    File \"/usr/python/lib/python3.13/site-packages/sqlalchemy/orm/decl_base.py\", line 245, in _as_declarative\r\n      return _MapperConfig.setup_mapping(registry, cls, dict_, None, {})\r\n             ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n    File \"/usr/python/lib/python3.13/site-packages/sqlalchemy/orm/decl_base.py\", line 326, in setup_mapping\r\n      return _ClassScanMapperConfig(\r\n          registry, cls_, dict_, table, mapper_kw\r\n      )\r\n    File \"/usr/python/lib/python3.13/site-packages/sqlalchemy/orm/decl_base.py\", line 562, in __init__\r\n      self._scan_attributes()\r\n      ~~~~~~~~~~~~~~~~~~~~~^^\r\n    File \"/usr/python/lib/python3.13/site-packages/sqlalchemy/orm/decl_base.py\", line 1022, in _scan_attributes\r\n      collected_annotation = self._collect_annotation(\r\n          name, annotation, base, None, obj\r\n      )\r\n    File \"/usr/python/lib/python3.13/site-packages/sqlalchemy/orm/decl_base.py\", line 1302, in _collect_annotation\r\n      extracted = _extract_mapped_subtype(\r\n          raw_annotation,\r\n      ...<6 lines>...\r\n          expect_mapped=expect_mapped and not is_dataclass,\r\n      )\r\n    File \"/usr/python/lib/python3.13/site-packages/sqlalchemy/orm/util.py\", line 2330, in _extract_mapped_subtype\r\n      raise orm_exc.MappedAnnotationError(\r\n      ...<3 lines>...\r\n      ) from ce\r\n  sqlalchemy.orm.exc.MappedAnnotationError: Could not interpret annotation Mapped[str].  Check that it uses names that are correctly imported at the module level. See chained stack trace for more hints.\r\n  \r\n  Error: check_environment returned 1. Exiting.\r\n  \r\n```\r\n\r\nExample: https://github.com/apache/airflow/actions/runs/18125410465/job/51580394658\r\n\r\nSeems to be caused by: #55954\r\n\r\nThe reason is that: \r\n\r\n\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56251\nTitle: [v3-1-test] Bump version of providers (#56208)\nState: closed\nAuthor: github-actions[bot]\nLabels: \nBody:\n(cherry picked from commit 8615b55ae33433f8014a1814d7c6f242cbb46d69)\n\nCo-authored-by: Elad Kalif <45845474+eladkal@users.noreply.github.com>",
  "Requirement ID: ISSUE-56250\nTitle: AF3 UI issues\nState: open\nAuthor: NilsJPWerner\nLabels: kind:bug, priority:medium, area:UI, affected_version:3.1\nBody:\n### Apache Airflow version\n3.1.0\n### If \"Other Airflow 2 version\" selected, which one?\n_No response_\n### What happened?\nCollecting a few issues related to the ui. Let me know if it's better if I split them up.\n- [ ] The dag options drop down like 'Number of Dag Runs' & 'Show Gantt' don't have persistent state and reset on reloads.\n- [ ] The hover tooltips disappear about a second after popping up.\n- [ ] The grid loads super slowly on first load. (https://github.com/apache/airflow/issues/56554)\n- [x] The log highlight color is extremely low contrast. (https://github.com/apache/airflow/pull/56379)\n- [ ] Missing \"Mark Task Group as ...\" button that existed in AF 2 (https://github.com/apache/airflow/issues/56103)\n- [x] The grid dag timing seems to always be in terms of seconds (https://github.com/apache/airflow/pull/56403)\n- [ ] Newly added tasks don't display a colored task status box in the grid if you force them to run for a pre-existing dag run.\n- [ ] The tasks are out of order (duplicate https://github.com/apache/airflow/issues/55899)\n- [x] The minimum width of the task names seems a bit too narrow. When you load more dag runs the text gets aggressively cutoff (https://github.com/apache/airflow/pull/56378)\n- [ ] The 'Rendered Templates' tab expands to the widest string rather than wrapping. This leads you to need to scroll for potentially a while if you want to use the copy button. (https://github.com/apache/airflow/pull/56382)\n\n- [ ] The 'Queue up new tasks' button is disabled even when there are new tasks that have been added to a dag\n\n![Image](https://github.com/user-attachments/assets/f207d409-f11f-4786-94da-d21495178edf)\n\n- [x] Dag run_id is no longer readily viewable. It is buried in details. (https://github.com/apache/airflow/pull/56392)\n- [ ] Grid position will reset if you go between a \"deselected\" and a \"selected\" state. Zoom in and click a task and then click it again to deselect.\n- [ ] Opening a dag in graph view should auto select the latest dag run\n\n### What you think should happen instead?\n\n_No response_\n\n### How to reproduce\n\nVisible on any dag. The slow loading seems worse with larger dags with more more history.\n\n### Operating System\n\nDebian\n\n### Versions of Apache Airflow Providers\n\n_No response_\n\n### Deployment\n\nAstronomer\n\n### Deployment details\n\n_No response_\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56249\nTitle: [UI] Allow copying text in Log view(remove user-select: none)\nState: closed\nAuthor: rich7420\nLabels: area:UI\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\nRelated #56248 \r\n\r\nWhat changed? \r\n- Removed the CSS properties that disabled text selection (userSelect: none, WebkitUserSelect: none).\r\n\r\n- Updated the <chakra.span> to explicitly allow text selection by using userSelect=\"text\".\r\n\r\n\r\n---\r\nHow to test \r\n1. Run Airflow webserver.\r\n\r\n2. Trigger a DAG run that produces some logs.\r\n\r\n3. Open any task\u2019s log view.\r\n\r\n4. Try selecting/copying log lines with your mouse or keyboard.\r\n\t\r\n\r\n<img width=\"877\" height=\"148\" alt=\"image\" src=\"https://github.com/user-attachments/assets/03c7afc3-3cca-4dd1-9831-ab860cb3fbc3\" />",
  "Requirement ID: ISSUE-56248\nTitle: Text in Log view can not be copied\nState: closed\nAuthor: anavrotski\nLabels: kind:bug, area:UI\nBody:\n### Description\n\nAt least in Chrome, I can not copy errors from the logs view, very inconvenient.\n\n\n### Use case/motivation\n\n_No response_\n\n### Related issues\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56247\nTitle: Remove TIMEOUT parameter from the dataproc_metastore system test\nState: closed\nAuthor: VladaZakharova\nLabels: provider:google, area:providers\nBody:\nThis PR removes using TIMEOUT parameter since sometimes the execution time can very and does not fit estimated timeout which leads to fail of the system test.\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56246\nTitle: Login Fails with \"Bad Request\" (Missing CSRF Token)\nState: open\nAuthor: ciancolo\nLabels: kind:bug, area:webserver, area:auth, area:core, needs-triage, affected_version:3.1\nBody:\n### Apache Airflow version\n\n3.1.0\n\n### If \"Other Airflow 2 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\nSometimes, during login, some users run into a Bad Request response from the server:\n\n<img width=\"743\" height=\"112\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/5263d4eb-9f11-4b39-98d9-c2db8899071f\" />\n\nIt looks like the request is missing the CSRF session token. Unfortunately, we couldn\u2019t find any useful information in the logs:\n\n<img width=\"1564\" height=\"415\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/43732416-ca5e-4734-ba0a-d12f9d34c6ca\" />\n\nInterestingly, the issue resolves itself after a few minutes without any manual intervention.\n\n### What you think should happen instead?\n\n_No response_\n\n### How to reproduce\n\nNot  sure what cause this issue. \n\n### Operating System\n\n\"Ubuntu 22.04.5 LTS\n\n### Versions of Apache Airflow Providers\n\n\napache-airflow                           3.1.0\napache-airflow-core                      3.1.0\napache-airflow-providers-apache-hdfs     4.10.3\napache-airflow-providers-apache-spark    5.3.2\napache-airflow-providers-apache-sqoop    3.2.0\napache-airflow-providers-common-compat   1.7.4\napache-airflow-providers-common-io       1.6.3\napache-airflow-providers-common-sql      1.28.1\napache-airflow-providers-elasticsearch   6.3.3\napache-airflow-providers-fab             2.4.3\napache-airflow-providers-http            5.3.4\napache-airflow-providers-jdbc            5.2.3\napache-airflow-providers-postgres        6.3.0\napache-airflow-providers-salesforce      5.11.3\napache-airflow-providers-smtp            2.2.1\napache-airflow-providers-ssh             4.1.4\napache-airflow-providers-standard        1.8.0\napache-airflow-providers-tableau         5.2.0\napache-airflow-task-sdk                  1.1.0\n\n\n### Deployment\n\nVirtualenv installation\n\n### Deployment details\n\n_No response_\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56245\nTitle: Chart gitsync vars\nState: closed\nAuthor: ido177\nLabels: area:helm-chart\nBody:\ncloses: #56143\r\n\r\n\r\n\r\n---\r\nAt this moment, users have to specify git_username and git_token twice in the git-sync secret, once for GIT_SYNC_* (v3) and once for GITSYNC_* (v4). This is redundant and confusing.\r\n```\r\n---\r\napiVersion: v1\r\nkind: Secret\r\nmetadata:\r\n  name: airflow-git-credentials\r\nstringData:\r\n  GITSYNC_USERNAME: \"git_username\"\r\n  GITSYNC_PASSWORD: \"git_token\"\r\n  GIT_SYNC_USERNAME: \"git_username\"\r\n  GIT_SYNC_PASSWORD: \"git_token\"\r\n```\r\n\r\nThis PR updates the Helm chart logic to choose the correct environment variable set based on the git-sync image tag:\r\nIf the tag is < 4.0.0 - use GIT_SYNC_*\r\nIf the tag is >= 4.0.0 or latest - use GITSYNC_*\r\nExample (after this change)\r\n```\r\napiVersion: v1\r\nkind: Secret\r\nmetadata:\r\n  name: airflow-git-credentials\r\nstringData:\r\n  GITSYNC_USERNAME: \"git_username\"\r\n  GITSYNC_PASSWORD: \"git_token\"\r\n```\r\nOnly the relevant variables need to be set, depending on the git-sync version.",
  "Requirement ID: ISSUE-56244\nTitle: Fix XCom object storage backend path validation\nState: closed\nAuthor: amoghrajesh\nLabels: area:providers, provider:common-io\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n`universal_pathlib` releases a 0.3.0 version and that added `pathlib_abc==0.5.1` in it,  which introduced stricter type checking on Path objects.\r\n\r\n\r\nThe original code was calling `is_relative_to()` incorrectly on `Path` objects leading to error:\r\n\r\n```\r\n  Using airflow version from current sources\r\n\r\n  providers/common/io/src/airflow/providers/common/io/xcom/backend.py:105: error: Argument 1 to \"is_relative_to\" of \"PurePath\" has incompatible type \"ObjectStoragePath\"; expected \"PurePath\"  [arg-type]\r\n                  if not Path.is_relative_to(ObjectStoragePath(data), p):\r\n                                             ^~~~~~~~~~~~~~~~~~~~~~~\r\n  providers/common/io/src/airflow/providers/common/io/xcom/backend.py:105: error: Argument 2 to \"is_relative_to\" of \"PurePath\" has incompatible type \"ObjectStoragePath\"; expected \"str | PathLike[str]\" \r\n  [arg-type]\r\n                  if not Path.is_relative_to(ObjectStoragePath(data), p):\r\n                                                                      ^\r\n  Found 2 errors in 1 file (checked 3984 source files)\r\n\r\n```\r\n\r\nI have changed to call it as an instance method on the ObjectStoragePath object itself.\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56243\nTitle: Unable to read logs from s3\nState: open\nAuthor: rcampos87\nLabels: kind:bug, provider:amazon, area:logging, area:core, affected_version:3.1\nBody:\n### Apache Airflow version\n\n3.1.0\n\n### If \"Other Airflow 2 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\nI was using airflow 3.0.6 on kubernetes and I had setup logging to s3, everything was working completely fine, and then I upgraded to airflow 3.1.0. Now Im unable to retrieve logs from s3 when I try to view them for a task -- the UI shows 500 Internal Server Error.\n\nChecking to api-server's logs i see:\n\n```                                                                                                                                                       \u2502\n\u2502 api-server   File \"/home/airflow/.local/lib/python3.10/site-packages/botocore/signers.py\", line 200, in sign                                                                                                                                                     \u2502\n\u2502 api-server     auth.add_auth(request)                                                                                                                                                                                                                            \u2502\n\u2502 api-server   File \"/home/airflow/.local/lib/python3.10/site-packages/botocore/auth.py\", line 422, in add_auth                                                                                                                                                    \u2502\n\u2502 api-server     raise NoCredentialsError()                                                                                                                                                                                                                        \u2502\n\u2502 api-server botocore.exceptions.NoCredentialsError: Unable to locate credentials                      \n```\n\nI can confirm logs are successfully added to s3 though.\n\n### What you think should happen instead?\n\nIf logs to s3 were working completely fine on 3.0.6 Id expect the same on 3.1.0\n\n### How to reproduce\n\nSetup remote logging to s3:\n```\n  logging:\n    remote_logging: 'True'\n    remote_base_log_folder: s3://airflow/logs\n    remote_log_conn_id: s3_default\n    encrypt_s3_logs: 'False'\n```\nTry viewing task logs on UI from DAGs\n\n### Operating System\n\nDebian GNU/Linux 12 (bookworm)\n\n### Versions of Apache Airflow Providers\n\napache-airflow-providers-amazon==9.14.0\napache-airflow-providers-celery==3.12.3\napache-airflow-providers-cncf-kubernetes==10.8.0\napache-airflow-providers-common-compat==1.7.4\napache-airflow-providers-common-io==1.6.3\napache-airflow-providers-common-messaging==2.0.0\napache-airflow-providers-common-sql==1.28.1\napache-airflow-providers-docker==4.4.3\napache-airflow-providers-elasticsearch==6.3.3\napache-airflow-providers-fab==2.4.3\napache-airflow-providers-ftp==3.13.2\napache-airflow-providers-git==0.0.8\napache-airflow-providers-google==18.0.0\napache-airflow-providers-grpc==3.8.2\napache-airflow-providers-hashicorp==4.3.2\napache-airflow-providers-http==5.3.4\napache-airflow-providers-microsoft-azure==12.7.1\napache-airflow-providers-mysql==6.3.4\napache-airflow-providers-odbc==4.10.2\napache-airflow-providers-openlineage==2.7.1\napache-airflow-providers-postgres==6.3.0\napache-airflow-providers-redis==4.3.1\napache-airflow-providers-sendgrid==4.1.3\napache-airflow-providers-sftp==5.4.0\napache-airflow-providers-slack==9.3.0\napache-airflow-providers-smtp==2.2.1\napache-airflow-providers-snowflake==6.5.4\napache-airflow-providers-ssh==4.1.4\napache-airflow-providers-standard==1.8.0\n\n### Deployment\n\nOfficial Apache Airflow Helm Chart\n\n### Deployment details\n\n_No response_\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56242\nTitle: Disable Gantt view by default\nState: closed\nAuthor: dheerajturaga\nLabels: area:UI\nBody:\nGantt view is great to get insight on runtimes for a given dag run. However, user may not want to see gantt chart for every dag run they click on by default. It also moves around the grid view which can be annoying when you are trying to quickly look at some results. \r\n\r\nThis feature is best when its default state is OFF and can be turned on as needed.\r\n\r\n<img width=\"852\" height=\"642\" alt=\"image\" src=\"https://github.com/user-attachments/assets/594eeaad-3f77-4952-86a4-a86994b79ec6\" />",
  "Requirement ID: ISSUE-56241\nTitle: Prepare fab and amazon providers to release (September 2025)\nState: closed\nAuthor: eladkal\nLabels: area:providers, kind:documentation, provider:fab\nBody:\n```\r\nSummary of prepared documentation:\r\n\r\nSuccess: 2\r\n\r\namazon fab\r\n\r\n\r\nSuccessfully prepared documentation for packages!\r\n```",
  "Requirement ID: ISSUE-56240\nTitle: Add revoke_task implementation to EdgeExecutor for task queued timeout support\nState: closed\nAuthor: dheerajturaga\nLabels: area:providers, provider:edge\nBody:\nEdgeExecutor was not implementing the revoke_task() method, causing it to raise NotImplementedError when the scheduler attempted to handle tasks stuck in queued state. This meant the task_queued_timeout feature was completely bypassed for EdgeExecutor.\r\n\r\nThis commit implements revoke_task() to:\r\n  - Remove tasks from executor's internal state (running, queued_tasks, last_reported_state)\r\n  - Delete corresponding EdgeJobModel records to prevent edge workers from picking them up\r\n  - Enable proper task queued timeout handling by the scheduler\r\n  \r\nBefore: \r\n<img width=\"1461\" height=\"583\" alt=\"image\" src=\"https://github.com/user-attachments/assets/c09ae6f8-22f4-4c87-b47a-0744a1c17201\" />\r\n\r\nAfter:\r\n<img width=\"1915\" height=\"932\" alt=\"image\" src=\"https://github.com/user-attachments/assets/f7fa37e2-79a1-4812-bfe9-9fd2dc411ba3\" />",
  "Requirement ID: ISSUE-56239\nTitle: Temporarily limit fastapi to less than 0.118.0 to fix CI\nState: closed\nAuthor: amoghrajesh\nLabels: \nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\nFastapi 0.118 introduces a breaking change that breaks our CI: https://github.com/apache/airflow/actions/runs/18116283428\r\n\r\n\r\n* Previously (\u22640.117), the yield dependency cleanup code (e.g., closing a create_session context manager in _get_session) ran before the response was sent.\r\n* In 0.118.0+, the cleanup now runs after the response starts streaming.\r\n* This means exceptions (like duplicate entity errors when the DB session commits implicitly) are raised too late \u2192 response has already started, so FastAPI can\u2019t change the status code (\"Caught handled exception, but response already started.\").\r\n\r\n\r\nNext steps is to raising this with Sebasti\u00e1n (FastAPI maintainer) since this was an intentional change to support streaming responses, and he has signaled awareness that it could cause issues and synchronise and resolve the issue.\r\n\r\nrelated: https://www.linkedin.com/posts/tiangolo_advanced-dependencies-fastapi-activity-7378282080457187328-ZTT_?utm_source=share&utm_medium=member_desktop&rcm=ACoAAAAbPJABWNiWtLGbVPUKL8PDTSlmqJVvFTs\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56238\nTitle: Fix multi-line drag selection in task log view\nState: closed\nAuthor: Brunda10\nLabels: area:UI, backport-to-v3-1-test\nBody:\nCloses : #55879\r\n\r\nPreviously, dragging across multiple lines in the task log view caused the selection to jump back to the top of the logs, making it impossible to select multiple lines reliably. This was caused by the combination of `position: absolute` and `transform: translateY()` on virtualized rows.\r\n\r\nThis PR fixes the issue by:\r\n\r\n- Removing the `transform` property and relying solely on `top` for positioning.\r\n- Ensuring each row remains virtualized for performance.\r\n- Adjusted selection CSS for better visibility \r\n\r\nUsers can now drag-select multiple lines smoothly, improving log readability and overall UX.\r\n\r\n[Screencast from 2025-09-29 16-15-40.webm](https://github.com/user-attachments/assets/4a353bbe-75a4-4047-8395-bf8f732ab13c)",
  "Requirement ID: ISSUE-56237\nTitle: Enable PT011 rule to prvoider tests\nState: closed\nAuthor: xchwan\nLabels: provider:google, area:providers\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\nissue: Enable Even More PyDocStyle Checks #40567\r\n@ferruzzi\r\nThis PR is for enable PT011 rule:\r\nPT011: all instances of pytest.raises should include a match term to make sure they are catching the right error.\r\nhttps://docs.astral.sh/ruff/rules/pytest-raises-too-broad/\r\nThere are 102 files changes is need.\r\nSo, I separate to many PR, which contains about 5 file changes for easy review.\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56236\nTitle: [v3-1-test] Fix scheduler crash during 3.0 to 3.1 migration when retry_delay is None (#56202)\nState: closed\nAuthor: github-actions[bot]\nLabels: area:dev-tools, area:serialization, backport-to-v3-1-test\nBody:\n* Kaxil's suggestions\n\n* make default a float because some tests are complaining\n\n* Fix test\n\n* fixup! Fix test\n\n* fixup! fixup! Fix test\n\n---------\n(cherry picked from commit 1f976d00fccb5875f0bf5aa07b51af12feb01995)\n\nCo-authored-by: Dheeraj Turaga <dheerajturaga@gmail.com>\nCo-authored-by: Kaxil Naik <kaxilnaik@gmail.com>",
  "Requirement ID: ISSUE-56235\nTitle: can't render gantt in safari\nState: closed\nAuthor: Allencccc\nLabels: kind:bug, good first issue, area:UI, affected_version:3.1\nBody:\n### Apache Airflow version\n\n3.1.0\n\n### If \"Other Airflow 2 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\nin dag run view, can't render gantt view in safari\n\n<img width=\"655\" height=\"384\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/b1de27d4-248a-4c6e-800b-634944f3222e\" />\n\n### What you think should happen instead?\n\n_No response_\n\n### How to reproduce\n\nview dag run in safari\n\n### Operating System\n\nmacos Version 24.6.0\n\n### Versions of Apache Airflow Providers\n\ndocker version: 3.1.0\n\n### Deployment\n\nDocker-Compose\n\n### Deployment details\n\n_No response_\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56234\nTitle: Fix CloudRunExecuteJobOperator logging links in deferred mode\nState: closed\nAuthor: steveahnahn\nLabels: provider:google, area:providers\nBody:\n## Problem\r\n\r\nFix `ValueError: dictionary update sequence element #0 has length 1; 2 is required error` when using CloudRunExecuteJobOperator with deferrable=True. The issue occurred because XCom data serialization in deferred execution sometimes stores logging link data as strings instead of dictionaries, causing BaseGoogleLink.get_config() to fail when attempting dict updates\r\n\r\n## Solution\r\n\r\nModified BaseGoogleLink.get_config() to handle both string and dictionary XCom values \r\n\r\n## Testing\r\n\r\nAdded comprehensive unit test test_get_config_with_string_xcom_value covering:\r\n    - String XCom values\r\n    - Dictionary XCom values\r\n    - None XCom values\r\n\r\n## Related Issue:\r\ncloses: #55996",
  "Requirement ID: ISSUE-56232\nTitle: Use airflow client in e2e tests\nState: open\nAuthor: gopidesupavan\nLabels: kind:meta, area:core\nBody:\n### Body\n\nWe have e2e tests, but it uses core airflow endpoints. potentially we could leverage airflow client , so that we have advantage of testing airflow client and also the real dags.\n\n\n\n### Committer\n\n- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",
  "Requirement ID: ISSUE-56231\nTitle: [v3-1-test] Fix upgrade checks with prek (#56222)\nState: closed\nAuthor: github-actions[bot]\nLabels: area:dev-tools, backport-to-v3-1-test\nBody:\n(cherry picked from commit 9d4447d2784e560551b8d870e485fc24994858c9)\n\nCo-authored-by: Jens Scheffler <95105677+jscheffl@users.noreply.github.com>",
  "Requirement ID: ISSUE-56230\nTitle: CLI backfill command does not marshall configuration\nState: closed\nAuthor: uq-os\nLabels: kind:bug, area:CLI, area:backfill, affected_version:3.1\nBody:\n### Apache Airflow version\n\n3.1.0\n\n### If \"Other Airflow 2 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\nI tried to run a backfill using the CLI. I get the following error.\n\n```\n(flux_raw) \u03bb  flux_raw git:(flux-raw) \u2717 airflow backfill create --dag-id example_dag \\\n      --from-date 2024-01-16 \\\n      --to-date 2024-01-17 \\\n      --dag-run-conf '{\"example_key\": \"example_value\"}'\n[2025-09-25T15:45:33.922+1000] {providers_manager.py:953} INFO - The hook_class 'airflow.providers.standard.hooks.filesystem.FSHook' is not fully initialized (UI widgets will be missing), because the 'flask_appbuilder' package is not installed, however it is not required for Airflow components to work\n[2025-09-25T15:45:33.923+1000] {providers_manager.py:953} INFO - The hook_class 'airflow.providers.standard.hooks.package_index.PackageIndexHook' is not fully initialized (UI widgets will be missing), because the 'flask_appbuilder' package is not installed, however it is not required for Airflow components to work\n[2025-09-25T15:45:33.980+1000] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.\nTraceback (most recent call last):\n  File \"/Users/xxx/tern_code/flux_dag/flux_raw/.venv/bin/airflow\", line 10, in <module>\n    sys.exit(main())\n             ~~~~^^\n  File \"/Users/xxx/tern_code/flux_dag/flux_raw/.venv/lib/python3.13/site-packages/airflow/__main__.py\", line 55, in main\n    args.func(args)\n    ~~~~~~~~~^^^^^^\n  File \"/Users/xxx/tern_code/flux_dag/flux_raw/.venv/lib/python3.13/site-packages/airflow/cli/cli_config.py\", line 48, in command\n    return func(*args, **kwargs)\n  File \"/Users/xxx/tern_code/flux_dag/flux_raw/.venv/lib/python3.13/site-packages/airflow/utils/cli.py\", line 112, in wrapper\n    return f(*args, **kwargs)\n  File \"/Users/xxx/tern_code/flux_dag/flux_raw/.venv/lib/python3.13/site-packages/airflow/utils/providers_configuration_loader.py\", line 55, in wrapped_function\n    return func(*args, **kwargs)\n  File \"/Users/xxx/tern_code/flux_dag/flux_raw/.venv/lib/python3.13/site-packages/airflow/cli/commands/backfill_command.py\", line 73, in create_backfill\n    _create_backfill(\n    ~~~~~~~~~~~~~~~~^\n        dag_id=args.dag_id,\n        ^^^^^^^^^^^^^^^^^^^\n    ...<5 lines>...\n        reprocess_behavior=reprocess_behavior,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/xxx/tern_code/flux_dag/flux_raw/.venv/lib/python3.13/site-packages/airflow/models/backfill.py\", line 488, in _create_backfill\n    _create_backfill_dag_run(\n    ~~~~~~~~~~~~~~~~~~~~~~~~^\n        dag=dag,\n        ^^^^^^^^\n    ...<5 lines>...\n        session=session,\n        ^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/xxx/tern_code/flux_dag/flux_raw/.venv/lib/python3.13/site-packages/airflow/models/backfill.py\", line 332, in _create_backfill_dag_run\n    dr = dag.create_dagrun(\n        run_id=DagRun.generate_run_id(\n    ...<11 lines>...\n        session=session,\n    )\n  File \"/Users/xxx/tern_code/flux_dag/flux_raw/.venv/lib/python3.13/site-packages/airflow/utils/session.py\", line 99, in wrapper\n    return func(*args, **kwargs)\n  File \"/Users/xxx/tern_code/flux_dag/flux_raw/.venv/lib/python3.13/site-packages/airflow/models/dag.py\", line 1584, in create_dagrun\n    copied_params.update(conf)\n    ~~~~~~~~~~~~~~~~~~~~^^^^^^\n  File \"/Users/xxx/tern_code/flux_dag/flux_raw/.venv/lib/python3.13/site-packages/airflow/sdk/definitions/param.py\", line 234, in update\n    super().update(*args, **kwargs)\n    ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"<frozen _collections_abc>\", line 992, in update\nValueError: not enough values to unpack (expected 2, got 1)\n```\n\n### What you think should happen instead?\n\nThe CLI should have created a backfill for my DAG. I tested the same setup executed using the web UI and was able to sucesfully launch a job. \n\nI noticed that my configuration is a string in the update function when submitted via CLI, however when submitted via the UI this configuration is a dictionary at this stage.\n\nTo do this I monkeypatched the update function that its failing on (`airflow/sdk/definitions/param.py:234`) with this (added a value error)\n```\n    def update(self, *args, **kwargs) -> None:\n        if len(args) == 1 and not kwargs and isinstance(args[0], ParamsDict):\n            return super().update(args[0].__dict)\n        \n        raise ValueError((*args,args, type(args[0])))\n        super().update(*args, **kwargs)\n```\n\nThen submit backfill jobs via the CLI and via the UI. You will see `class 'str'` for the CLI submission and `class 'dict'` for the UI submission\n\n### How to reproduce\n\nRun the following command\n\n```bash\nairflow backfill create --dag-id example_dag \\\n      --from-date 2024-01-16 \\\n      --to-date 2024-01-17 \\\n      --dag-run-conf '\n        {\n            \"example_key\": \"example_value\"\n        }\n    '\n```\n\n### Operating System\n\nOSX 15.7\n\n### Versions of Apache Airflow Providers\n\n_No response_\n\n### Deployment\n\nVirtualenv installation\n\n### Deployment details\n\nrun using `airflow standalone`\n\n### Anything else?\n\n[See slack discussion for further details](https://apache-airflow.slack.com/archives/CCQ7EGB1P/p1758771319935249)\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56229\nTitle: Add Greek UI translation\nState: open\nAuthor: PApostol\nLabels: area:dev-tools, area:UI, backport-to-v3-1-test, area:translations\nBody:\nFollowing the addition of several translations, I thought of adding Greek to Airflow UI.\r\n\r\n<img width=\"1230\" height=\"859\" alt=\"ui\" src=\"https://github.com/user-attachments/assets/30ab39b2-5160-4e05-bf4a-62408cff7ea4\" />\r\n\r\n<img width=\"1127\" height=\"644\" alt=\"tests\" src=\"https://github.com/user-attachments/assets/2a817295-ffd3-4d84-aeee-331226fdb8f7\" />",
  "Requirement ID: ISSUE-56228\nTitle: Reapply \"update AzureBaseHook to return credentials that supports get_token method\"\nState: closed\nAuthor: karunpoudel\nLabels: provider:microsoft-azure, area:providers, kind:documentation, full tests needed, all versions\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\nReapply #52182\r\n\r\nDatabases like Snowflake and Azure Postgres Flex support connection using Azure OAUTH token. Newer Azure Identity credential objects like `ClientSecretCredential` and `DefaultAzureCredential` support `get_token` method which can be used to generate OAUTH token. SnowflakeHook and PostgresHook can leverage AzureBaseHook to retrieve required token.\r\n\r\nIn this PR:\r\n- Expose the credential object directly using `get_credential` method and make `sdk_client` argument optional so that the credential object can be retrieved without needing sdk_client. eg. `AzureBaseHook().get_credential()`\r\n- Added new extra `use_azure_identity_object` in connection for backward compatibility. When set to true, `get_credential` method will return newer credential object of type `ClientSecretCredential` or `DefaultAzureCredential` instead of current `ServicePrincipalCredentials` or `AzureIdentityCredentialAdapter`. Current credential object doesn't support `get_token` method. eg. `AzureBaseHook().get_credential().get_token()`\r\n\r\nI am ok with not making `sdk_client` argument optional if there is any concern.\r\n\r\nUsage:\r\n\r\nCreate base azure connection using either client_secret or default azure credential.\r\n\r\n```.env\r\n# Using client_secret\r\nAIRFLOW_CONN_AZURE_SP=azure://client_id:client_secret@/?tenantId=xyz&use_azure_identity_object=true\r\n\r\n# Using default azure credentials\r\nAIRFLOW_CONN_AZURE_SP=azure:///?use_azure_identity_object=true\r\n```\r\n\r\nUse  above connection to retrieve token to connect to specific resource (e.g., azure postgres flex server). \r\n\r\n```python\r\nfrom airflow.models import Connection\r\nconn = Connection.get_connection_from_secrets(\"azure_sp\")\r\nazure_base_hook= conn.get_hook()\r\nscope = \"https://ossrdbms-aad.database.windows.net/.default\"\r\npassword = azure_base_hook.get_token(scope).token\r\n```\r\n\r\n[PostgresHook](https://airflow.apache.org/docs/apache-airflow-providers-postgres/stable/_api/airflow/providers/postgres/hooks/postgres/index.html#airflow.providers.postgres.hooks.postgres.PostgresHook) could be updated to retrieve token using azure connection similar to how it currently uses aws connection.\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56226\nTitle: Add auto refresh to backfill banner\nState: open\nAuthor: omareltomy\nLabels: area:UI, backport-to-v3-1-test\nBody:\nThe backfill banner was not auto-refreshing when backfills completed, requiring manual refresh (F5) to remove the banner. This change adds the same auto-refresh pattern used by other components.\r\n\r\nImport useAutoRefresh from src/utils\r\nAdd refetchInterval to useBackfillServiceListBackfillsUi query\r\nOnly refresh when there are active backfills (completed_at === null)\r\nThis ensures the banner automatically disappears when all backfills are completed, providing consistent UX with other UI components.\r\n\r\nWhat changes were proposed in this pull request?\r\nThis PR adds auto-refresh functionality to the backfill banner component to ensure it automatically disappears when backfills complete, providing consistent user experience with other UI components in Airflow.\r\n\r\nChanges made:\r\n\r\nImport useAutoRefresh utility from src/utils\r\nAdd refetchInterval configuration to the useBackfillServiceListBackfillsUi query\r\nImplement conditional refresh logic that only triggers when there are active backfills (completed_at === null)\r\nWhy are the changes needed?\r\nThe backfill banner was not automatically updating its state when backfills completed, causing it to remain visible even after all backfill operations finished. Users had to manually refresh (F5) or navigate away and back to make the banner disappear.\r\n\r\nIssue reproduction:\r\n\r\nStart a backfill on any DAG - banner appears correctly\r\nWait for backfill to complete\r\nNavigate around the UI - banner remains visible\r\nManual refresh (F5) was required to remove the completed banner\r\nThis created an inconsistent user experience compared to other auto-refreshing components in the Airflow UI.\r\n\r\nDoes this PR introduce any user-facing change?\r\nYes - The backfill banner will now automatically disappear when all backfills complete, without requiring manual page refresh. The banner will refresh at the configured interval (auto_refresh_interval setting) only when there are active backfills running.\r\n\r\nHow was this patch tested?\r\nCode pattern validation: The implementation follows the exact same pattern used by other auto-refreshing components in the Airflow UI (DAGs list, HITL components, etc.)\r\nLogic verification: The conditional refresh logic (completed_at === null) ensures the banner only refreshes when there are actually active backfills to avoid unnecessary API calls\r\nTypeScript compliance: Proper typing added for query parameters to maintain code quality\r\nTesting approach:\r\nThe fix leverages the existing useAutoRefresh utility which is already proven to work across the Airflow UI. The implementation is identical to other successful auto-refresh implementations, ensuring reliability and consistency.\r\n\r\n^ Add meaningful description above\r\nRead the [Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines) for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named {pr_number}.significant.rst or {issue_number}.significant.rst, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).\r\n\r\n[Closes #56080](https://github.com/apache/airflow/issues/56080)",
  "Requirement ID: ISSUE-56223\nTitle: Revert \"update AzureBaseHook to return credentials that supports get_token method\"\nState: closed\nAuthor: jscheffl\nLabels: provider:microsoft-azure, area:providers, kind:documentation\nBody:\nSorry, this PR broke our canary test in https://github.com/apache/airflow/actions/runs/18108381988 after merged to main.\r\n\r\nI suspect that this is caused by the last commit was Aug 13th and after this no rebase was made prior merge.\r\nAs I do not understand the last failing test and the check is non obviously fixable by me, proposal is to revert and re-apply.\r\n\r\nReverts apache/airflow#52182\r\n\r\nFYI @karunpoudel / @potiuk",
  "Requirement ID: ISSUE-56222\nTitle: Fix upgrade checks with prek\nState: closed\nAuthor: jscheffl\nLabels: area:dev-tools, backport-to-v3-1-test\nBody:\nFix upgrade checks in https://github.com/apache/airflow/actions/runs/18108381988/job/51528661227",
  "Requirement ID: ISSUE-56220\nTitle: Dag run_id disappears on toggling runs in ui\nState: closed\nAuthor: NilsJPWerner\nLabels: kind:bug, area:UI, affected_version:3.1\nBody:\n### Apache Airflow version\n\n3.1.0\n\n### If \"Other Airflow 2 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\nOn selecting a dag run the dag run_id is briefly shown before getting replaced with the data interval start date. Feels like they should both be kept? Was it intentional to hide run_id in the new ui?\n\nhttps://github.com/user-attachments/assets/a863d0e1-f1b4-49c5-845f-0eff6279e9c8\n\n### What you think should happen instead?\n\n_No response_\n\n### How to reproduce\n\nClick on a different dag run.\n\n### Operating System\n\nDebian\n\n### Versions of Apache Airflow Providers\n\n_No response_\n\n### Deployment\n\nAstronomer\n\n### Deployment details\n\n_No response_\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56219\nTitle: Dags list is not loading in the UI, espically when filtering with tags\nState: open\nAuthor: dor-bernstein\nLabels: kind:bug, area:API, area:UI, affected_version:3.1\nBody:\n### Apache Airflow version\n\n3.1.0\n\n### If \"Other Airflow 2 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\nWhen clicking on the dags in the UI I get the first page of my dags as expected most of the time, sometimes it gets a timeout.\nHowever when trying to filter on a specific tag the request gets timeout\n\n\n### What you think should happen instead?\n\nthe dags should be loaded\n\n### How to reproduce\n\nNot sure, I suspect that it is because we have relatively a lot of entries in the serialized_dag table\n\n### Operating System\n\noffical airflow docker image\n\n### Versions of Apache Airflow Providers\n\n_No response_\n\n### Deployment\n\nOfficial Apache Airflow Helm Chart\n\n### Deployment details\n\n_No response_\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56217\nTitle: Fix: Make SFTPOperator delete idempotent (return a warning instead of a error when file is already absent)\nState: open\nAuthor: MadScientistBR\nLabels: area:providers, provider:sftp\nBody:\nThe SFTPOperator's DELETE operation no longer errors out if the target file or directory is already absent. Instead, it logs a warning.\r\n\r\nThis makes the operation idempotent and prevents unnecessary task failures.\r\n\r\nCloses: #56190\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56216\nTitle: Fix log text selection contrast in light mode\nState: closed\nAuthor: dheerajturaga\nLabels: type:bug-fix, area:UI\nBody:\nRemove CSS prop to improve readability when highlighting text in the log viewer.\r\nThis aligns with the DAG code viewer styling and provides better\r\ncontrast across both light and dark themes.\r\n  \r\nBefore: (yes the lines are highlighted but you cant see it!!)\r\n\r\n<img width=\"1918\" height=\"871\" alt=\"image\" src=\"https://github.com/user-attachments/assets/6f44fe9c-abfb-4266-a65a-0488f9c5b6a5\" />\r\n\r\nAfter: \r\n<img width=\"1917\" height=\"1032\" alt=\"image\" src=\"https://github.com/user-attachments/assets/04605c95-6fab-4ad6-8e7e-e2c7ec1cdef4\" />\r\n\r\n<img width=\"1917\" height=\"925\" alt=\"image\" src=\"https://github.com/user-attachments/assets/6368e8ff-d61a-45ef-95da-ca9eb945151a\" />",
  "Requirement ID: ISSUE-56215\nTitle: Custom registered serializers stopped working due to type changes\nState: open\nAuthor: FrankPortman\nLabels: kind:bug, area:serialization, area:core, needs-triage, area:task-sdk, affected_version:3.1\nBody:\n### Apache Airflow version\n\n3.1.0\n\n### If \"Other Airflow 2 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\nThe way classname gets resolved has changed, breaking custom `deserialize` calls.\n\nIsolated the issue to these PRs:\n\n* https://github.com/apache/airflow/pull/52360/files#diff-5d7a43b9860e3c6d80d8feadce5ae76a3a56698fd8dff389fb6095c01153e74eR267\n* https://github.com/apache/airflow/pull/51059\n\nWhich mentions `pydantic` only but has a `classname` --> `cls` change for custom deserializers bundled in the commentary. The `pydantic` specific naming of the PRs made this difficult to debug when looking at git history.\n\nThe documentation also still shows comparing `classname` to `qualname(...)`, as well as `str` type hints for `classname`. This also made it difficult to debug when poring over changes for 3.1.0.\n\n### What you think should happen instead?\n\nRelease notes and documentation (including this example, which is basically the only example of registering serializers: https://airflow.apache.org/docs/apache-airflow/stable/authoring-and-scheduling/serializers.html) should be updated if behavior changes, and this should have been listed as a significant/breaking change in 3.1.0 release notes.\n\n\n\n### How to reproduce\n\nUse registered custom serializers and follow the official documentation for the `deserialize` function.\n\n### Operating System\n\nAny\n\n### Versions of Apache Airflow Providers\n\n_No response_\n\n### Deployment\n\nAstronomer\n\n### Deployment details\n\n_No response_\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56214\nTitle: Fix fastapi new version issue\nState: closed\nAuthor: pierrejeambrun\nLabels: area:API, backport-to-v3-1-test\nBody:\nI would like to fix the 'hard coded' string there. Some more small change might come, lets see if CI is happy.",
  "Requirement ID: ISSUE-56213\nTitle: [v3-1-test] Add react-router-dom to external deps in plugins (#56205)\nState: closed\nAuthor: github-actions[bot]\nLabels: area:dev-tools, area:UI, backport-to-v3-1-test\nBody:\n(cherry picked from commit 54453b34af953e3c474e3d9d9167e1ffe31e8e9f)\n\nCo-authored-by: Pierre Jeambrun <pierrejbrun@gmail.com>",
  "Requirement ID: ISSUE-56212\nTitle: Remove SQLA 1 limit in Fab provider\nState: open\nAuthor: vincbeck\nLabels: full tests needed\nBody:\nSee what breaks.\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56211\nTitle: Fix install_java.sh script to correctly clear /files/bin folder before reinstalling java \nState: closed\nAuthor: VladaZakharova\nLabels: provider:google, area:providers, area:dev-tools, provider:apache-beam, backport-to-v3-1-test\nBody:\nThis PR fixes install_java.sh script to use newer version of Java. \r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56210\nTitle: Bump `google-cloud-aiplatform[evaluation]>=1.117.0`\nState: open\nAuthor: eladkal\nLabels: provider:google, area:providers\nBody:\nThe referenced issue has been fixed so lets try to see if the underlying issue is resolved.\r\ncc @VladaZakharova @Crowiant \r\n\r\nrelated https://github.com/apache/airflow/pull/53182",
  "Requirement ID: ISSUE-56209\nTitle: Remove configuration embedded note from providers\nState: closed\nAuthor: eladkal\nLabels: changelog:skip\nBody:\nThis change removed the configuration embedded block from the Configuration menu tab in providers:\r\n<img width=\"1549\" height=\"928\" alt=\"Screenshot 2025-09-29 at 15 28 24\" src=\"https://github.com/user-attachments/assets/10abf7ab-c68a-41d9-96f4-0c65175c57ee\" />\r\n\r\n\r\nthe minimum version of Airflow for providers is 2.10 - There is no longer need to note about something that is relevant for 2.7+",
  "Requirement ID: ISSUE-56208\nTitle: Bump version of providers\nState: closed\nAuthor: eladkal\nLabels: backport-to-v3-1-test\nBody:\nSome Airflow 3.1 compatibility issues were addressed in recent provider releases. Lets bump min version to help users avoid hitting problems that are solved by updating providers.",
  "Requirement ID: ISSUE-56207\nTitle: DAG triggered via TriggerDagRunOperator stuck in queued state\nState: closed\nAuthor: MarcusCramer91\nLabels: kind:bug, pending-response, area:core, area:Triggerer, affected_version:3.1\nBody:\n### Apache Airflow version\n\n3.1.0\n\n### If \"Other Airflow 2 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\nWhen launching a child DAG via TriggerDagRunOperator, the DAG is stuck in \"queued\" state why all tasks show up with status \"No Status\" in the UI. Can't seem to find any logs related to the child DAG in the scheduler logs, as if it doesn't notice it's there.\n\n### What you think should happen instead?\n\nChild DAG execution should work as normal.\n\n### How to reproduce\n\nchild.py\n```\nfrom airflow.providers.standard.operators.empty import EmptyOperator\nfrom airflow.sdk import dag\n\n\n@dag(dag_id=\"test-orchestration-child\")\ndef get_dag() -> None:\n    EmptyOperator(task_id=\"Empty\")\n\n\nget_dag()\n```\n\nparent.py\n```\nfrom airflow.providers.standard.operators.trigger_dagrun import TriggerDagRunOperator\nfrom airflow.sdk import dag\n\n\n@dag(dag_id=\"test-orchestration-parent\")\ndef get_dag() -> None:\n    TriggerDagRunOperator(\n        task_id=\"trigger-test-orchestration-child\",\n        trigger_dag_id=\"test-orchestration-child\",\n        wait_for_completion=True,\n    )\n\n\nget_dag()\n```\n\n### Operating System\n\nDebian GNU/Linux 12 (bookworm)\n\n### Versions of Apache Airflow Providers\n\napache-airflow-providers-standard==1.8.0\n\n### Deployment\n\nOfficial Apache Airflow Helm Chart\n\n### Deployment details\n\n_No response_\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56206\nTitle: Fix corrupted bare Git repository recovery in DAG bundles\nState: open\nAuthor: dheerajturaga\nLabels: area:providers, provider:git\nBody:\nWhen using git DAG bundles, corrupted bare repositories can cause all tasks\r\n  landing on a host to fail with InvalidGitRepositoryError. This adds retry\r\n  logic that detects corrupted bare repositories, cleans them up, and attempts\r\n  to re-clone them once before failing.\r\n\r\n  Changes:\r\n  - Add InvalidGitRepositoryError handling in _clone_bare_repo_if_required()\r\n  - Implement cleanup and retry logic with shutil.rmtree()\r\n  - Add comprehensive tests for both successful retry and retry failure scenarios\r\n  - Ensure all existing tests continue to pass",
  "Requirement ID: ISSUE-56205\nTitle: Add react-router-dom to external deps in plugins\nState: closed\nAuthor: pierrejeambrun\nLabels: area:dev-tools, backport-to-v3-1-test\nBody:",
  "Requirement ID: ISSUE-56204\nTitle: TriggerDagRunOperator.execute cannot be called outside of the Task Runner!\nState: open\nAuthor: mweisshaupt1988\nLabels: kind:bug, area:core, affected_version:main_branch, affected_version:3.0\nBody:\n### Apache Airflow version\n\nOther Airflow 2 version (please specify below)\n\n### If \"Other Airflow 2 version\" selected, which one?\n\n3.0.6\n\n### What happened?\n\nThe TriggerDagRunOperator can not be executed from a task anymore\n\n### What you think should happen instead?\n\nEither it should work like in Airflow 2.11 or there should be some documentation how to migrate existing DAGs\n\n### How to reproduce\n\nI have added an example with two minimal Dags that reproduce the problem\n[x_task_run_demo.py](https://github.com/user-attachments/files/22595855/x_task_run_demo.py)\n\n### Operating System\n\nUbuntu 24.04\n\n### Versions of Apache Airflow Providers\n\n_No response_\n\n### Deployment\n\nDocker-Compose\n\n### Deployment details\n\nIt does not matter if I run it in docker compose or on OpenShift. The error is the same.\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56203\nTitle: [v3-1-test] Cleanup disk space in image cache push jobs (#56198)\nState: closed\nAuthor: github-actions[bot]\nLabels: area:dev-tools, backport-to-v3-1-test\nBody:\n(cherry picked from commit 1024b65a86a448f11c241c0d8be26a57d0be2c50)\n\nCo-authored-by: Amogh Desai <amoghrajesh1999@gmail.com>",
  "Requirement ID: ISSUE-56202\nTitle: Fix scheduler crash during 3.0 to 3.1 migration when retry_delay is None\nState: closed\nAuthor: dheerajturaga\nLabels: backport-to-v3-1-test, affected_version:3.1\nBody:\nDuring migration from Airflow 3.0 to 3.1, some task instances may have\r\n  retry_delay set to None due to serialization/deserialization changes or\r\n  database migration issues. This causes the scheduler to crash with:\r\n\r\n  TypeError: unsupported operand type(s) for +: 'datetime.datetime' and 'NoneType'\r\n\r\n  The error occurs in TaskInstance.next_retry_datetime() when attempting to\r\n  add a None retry_delay to end_date.\r\n\r\n  Related: Airflow 3.0 to 3.1 migration compatibility\r\n\r\n```\r\n  File \"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py\", line 991, in next_retry_datetime\r\n    return self.end_date + delay\r\n           ~~~~~~~~~~~~~~^~~~~~~  \r\nTypeError: unsupported operand type(s) for +: 'datetime.datetime' and 'NoneType'           \r\n```",
  "Requirement ID: ISSUE-56200\nTitle: move disk cleanup to before image build in publish-docs\nState: closed\nAuthor: gopidesupavan\nLabels: area:dev-tools, backport-to-v3-1-test\nBody:\nThis is clearing ./local folder aswell, causing this error https://github.com/apache/airflow/actions/runs/18088075206/job/51476190888#step:18:21\r\n\r\nbut somehow few builds worked without any issues. https://github.com/apache/airflow/actions/runs/18072745669/job/51424945747\r\n\r\nAnyway we can move to after checkout.\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56199\nTitle: Airflow 3.1 dag procesor typo in function name\nState: closed\nAuthor: fenix7\nLabels: kind:bug, area:core, needs-triage, affected_version:3.1\nBody:\n### Apache Airflow version\n\n3.1.0\n\n### If \"Other Airflow 2 version\" selected, which one?\n\n_No response_\n\n### What happened?\n```\nag-processor | 2025-09-29T09:31:51.190294Z [info     ] Setting next_dagrun for example_weekday_branch_operator to 2025-09-29 00:00:00+00:00, run_after=2025-09-29 00:00:00+00:00 [airflow.models.dag] loc=dag.py:688\ndag-processor | 2025-09-29T09:31:51.214129Z [error    ] Exception when executing DagProcessorJob [airflow.jobs.dag_processor_job_runner.DagProcessorJobRunner] loc=dag_processor_job_runner.py:63\ndag-processor | Traceback (most recent call last):\ndag-processor | File \"/home/mkutwin/.venv/lib/python3.12/site-packages/airflow/jobs/dag_processor_job_runner.py\", line 61, in _execute\ndag-processor | self.processor.run()\ndag-processor | File \"/home/mkutwin/.venv/lib/python3.12/site-packages/airflow/dag_processing/manager.py\", line 272, in run\ndag-processor | return self._run_parsing_loop()\ndag-processor | ^^^^^^^^^^^^^^^^^^^^^^^^\ndag-processor | File \"/home/mkutwin/.venv/lib/python3.12/site-packages/airflow/dag_processing/manager.py\", line 357, in _run_parsing_loop\ndag-processor | self._start_new_processes()\ndag-processor | File \"/home/mkutwin/.venv/lib/python3.12/site-packages/airflow/dag_processing/manager.py\", line 924, in _start_new_processes\ndag-processor | processor = self._create_process(file)\ndag-processor | ^^^^^^^^^^^^^^^^^^^^^^^^^^\ndag-processor | File \"/home/mkutwin/.venv/lib/python3.12/site-packages/airflow/dag_processing/manager.py\", line 905, in _create_process\ndag-processor | return DagFileProcessorProcess.start(\ndag-processor | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ndag-processor | File \"/home/mkutwin/.venv/lib/python3.12/site-packages/airflow/dag_processing/processor.py\", line 459, in start\ndag-processor | _pre_import_airflow_modules(os.fspath(path), logger)\ndag-processor | File \"/home/mkutwin/.venv/lib/python3.12/site-packages/airflow/dag_processing/processor.py\", line 158, in _pre_import_airflow_modules\ndag-processor | importlib.import_module(module)\ndag-processor | File \"/usr/lib/python3.12/importlib/__init__.py\", line 90, in import_module\ndag-processor | return _bootstrap._gcd_import(name[level:], package, level)\ndag-processor | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ndag-processor | File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\ndag-processor | File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\ndag-processor | File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\ndag-processor | File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n\n(.venv) mkutwin@cas-ai:~$ dag-processor | File \"<frozen importlib._bootstrap_external>\", line 995, in exec_module\ndag-processor | File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\ndag-processor | File \"/home/mkutwin/.venv/lib/python3.12/site-packages/airflow/providers/standard/operators/hitl.py\", line 33, in <module>\ndag-processor | from airflow.providers.standard.triggers.hitl import HITLTrigger, HITLTriggerEventSuccessPayload\ndag-processor | File \"/home/mkutwin/.venv/lib/python3.12/site-packages/airflow/providers/standard/triggers/hitl.py\", line 33, in <module>\ndag-processor | from airflow.sdk.execution_time.hitl import (\ndag-processor | ImportError: cannot import name 'update_htil_detail_response' from 'airflow.sdk.execution_time.hitl' (/home/mkutwin/.venv/lib/python3.12/site-packages/airflow/sdk/execution_time/hitl.py)\ndag-processor | 2025-09-29T09:31:51.226315Z [info     ] Process exited                 [supervisor] exit_code=<Negsignal.SIGTERM: -15> loc=supervisor.py:709 pid=6868 signal_sent=SIGTERM\ndag-processor | 2025-09-29T09:31:51.234770Z [info     ] Waiting up to 5 seconds for processes to exit... [airflow.utils.process_utils] loc=process_utils.py:285\ndag-processor | Traceback (most recent call last):\ndag-processor | File \"/home/mkutwin/.venv/bin/airflow\", line 8, in <module>\ndag-processor | sys.exit(main())\ndag-processor | ^^^^^^\ndag-processor | File \"/home/mkutwin/.venv/lib/python3.12/site-packages/airflow/__main__.py\", line 55, in main\ndag-processor | args.func(args)\ndag-processor | File \"/home/mkutwin/.venv/lib/python3.12/site-packages/airflow/cli/cli_config.py\", line 49, in command\ndag-processor | return func(*args, **kwargs)\ndag-processor | ^^^^^^^^^^^^^^^^^^^^^\ndag-processor | File \"/home/mkutwin/.venv/lib/python3.12/site-packages/airflow/utils/cli.py\", line 114, in wrapper\ndag-processor | return f(*args, **kwargs)\ndag-processor | ^^^^^^^^^^^^^^^^^^\ndag-processor | File \"/home/mkutwin/.venv/lib/python3.12/site-packages/airflow/utils/providers_configuration_loader.py\", line 54, in wrapped_function\ndag-processor | return func(*args, **kwargs)\ndag-processor | ^^^^^^^^^^^^^^^^^^^^^\ndag-processor | File \"/home/mkutwin/.venv/lib/python3.12/site-packages/airflow/cli/commands/dag_processor_command.py\", line 53, in dag_processor\ndag-processor | run_command_with_daemon_option(\ndag-processor | File \"/home/mkutwin/.venv/lib/python3.12/site-packages/airflow/cli/commands/daemon_utils.py\", line 86, in run_command_with_daemon_option\ndag-processor | callback()\ndag-processor | File \"/home/mkutwin/.venv/lib/python3.12/site-packages/airflow/cli/commands/dag_processor_command.py\", line 56, in <lambda>\ndag-processor | callback=lambda: run_job(job=job_runner.job, execute_callable=job_runner._execute),\ndag-processor | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ndag-processor | File \"/home/mkutwin/.venv/lib/python3.12/site-packages/airflow/utils/session.py\", line 100, in wrapper\ndag-processor | return func(*args, session=session, **kwargs)\ndag-processor | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ndag-processor | File \"/home/mkutwin/.venv/lib/python3.12/site-packages/airflow/jobs/job.py\", line 368, in run_job\ndag-processor | return execute_job(job, execute_callable=execute_callable)\ndag-processor | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ndag-processor | File \"/home/mkutwin/.venv/lib/python3.12/site-packages/airflow/jobs/job.py\", line 397, in execute_job\ndag-processor | ret = execute_callable()\ndag-processor | ^^^^^^^^^^^^^^^^^^\ndag-processor | File \"/home/mkutwin/.venv/lib/python3.12/site-packages/airflow/jobs/dag_processor_job_runner.py\", line 61, in _execute\ndag-processor | self.processor.run()\ndag-processor | File \"/home/mkutwin/.venv/lib/python3.12/site-packages/airflow/dag_processing/manager.py\", line 272, in run\ndag-processor | return self._run_parsing_loop()\ndag-processor | ^^^^^^^^^^^^^^^^^^^^^^^^\ndag-processor | File \"/home/mkutwin/.venv/lib/python3.12/site-packages/airflow/dag_processing/manager.py\", line 357, in _run_parsing_loop\ndag-processor | self._start_new_processes()\ndag-processor | File \"/home/mkutwin/.venv/lib/python3.12/site-packages/airflow/dag_processing/manager.py\", line 924, in _start_new_processes\ndag-processor | processor = self._create_process(file)\ndag-processor | ^^^^^^^^^^^^^^^^^^^^^^^^^^\ndag-processor | File \"/home/mkutwin/.venv/lib/python3.12/site-packages/airflow/dag_processing/manager.py\", line 905, in _create_process\ndag-processor | return DagFileProcessorProcess.start(\ndag-processor | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ndag-processor | File \"/home/mkutwin/.venv/lib/python3.12/site-packages/airflow/dag_processing/processor.py\", line 459, in start\ndag-processor | _pre_import_airflow_modules(os.fspath(path), logger)\ndag-processor | File \"/home/mkutwin/.venv/lib/python3.12/site-packages/airflow/dag_processing/processor.py\", line 158, in _pre_import_airflow_modules\ndag-processor | importlib.import_module(module)\ndag-processor | File \"/usr/lib/python3.12/importlib/__init__.py\", line 90, in import_module\ndag-processor | return _bootstrap._gcd_import(name[level:], package, level)\ndag-processor | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ndag-processor | File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\ndag-processor | File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\ndag-processor | File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\ndag-processor | File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\ndag-processor | File \"<frozen importlib._bootstrap_external>\", line 995, in exec_module\ndag-processor | File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\ndag-processor | File \"/home/mkutwin/.venv/lib/python3.12/site-packages/airflow/providers/standard/operators/hitl.py\", line 33, in <module>\ndag-processor | from airflow.providers.standard.triggers.hitl import HITLTrigger, HITLTriggerEventSuccessPayload\ndag-processor | File \"/home/mkutwin/.venv/lib/python3.12/site-packages/airflow/providers/standard/triggers/hitl.py\", line 33, in <module>\ndag-processor | from airflow.sdk.execution_time.hitl import (\ndag-processor | ImportError: cannot import name 'update_htil_detail_response' from 'airflow.sdk.execution_time.hitl' (/home/mkutwin/.venv/lib/python3.12/site-packages/airflow/sdk/execution_time/hitl.py). Did you mean: 'update_hitl_detail_response'?\n```\n### What you think should happen instead?\n\n_No response_\n\n### How to reproduce\n\nupgrade airflow\n\n### Operating System\n\nubuntu 24.02\n\n### Versions of Apache Airflow Providers\n\n_No response_\n\n### Deployment\n\nOfficial Apache Airflow Helm Chart\n\n### Deployment details\n\n_No response_\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56198\nTitle: Cleanup disk space in image cache push jobs\nState: closed\nAuthor: amoghrajesh\nLabels: area:dev-tools, backport-to-v3-1-test\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\nIn some recent jobs, we saw some issues such related to disk space: https://github.com/apache/airflow/actions/runs/18071055239/job/51422086979\r\n\r\n\r\n```\r\n  #59 loading layer 2f7fcd295b18 61.07MB / 61.07MB 40.3s done\r\n  #59 ERROR: write /usr/python/lib/python3.10/site-packages/scipy/linalg/cython_lapack.cpython-310-x86_64-linux-gnu.so: no space left on device\r\n  \r\n  #56 exporting to docker image format\r\n  #56 sending tarball 74.3s done\r\n  #56 ERROR: rpc error: code = Unknown desc = write /usr/python/lib/python3.10/site-packages/scipy/linalg/cython_lapack.cpython-310-x86_64-linux-gnu.so: no space left on device\r\n  ------\r\n   > exporting to docker image format:\r\n  ------\r\n  ------\r\n   > importing to docker:\r\n  ------\r\n  ERROR: failed to build: failed to solve: rpc error: code = Unknown desc = write /usr/python/lib/python3.10/site-packages/scipy/linalg/cython_lapack.cpython-310-x86_64-linux-gnu.so: no space left on device\r\nYour image build failed. It could be caused by conflicting dependencies.\r\n```\r\n\r\nAs a potential improvement, we could reuse of disk cleanup job in these CI workflows too, right after checkout to clean some clutter up.\r\n\r\nI do not see side effects because the cleanup step:\r\n* Removes large unused toolchains (.NET, GraalVM, Android SDK, etc.)\r\n* Cleans package caches (APT, pip, npm)\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56197\nTitle: Apply vote/accelerated vote to vote template based on vote duration in release guide\nState: closed\nAuthor: eladkal\nLabels: area:dev-tools\nBody:",
  "Requirement ID: ISSUE-56196\nTitle: `on_failure_callback` executed when task with retries is killed externally\nState: closed\nAuthor: wolfier\nLabels: kind:bug, area:core, affected_version:3.1\nBody:\n### Apache Airflow version\n\n3.1.0\n\n### If \"Other Airflow 2 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\n`on_failure_callback` of a task with remaining retry attempts executed when the worker executing the task terminated abruptly. This is not expected because the task has retry attempts.\n\n### What you think should happen instead?\n\nWhen a task with remaining retries fails, the `on_retry_callback` should be executed and **NOT** `on_failure_callback`.\n\n### How to reproduce\n\n1. Execute a long running task\n2. Terminate the worker \n3. Allow the scheduler to identify the task instance was killed externally\n4. Examine dag processor logs for execution of the task's`on_failure_callback`\n\n### Operating System\n\nDebian\n\n### Versions of Apache Airflow Providers\n\n_No response_\n\n### Deployment\n\nAstronomer\n\n### Deployment details\n\n_No response_\n\n### Anything else?\n\nWhen a task with either `on_failure_callback` or `on_retry_callback` is killed externally, a [TaskCallbackRequest](https://github.com/apache/airflow/blob/3.1.0/airflow-core/src/airflow/jobs/scheduler_job_runner.py#L930C31-L942) is sent. The request does not have `task_callback_type` set and defaults to `None`. The callback request is considered a [failure callback](https://github.com/apache/airflow/blob/3.1.0/airflow-core/src/airflow/callbacks/callback_requests.py#L69-L72) when `task_callback_type` is `None`. \n\nSince the `task_callback_type` is `None`, the task's `on_failure_callback` is queued for execution when the dag processor processes the callback request.\n\n```python\n    if request.task_callback_type is TaskInstanceState.UP_FOR_RETRY:\n        callbacks = task.on_retry_callback\n    else:\n        callbacks = task.on_failure_callback\n```\n\nThe `TaskCallbackRequest` should set `task_callback_type` according to whether the task is eligible to retry. If it is, the task should set `task_callback_type` to `TaskInstanceState.UP_FOR_RETRY` otherwise `TaskInstanceState.FAILED` is an appropriate mode.\n\n```python\n    request = TaskCallbackRequest(\n        filepath=ti.dag_model.relative_fileloc,\n        bundle_name=ti.dag_version.bundle_name,\n        bundle_version=ti.dag_version.bundle_version,\n        ti=ti,\n        msg=msg,\n        context_from_server=TIRunContext(\n            dag_run=DRDataModel.model_validate(ti.dag_run, from_attributes=True),\n            max_tries=ti.max_tries,\n            variables=[],\n            connections=[],\n            xcom_keys_to_clear=[],\n        ),\n        task_callback_type=(\n            TaskInstanceState.UP_FOR_RETRY\n            if ti.is_eligible_to_retry() else\n            TaskInstanceState.FAILED\n        ),\n    )\n```\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56195\nTitle: Add CLI command to remove all queues from Celery worker\nState: closed\nAuthor: dheerajturaga\nLabels: area:providers, provider:celery\nBody:\nImplement  command that unsubscribes a Celery worker from all its active queues. This complements the existing command by providing a bulk operation for queue management. Idea is to quickly move a worker out from service without actually shutting it down. Intent is to create a similar effect as Edge Executor's `Maintenance Mode`\r\n\r\n<img width=\"1210\" height=\"178\" alt=\"image\" src=\"https://github.com/user-attachments/assets/0d4b7a24-2070-4c9a-b46b-e325d8b85067\" />\r\n\r\n\r\ncc: @jscheffl",
  "Requirement ID: ISSUE-56194\nTitle: Using Gitdagbundle with git connection in the UI doesn't sync dags anymore. Reserialization throws a 'host url is missing in the git connection' error\nState: closed\nAuthor: dineshmarimu2\nLabels: kind:bug, area:providers, area:core, provider:cncf-kubernetes, needs-triage, affected_version:3.1\nBody:\n### Apache Airflow version\n\n3.1.0\n\n### If \"Other Airflow 2 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\nI migrated from Airflow 3.0.4 to 3.1 with Python 3.12. \n\nI use a Helm 1.18.0 for deployment to AKS in Azure cloud.\n\nI use GitDagBundle for syncing my DAGs with our Git repository. This was working well in Airflow 3.x versions till 3.0.4 and I could verify that this works even in 3.0.6.\nAfter upgrading my Helm deployment to Airflow 3.1.0, my dags no longer sync with Git and the last sync had the timestamp before upgrading to 3.1.0.\nReparsing DAG on the UI had no effect.\nExecuting `airflow dags reserialize` in shell of dag-processor resulted in a error which stated that 'host url for the git_connection was missing'.\nI tried deleting and recreating the connection on the UI and the error stayed.\nFinally downgraded db to 3.0.6 and deployed 3.0.6 and everything worked well as before\n\n### What you think should happen instead?\n\nAirflow 3.1.0's git sync with GitDagBundle should work normally without any changes to my dag_bundle_config_list setting in values.yaml\n\n### How to reproduce\n\nDeploy Airflow with Helm 1.18.0 in AKS with the following config for GitDagBUndle in values.yaml\n\n```\nconfig:\n  dag_processor:\n    dag_bundle_config_list: '[{\"name\": \"skynet_dags\",\"classpath\": \"airflow.providers.git.bundles.git.GitDagBundle\",\"kwargs\": { \"git_conn_id\": \"git_dags_conn\", \"subdir\": \"dags\", \"tracking_ref\": \"development\",\"refresh_interval\": 30}}]'\n```\nCreate a git connection in the UI after deployment with the name git_dags_conn.\nVerify if changes to dags are synced in the UI's code tab\n\n### Operating System\n\nUbuntu linux\n\n### Versions of Apache Airflow Providers\n\nThis is my image\n```\nFROM apache/airflow:3.1.0-python3.12\nRUN pip install --no-cache-dir apache-airflow-providers-cncf-kubernetes==10.5.0 \\\n    && pip install --no-cache-dir pandas==2.1.4 \\\n    && pip install --no-cache-dir openpyxl==3.1.5\n```\nI use an older version of pandas as a particular feature I need is buggy in the newer versions\n\n### Deployment\n\nOfficial Apache Airflow Helm Chart\n\n### Deployment details\n\nI use the Airflow official Helm chart with suitable modifications for our deplyoment\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56193\nTitle: Add triggering_user_name to DagRunProtocol interface\nState: closed\nAuthor: dheerajturaga\nLabels: area:API, area:task-sdk, backport-to-v3-1-test\nBody:\nEnable tasks running in isolated environments to access the username\r\n  that triggered a DAG run without requiring direct database access.\r\n\r\n  Changes:\r\n  - Add triggering_user_name field to DagRunProtocol interface\r\n  - Update task-sdk DagRun data model to include triggering_user_name\r\n  - Update execution API DagRun model to include triggering_user_name\r\n  - Ensure field flows from database through API to task context\r\n\r\n  This allows developers to identify DAG run triggers from within task\r\n  code using the existing DagRunProtocol interface.\r\n\r\n```python\r\ndef find_owner(dag_run: DagRunProtocol) -> str:\r\n    \"\"\"\r\n    Extract the owner information from the DAG run.\r\n\r\n    Args:\r\n        dag_run: The DAG run protocol object containing run information\r\n\r\n    Returns:\r\n        str: Information about who triggered the DAG run\r\n    \"\"\"\r\n    triggering_user = dag_run.triggering_user_name\r\n\r\n    if triggering_user:\r\n        return f\"DAG was triggered by user: {triggering_user}\"\r\n    else:\r\n        return \"DAG was triggered automatically (no user specified)\"\r\n```",
  "Requirement ID: ISSUE-56192\nTitle: ForeignKeyViolation error when doing `airflow db clean`\nState: open\nAuthor: hket-ianchu\nLabels: kind:bug, area:MetaDB, area:core, needs-triage, affected_version:3.0, affected_version:3.1\nBody:\n### Apache Airflow version\n\n3.1.0\n\n### If \"Other Airflow 2 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\nThe command is used to clean up the records created for 30 days+ :\n```\nairflow db clean --clean-before-timestamp \"$(date -d \"30 days ago\" +%Y-%m-%dT%H:%M)\" --skip-archive --yes\n```\n\nThe error :\n```\nChecking table dag_version\nFound 155 rows meeting deletion criteria.\nPerforming Delete...\nMoving data to table _airflow_deleted__dag_version__20250929024208\n    self.dialect.do_execute(\n  File \"/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py\", line 736, in do_execute\n    cursor.execute(statement, parameters)\npsycopg2.errors.ForeignKeyViolation: update or delete on table \"dag_version\" violates foreign key constraint \"task_instance_dag_version_id_fkey\" on table \"task_instance\"\nDETAIL:  Key (id)=(0198edf9-f9f1-7d73-92c1-fac5359c10b7) is still referenced from table \"task_instance\".\n```\n\nFor the DAG Version ID `0198edf9-f9f1-7d73-92c1-fac5359c10b7`, I found :-\n- 1 record in  `dag_version` with `last_updated` = 2025-08-28  (more than 1 month ago, should be scoped)\n- some records in `task_instance` with `updated_at` = 2025-09-02  (less than 1 month ago, should be not scoped)\n\nSeem I can workaround by retrying it after 2025-10-02 so all mentioned records will be scoped to remove.\n\n### What you think should happen instead?\n\nIt should skip the problematic records / tables, or aggressively remove the referencing records as well.\n\n### How to reproduce\n\nn/a\n\n### Operating System\n\nAlmaLinux release 9.6 (Sage Margay)\n\n### Versions of Apache Airflow Providers\n\n_No response_\n\n### Deployment\n\nOfficial Apache Airflow Helm Chart\n\n### Deployment details\n\n_No response_\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56191\nTitle: Add e2e test for remote logging\nState: closed\nAuthor: gopidesupavan\nLabels: \nBody:\nAdding Remote logging tests to E2E test.\r\n\r\nWhy:\r\n\r\nCurrently we dont have a way to test remote logging, all that we are doing so far mocking handlers and we have seen recent times remote logging broken many times.\r\n\r\nThis is to add remote logging e2e test that replicates real environment with AWS using localstack. This test is remote logging with S3. for other remote logging it can be extended later. \r\n\r\n\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56190\nTitle: SFTPOperator DELETE operation should not error if the file/directory is already absent\nState: open\nAuthor: marcbal\nLabels: kind:bug, area:providers, kind:feature, good first issue, provider:sftp\nBody:\n### Description\n\nSFTPOperator DELETE operation should not error if the file/directory is already absent.\n\nIf having this as the default behaviour is not appropriate, I would suggest adding a parameter to ignore this specific error.\n\n### Use case/motivation\n\nI have a SFTPOperator deleting a folder, so a subsequent SFTPOperator can PUT a new version of the folder's content.\nWhen the folder does not exists initialy, the first operator crashes.\nI expect the DELETE operation to succeed by default when there is nothing to delete, or at least have the option to ignore the error in case the directory is non existant (I don't want to ignore other error, like connection failure, permission issues, ...)\n\n### Related issues\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56189\nTitle: Fix Edge3 provider navigation with webserver base_url configuration\nState: closed\nAuthor: dheerajturaga\nLabels: \nBody:\nThe Edge3 provider's BrowserRouter was not respecting the Airflow\r\n  webserver.base_url configuration, causing navigation links to generate\r\n  absolute paths from root instead of properly prefixed paths. This\r\n  resulted in 404 errors when Airflow is deployed with a base URL prefix\r\n  (e.g., my_company.com/airflow).\r\n\r\n  **Problem:**\r\n  - Edge3 provider pages were navigating to /plugin/edge_worker instead of /airflow/plugin/edge_worker\r\n  - This broke navigation in deployments where nginx redirects traffic from my_company.com/airflow to localhost:8080\r\n  - The issue was in JobsPage.tsx:94 where RouterLink generated links without the base URL prefix\r\n\r\n<img width=\"1642\" height=\"191\" alt=\"image\" src=\"https://github.com/user-attachments/assets/54399bb9-e04a-4c0a-97be-f0916808b23a\" />",
  "Requirement ID: ISSUE-56188\nTitle: i18n: Remove showGraph and showGrid keys\nState: closed\nAuthor: choo121600\nLabels: area:UI, area:translations, translation:ko\nBody:\nThe keys `showGraph` and `showGrid` were removed from the en locale.\r\n\r\n<img width=\"889\" height=\"555\" alt=\"image\" src=\"https://github.com/user-attachments/assets/59412aaa-45fc-4233-8fba-cde160fc93d2\" />\r\n\r\nPlease review this translation.\r\n/cc @kgw7401 @0ne-stone\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56187\nTitle: Move the traces and metrics code under a common observability package\nState: open\nAuthor: xBis7\nLabels: area:Scheduler, area:CLI, area:serialization, area:plugins, area:Triggerer, area:DAG-processing, area:Executors-core\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\nThis patch is refactoring the `metrics` and the `traces` packages and moves them under a common package named `observability`.\r\n\r\nThis is done so that it will be easier to add common files in the future.\r\n\r\nThe need for these changes came from this discussion\r\n\r\nhttps://github.com/apache/airflow/pull/56150#discussion_r2384025894\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56186\nTitle: Tasks queued in docker setup\nState: open\nAuthor: inkerinmaa\nLabels: kind:bug, area:API, area:core, needs-triage\nBody:\n### Apache Airflow version\n\n3.1.0\n\n### If \"Other Airflow 2 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\nUsing official docker compose tutorial[https://airflow.apache.org/docs/apache-airflow/3.1.0/howto/docker-compose/index.html](url) added network settings to each service in compose:\n`\n  networks:\n    - test`\nAnd at the end of compose file:\n`networks:\n  test:\n    name: test\n    external: true`\ncaused tasks in example dags get stucked in queued state and apiserver log says:\n`JWT token is not valid: Signature verification failed` \nSame happens if I add \n`AIRFLOW__API__BASE_URL: 'https://tst.mycompany.com/airflow'`\nto use reverse-proxy for accessing UI\n\n### What you think should happen instead?\n\nTasks should be running and no errors in logs appear\n\n### How to reproduce\n\nUse official docker compose tutorial[https://airflow.apache.org/docs/apache-airflow/3.1.0/howto/docker-compose/index.html](url)\nand add all services to any external docker network\ndocker network create airflow\nand add to each service\n`networks:\n    - airflow`\nOR \nadd environment varible\n`AIRFLOW__API__BASE_URL: 'https://tst.mycompany.com/airflow'`\n\n\n### Operating System\n\nDebian\n\n### Versions of Apache Airflow Providers\n\n_No response_\n\n### Deployment\n\nDocker-Compose\n\n### Deployment details\n\n_No response_\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56185\nTitle: fix: Add max_retry_delay to MappedOperator model\nState: closed\nAuthor: Joffreybvn\nLabels: type:bug-fix\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\nFixes #56184\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56184\nTitle: Scheduler crash when `retry_exponential_backoff` is set on MappedOperator which retries\nState: open\nAuthor: Joffreybvn\nLabels: kind:bug, area:Scheduler, area:core, needs-triage\nBody:\n### Apache Airflow version\n\n3.1.0\n\n### If \"Other Airflow 2 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\n1. Declare a MappedOperator (partial expand) with `retry_exponential_backoff=True`\n2. Make the task failing\n3. The scheduler will fail to re-schedule the task, and crash\n\n**Root cause**: When [retry_exponential_backoff is set](https://github.com/apache/airflow/blob/main/airflow-core/src/airflow/models/taskinstance.py#L951), the scheduler [looks for max_retry_delay](https://github.com/apache/airflow/blob/main/airflow-core/src/airflow/models/taskinstance.py#L989). But this attribute is not in the [MappedOperator model ](https://github.com/apache/airflow/blob/main/airflow-core/src/airflow/models/mappedoperator.py)(anymore?)\n\nPR: #56185 \n\n```\n[2m2025-09-26T17:01:49.607085Z[0m [[31m[1merror    [0m] [1mException when executing SchedulerJob._run_scheduler_loop[0m [[0m[1m[34mairflow.jobs.scheduler_job_runner.SchedulerJobRunner[0m][0m [36mloc[0m=[35mscheduler_job_runner.py:1046[0m\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.13/site-packages/airflow/jobs/scheduler_job_runner.py\", line 1042, in _execute\n    self._run_scheduler_loop()\n    ~~~~~~~~~~~~~~~~~~~~~~~~^^\n  File \"/usr/local/lib/python3.13/site-packages/airflow/jobs/scheduler_job_runner.py\", line 1332, in _run_scheduler_loop\n    num_queued_tis = self._do_scheduling(session)\n  File \"/usr/local/lib/python3.13/site-packages/airflow/jobs/scheduler_job_runner.py\", line 1442, in _do_scheduling\n    callback_tuples = self._schedule_all_dag_runs(guard, dag_runs, session)\n  File \"/usr/local/lib/python3.13/site-packages/airflow/utils/retries.py\", line 97, in wrapped_function\n    for attempt in run_with_db_retries(max_retries=retries, logger=logger, **retry_kwargs):\n                   ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.13/site-packages/tenacity/__init__.py\", line 445, in __iter__\n    do = self.iter(retry_state=retry_state)\n  File \"/usr/local/lib/python3.13/site-packages/tenacity/__init__.py\", line 378, in iter\n    result = action(retry_state)\n  File \"/usr/local/lib/python3.13/site-packages/tenacity/__init__.py\", line 400, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ~~~~~~~~~~~~~~~~~^^\n  File \"/usr/lib64/python3.13/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ~~~~~~~~~~~~~~~~~^^\n  File \"/usr/lib64/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/usr/local/lib/python3.13/site-packages/airflow/utils/retries.py\", line 106, in wrapped_function\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.13/site-packages/airflow/jobs/scheduler_job_runner.py\", line 1850, in _schedule_all_dag_runs\n    callback_tuples = [(run, self._schedule_dag_run(run, session=session)) for run in dag_runs]\n                             ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.13/site-packages/airflow/jobs/scheduler_job_runner.py\", line 1966, in _schedule_dag_run\n    schedulable_tis, callback_to_run = dag_run.update_state(session=session, execute_callbacks=False)\n                                       ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.13/site-packages/airflow/utils/session.py\", line 98, in wrapper\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.13/site-packages/airflow/models/dagrun.py\", line 1149, in update_state\n    info = self.task_instance_scheduling_decisions(session)\n  File \"/usr/local/lib/python3.13/site-packages/airflow/utils/session.py\", line 98, in wrapper\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.13/site-packages/airflow/models/dagrun.py\", line 1329, in task_instance_scheduling_decisions\n    schedulable_tis, changed_tis, expansion_happened = self._get_ready_tis(\n                                                       ~~~~~~~~~~~~~~~~~~~^\n        schedulable_tis,\n        ^^^^^^^^^^^^^^^^\n        finished_tis,\n        ^^^^^^^^^^^^^\n        session=session,\n        ^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/usr/local/lib/python3.13/site-packages/airflow/models/dagrun.py\", line 1519, in _get_ready_tis\n    if not schedulable.are_dependencies_met(session=session, dep_context=dep_context):\n           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.13/site-packages/airflow/utils/session.py\", line 98, in wrapper\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.13/site-packages/airflow/models/taskinstance.py\", line 901, in are_dependencies_met\n    for dep_status in self.get_failed_dep_statuses(dep_context=dep_context, session=session):\n                      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.13/site-packages/airflow/models/taskinstance.py\", line 924, in get_failed_dep_statuses\n    for dep_status in dep.get_dep_statuses(self, session, dep_context):\n                      ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.13/site-packages/airflow/ti_deps/deps/base_ti_dep.py\", line 116, in get_dep_statuses\n    yield from self._get_dep_statuses(ti, session, cxt)\n  File \"/usr/local/lib/python3.13/site-packages/airflow/ti_deps/deps/not_in_retry_period_dep.py\", line 48, in _get_dep_statuses\n    next_task_retry_date = ti.next_retry_datetime()\n  File \"/usr/local/lib/python3.13/site-packages/airflow/models/taskinstance.py\", line 989, in next_retry_datetime\n    if self.task.max_retry_delay:\n       ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<attrs generated getattr airflow.models.mappedoperator.MappedOperator>\", line 11, in __getattr__\n    return super().__getattribute__(item)\n           ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^\nAttributeError: 'MappedOperator' object has no attribute 'max_retry_delay'. Did you mean: 'retry_delay'?\n```\n\n### What you think should happen instead?\n\n_No response_\n\n### How to reproduce\n\nMappedOperator re-scheduled after a first try, with `retry_exponential_backoff=True`\n\n### Operating System\n\nFedora 42\n\n### Versions of Apache Airflow Providers\n\n_No response_\n\n### Deployment\n\nOther 3rd-party Helm chart\n\n### Deployment details\n\n_No response_\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [x] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56183\nTitle: Status of testing Providers that were prepared on September 28, 2025\nState: closed\nAuthor: eladkal\nLabels: area:providers, kind:meta, testing status\nBody:\n### Body\n\nI have a kind request for all the contributors to the latest provider distributions release.\nCould you please help us to test the RC versions of the providers?\n\nThe guidelines on how to test providers can be found in\n\n[Verify providers by contributors](https://github.com/apache/airflow/blob/main/dev/README_RELEASE_PROVIDERS.md#verify-the-release-candidate-by-contributors)\n\nLet us know in the comments, whether the issue is addressed.\n\nThese are providers that require testing as there were some substantial changes introduced:\n\n\n## Provider [standard: 1.9.0rc2](https://pypi.org/project/apache-airflow-providers-standard/1.9.0rc2)\n   - [x] [Add a `@task.stub` to allow tasks in other languages to be defined in dags (#56055)](https://github.com/apache/airflow/pull/56055): @ashb\n   - [x] [Fix DagBag imports in 3.2+ (#56109)](https://github.com/apache/airflow/pull/56109): @jedcunningham\n   - [ ] [Move DagBag to airflow/dag_processing (#55139)](https://github.com/apache/airflow/pull/55139): @ephraimbuddy\n\n<!--\n\nNOTE TO RELEASE MANAGER:\n\nYou can move here the providers that have doc-only changes or for which changes are trivial, and\nyou could assess that they are OK.\n\n-->\nAll users involved in the PRs:\n@jedcunningham @ephraimbuddy @ashb\n\n\n### Committer\n\n- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",
  "Requirement ID: ISSUE-56182\nTitle: Fix release guide prepare email commands\nState: closed\nAuthor: eladkal\nLabels: area:dev-tools\nBody:",
  "Requirement ID: ISSUE-56181\nTitle: Prepare standard provider to release (RC2 September 2025)\nState: closed\nAuthor: eladkal\nLabels: area:providers, kind:documentation, provider:standard\nBody:",
  "Requirement ID: ISSUE-56180\nTitle: Add options to manage translations: sync, remove extra, and dry run\nState: open\nAuthor: choo121600\nLabels: area:dev-tools, backport-to-v3-1-test\nBody:\nCurrently, we use the command `uv run dev/i18n/check_translations_completeness.py --language ko --add-missing`\r\nto check translation completeness. However, this approach has limitations.\r\nIf existing content is removed and translation keys disappear from en, we must manually locate and remove those keys.\r\n\r\n<img width=\"956\" height=\"757\" alt=\"image\" src=\"https://github.com/user-attachments/assets/b7665912-096e-420a-96e0-ec5afe63c42a\" />\r\n\r\nTo address this, I have added a new `sync` command that runs both `remove-extra` and `add-missing`.\r\nThis command automatically synchronizes removed translation keys in en across other language folders.\r\n\r\nhttps://github.com/user-attachments/assets/efd91bf5-64c1-4bfe-99bf-1cbb873b22cc\r\n\r\n\r\nhttps://github.com/user-attachments/assets/fed5e49e-daee-4034-a2f7-6bfa15b308aa\r\n\r\n\r\nAs a result, translation management becomes more efficient and consistent.\r\n\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56179\nTitle: [v3-1-test] Update changelog of dockerfile to include Fab missing in Python 3.13 (#56176)\nState: closed\nAuthor: github-actions[bot]\nLabels: kind:documentation, area:production-image\nBody:\n(cherry picked from commit 19fd4dea3594fb059fd58ce7700c0124e3ef66ed)\n\nCo-authored-by: Jarek Potiuk <jarek@potiuk.com>\nCloses: #56123",
  "Requirement ID: ISSUE-56178\nTitle: Fix static check error resulting from not rebased change in FAB5\nState: closed\nAuthor: potiuk\nLabels: area:providers, provider:fab\nBody:\nFollow up to #50960\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56177\nTitle: docs:improve plugin system documentation for clarity and completeness\nState: open\nAuthor: Lohith625\nLabels: area:plugins, kind:documentation\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n### Overview\r\nThis PR improves the plugin system documentation for Airflow by:\r\n- Adding a clear explanation of what the plugin system is and why it matters.\r\n- Listing available building blocks (External Views, React Apps, FastAPI Apps, Middlewares, Macros, Operator Extra Links, Timetables, Listeners).\r\n- Including examples for plugin management interface and external view iframe integration.\r\n- Making the documentation more comprehensive and beginner-friendly.\r\n\r\n### Motivation\r\nWith the plugin system enhancements released in Airflow 3.1, it is a good time to revisit and improve the documentation to help developers adopt the new features quickly.\r\n\r\n### Related Issues\r\nCloses #55837\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56176\nTitle: Update changelog of dockerfile to include Fab missing in Python 3.13\nState: closed\nAuthor: potiuk\nLabels: kind:documentation, area:production-image, backport-to-v3-1-test\nBody:\nCloses: https://github.com/apache/airflow/issues/56123\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56175\nTitle: Add asset_helpers.py with tests and support it in asset definitions.\nState: open\nAuthor: gkirchou\nLabels: area:task-sdk\nBody:\nWhen `Asset` or `Metadata` objects are created with `extra` dictionaries containing numpy types (e.g., `np.int64`, `np.float64`, `np.ndarray`), downstream JSON serialization can fail, as these types are not directly\r\nserializable to JSON. This issue can arise when metadata is passed via `TaskOutlet`s.\r\n\r\nThis change introduces automatic normalization of the `extra` metadata upon `Asset` and `Metadata` instantiation. It recursively converts numpy integer, float, boolean, array, and complex types into standard Python types (int, float, bool, list, dict) to ensure JSON serializability.\r\n\r\ncloses: #53474\r\n\r\n---\r\n\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56174\nTitle: Fix airbyte connection UI\nState: closed\nAuthor: Zeglow\nLabels: area:providers, provider:airbyte\nBody:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #55841\r\nrelated: #55841\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\nFix: Make Airbyte connection fields properly labeled in Airflow 3\r\n\r\nFixes the formatting issue in `get_ui_field_behaviour` method that was preventing proper field labeling for Airbyte connections in Airflow 3.\r\n\r\n**Problem:**\r\nIn Airflow 3, the Airbyte provider connection form shows generic field names (login, password, schema) instead of user-friendly labels (Client ID, Client Secret, Token URL), making it confusing for users to set up Airbyte connections.\r\n\r\n**Root Cause:**\r\nThe `get_ui_field_behaviour` method had incorrect formatting/indentation in the `relabeling` dictionary, causing the UI field relabeling functionality to fail silently.\r\n\r\n**Solution:**\r\n- Fixed formatting and indentation in the `relabeling` dictionary\r\n- Ensures proper field labeling:\r\n  - \"host\" \u2192 \"Server URL\"  \r\n  - \"login\" \u2192 \"Client ID\"\r\n  - \"password\" \u2192 \"Client Secret\"\r\n  - \"schema\" \u2192 \"Token URL\"\r\n\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n\r\n---\r\n\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56173\nTitle: [WIP]Enhance FilterBar with DateRangeFilter for compact UI\nState: open\nAuthor: RoyLee1224\nLabels: area:UI, area:translations, translation:default\nBody:\n## Why\r\n\r\n`logical date` and `run after` from/to filters are taking too much space in both filter bar & dropdown\r\n\r\nSwitching to a range filter saves space, aligns better with the design of other components, and provides a more convenient user experience.\r\n\r\n\r\n## What\r\n\r\n- [x] Added `DateRangeFilter` & `DateRangeCalendar` components.\r\n- [x] Add utility functions: `getDefaultFilterValue` and `isValidFilterValue`.\r\n- [x] Define date format (YYYY/MM/DD) as constant\r\n- [x] Replace hardcoded colors\r\n- [x] Support dark mode\r\n- [ ] Add optional setting to let users include custom time when selecting date ranges\r\n- [ ] Implement range filter for `Events`, `AssetEvents`, `DagRuns`,\r\n- [ ] Replace from/to filters\r\n- [ ] Test `datetime-local` issues for Safari and Firefox\r\n- [ ] Add tests for Date Range\r\n\r\n## Screenshots\r\n### Dropdown before / after\r\n<img width=\"299\" height=\"452\" alt=\"dropdown_before\" src=\"https://github.com/user-attachments/assets/7cb77815-9c53-40a9-9dd5-45bba4d2ea94\" />\r\n<img width=\"299\" height=\"452\" alt=\"dropdown_after\" src=\"https://github.com/user-attachments/assets/d2bb6cc3-57ba-4fb3-8415-156f477f3f7e\" />\r\n\r\n### Date picker before / after\r\n<img width=\"1095\" height=\"597\" alt=\"before\" src=\"https://github.com/user-attachments/assets/d09c8625-26ff-4f66-a775-523029de10eb\" />\r\n<img width=\"1095\" height=\"597\" alt=\"after\" src=\"https://github.com/user-attachments/assets/e299a829-fde7-4b72-83d2-c68fff776dbe\" />\r\n\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).",
  "Requirement ID: ISSUE-56172\nTitle: Add an ability for breeze to utilize local Kubernetes cluster\nState: open\nAuthor: shahar1\nLabels: area:dev-env, area:dev-tools, kind:feature\nBody:\n### Description\n\nI came up with this idea while working on #39791.\nCurrently there is no comfortable way to integrate a local k8s cluster (e.g., `kind`) with Breeze, which makes KPO and KubernetesExecutor hard to emulate locally with a regular `breeze start-airflow`. As `kind` cluster can be deployed as a Docker container, it's just a matter of few configurations to make it accessible by the Airflow instance delpoyed with Breeze.\n\n### Use case/motivation\n\nUsing `KubernetesPodOperator` and `KubernetesExecutor`  within breeze deployment while working with a local ks8 cluster\n\n### Related issues\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [x] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)",
  "Requirement ID: ISSUE-56171\nTitle: Fix upgrade checks with prek in v3-1-test\nState: closed\nAuthor: jscheffl\nLabels: area:dev-tools, area:production-image\nBody:\nUpgrade checks fail on v3-1-test. Fix them\r\nSee https://github.com/apache/airflow/actions/runs/18049502618/job/51368367749",
  "Requirement ID: ISSUE-56170\nTitle: Fix upgrade checks with prek\nState: closed\nAuthor: jscheffl\nLabels: area:dev-tools, area:go-sdk\nBody:\nFixes auto-upgrade checks as CI fails in https://github.com/apache/airflow/actions/runs/18064002368/job/51404322695"
]